"""
Enso Atlas API - FastAPI Backend

Provides REST API endpoints for the professional frontend:
- Slide analysis with MIL prediction
- Evidence generation (heatmaps, patches)
- Similar case retrieval (FAISS)
- Report generation (MedGemma)
- Patch embedding (Path Foundation)
"""

from pathlib import Path
from typing import Optional, List, Dict, Any
import logging
import json
import base64
import io

import numpy as np
from PIL import Image

from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse
from pydantic import BaseModel, Field

logger = logging.getLogger(__name__)


def _check_cuda() -> bool:
    """Check if CUDA is available."""
    try:
        import torch
        return torch.cuda.is_available()
    except ImportError:
        return False


# Request/Response Models
class AnalyzeRequest(BaseModel):
    slide_id: str
    generate_report: bool = False


class AnalyzeResponse(BaseModel):
    slide_id: str
    prediction: str
    score: float
    confidence: float
    patches_analyzed: int
    top_evidence: List[Dict[str, Any]]
    similar_cases: List[Dict[str, Any]]


class ReportRequest(BaseModel):
    slide_id: str
    include_evidence: bool = True
    include_similar: bool = True


class ReportResponse(BaseModel):
    slide_id: str
    report_json: Dict[str, Any]
    summary_text: str


class SlideInfo(BaseModel):
    slide_id: str
    patient_id: Optional[str] = None
    has_embeddings: bool = False
    label: Optional[str] = None
    num_patches: Optional[int] = None


class EmbedRequest(BaseModel):
    """Request for patch embedding."""
    patches: List[str] = Field(
        ...,
        description="Base64-encoded patch images (224x224 RGB)",
        min_length=1,
        max_length=128,
    )
    return_embeddings: bool = Field(
        default=True,
        description="Whether to return embedding vectors",
    )


class EmbedResponse(BaseModel):
    """Response from embedding endpoint."""
    num_patches: int
    embedding_dim: int = 384
    embeddings: Optional[List[List[float]]] = None


class SimilarRequest(BaseModel):
    """Request for similar case search."""
    slide_id: str
    k: int = Field(default=5, ge=1, le=20)
    top_patches: int = Field(default=3, ge=1, le=10)


class SimilarResponse(BaseModel):
    """Response from similar case search."""
    slide_id: str
    similar_cases: List[Dict[str, Any]]
    num_queries: int


def create_app(
    embeddings_dir: Path = Path("data/demo/embeddings"),
    model_path: Path = Path("models/demo_clam.pt"),
    enable_cors: bool = True,
) -> FastAPI:
    """Create and configure the FastAPI application."""
    
    app = FastAPI(
        title="Enso Atlas API",
        description="On-Prem Pathology Evidence Engine for Treatment-Response Insight",
        version="0.1.0",
        docs_url="/api/docs",
        redoc_url="/api/redoc",
    )
    
    # CORS middleware for frontend development
    if enable_cors:
        app.add_middleware(
            CORSMiddleware,
            allow_origins=[
                "http://localhost:3000",
                "http://localhost:7860",
                "http://100.111.126.23:3000",
                "http://100.111.126.23:8003",
            ],
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )
    
    # Load models on startup
    classifier = None
    evidence_gen = None
    embedder = None
    reporter = None
    available_slides = []
    
    @app.on_event("startup")
    async def load_models():
        nonlocal classifier, evidence_gen, embedder, reporter, available_slides
        
        from ..config import MILConfig, EvidenceConfig, EmbeddingConfig
        from ..mil.clam import CLAMClassifier
        from ..evidence.generator import EvidenceGenerator
        from ..embedding.embedder import PathFoundationEmbedder
        from ..reporting.medgemma import MedGemmaReporter, ReportingConfig
        
        # Load MIL classifier
        config = MILConfig(input_dim=384, hidden_dim=128)
        classifier = CLAMClassifier(config)
        if model_path.exists():
            classifier.load(model_path)
            logger.info(f"Loaded MIL model from {model_path}")
        
        # Setup evidence generator
        evidence_config = EvidenceConfig()
        evidence_gen = EvidenceGenerator(evidence_config)
        
        # Setup embedder (lazy-loaded on first use)
        embedding_config = EmbeddingConfig()
        embedder = PathFoundationEmbedder(embedding_config)
        
        # Setup MedGemma reporter (lazy-loaded on first use)
        reporting_config = ReportingConfig()
        reporter = MedGemmaReporter(reporting_config)
        logger.info("MedGemma reporter initialized (model loads on first call)")
        
        # Find available slides and build FAISS index
        all_embeddings = []
        all_metadata = []
        
        if embeddings_dir.exists():
            for f in sorted(embeddings_dir.glob("*.npy")):
                if not f.name.endswith("_coords.npy"):
                    slide_id = f.stem
                    available_slides.append(slide_id)
                    
                    # Load embeddings for FAISS index
                    embs = np.load(f)
                    all_embeddings.append(embs)
                    all_metadata.append({
                        "slide_id": slide_id,
                        "n_patches": len(embs),
                    })
            
            logger.info(f"Found {len(available_slides)} slides with embeddings")
            
            # Build FAISS index for similarity search
            if all_embeddings:
                evidence_gen.build_reference_index(all_embeddings, all_metadata)
                logger.info(f"Built FAISS index with {len(all_embeddings)} slides")
    
    @app.get("/health")
    async def health_check():
        """Health check endpoint."""
        return {
            "status": "healthy",
            "version": "0.1.0",
            "model_loaded": classifier is not None,
            "cuda_available": _check_cuda(),
            "slides_available": len(available_slides),
        }
    
    @app.get("/")
    async def root():
        """API root endpoint."""
        return {
            "name": "Enso Atlas API",
            "version": "0.1.0",
            "docs": "/api/docs",
        }
    
    @app.get("/api/slides", response_model=List[SlideInfo])
    async def list_slides():
        """List all available slides."""
        slides = []
        labels_path = embeddings_dir.parent / "labels.csv"
        labels = {}
        
        if labels_path.exists():
            import csv
            with open(labels_path) as f:
                reader = csv.DictReader(f)
                for row in reader:
                    labels[row["slide_id"]] = row.get("label", "")
        
        for slide_id in available_slides:
            # Get patch count
            emb_path = embeddings_dir / f"{slide_id}.npy"
            num_patches = None
            if emb_path.exists():
                try:
                    emb = np.load(emb_path)
                    num_patches = len(emb)
                except Exception:
                    pass
            
            slides.append(SlideInfo(
                slide_id=slide_id,
                has_embeddings=True,
                label=labels.get(slide_id),
                num_patches=num_patches,
            ))
        
        return slides
    
    @app.post("/api/analyze", response_model=AnalyzeResponse)
    async def analyze_slide(request: AnalyzeRequest):
        """Analyze a slide and return prediction with evidence."""
        if classifier is None:
            raise HTTPException(status_code=503, detail="Model not loaded")
        
        slide_id = request.slide_id
        emb_path = embeddings_dir / f"{slide_id}.npy"
        
        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")
        
        # Load embeddings
        embeddings = np.load(emb_path)
        
        # Run prediction
        score, attention = classifier.predict(embeddings)
        label = "RESPONDER" if score > 0.5 else "NON-RESPONDER"
        confidence = abs(score - 0.5) * 2
        
        # Get top evidence patches
        top_k = min(8, len(attention))
        top_indices = np.argsort(attention)[-top_k:][::-1]
        
        top_evidence = []
        for i, idx in enumerate(top_indices):
            top_evidence.append({
                "rank": i + 1,
                "patch_index": int(idx),
                "attention_weight": float(attention[idx]),
            })
        
        # Get similar cases using FAISS
        similar_cases = []
        if evidence_gen is not None:
            try:
                similar_results = evidence_gen.find_similar(
                    embeddings, attention, k=10, top_patches=3
                )
                seen_slides = set()
                for s in similar_results:
                    meta = s.get("metadata", {})
                    sid = meta.get("slide_id", "unknown")
                    if sid != slide_id and sid not in seen_slides:
                        seen_slides.add(sid)
                        similar_cases.append({
                            "slide_id": sid,
                            "similarity_score": 1.0 / (1.0 + s["distance"]),
                            "distance": float(s["distance"]),
                        })
                    if len(similar_cases) >= 5:
                        break
            except Exception as e:
                logger.warning(f"Similar case search failed: {e}")
                # Fall back to random
                for sid in available_slides[:5]:
                    if sid != slide_id:
                        similar_cases.append({
                            "slide_id": sid,
                            "similarity_score": float(np.random.uniform(0.7, 0.95)),
                        })
        
        return AnalyzeResponse(
            slide_id=slide_id,
            prediction=label,
            score=float(score),
            confidence=float(confidence),
            patches_analyzed=len(embeddings),
            top_evidence=top_evidence,
            similar_cases=similar_cases[:5],
        )
    
    @app.post("/api/report", response_model=ReportResponse)
    async def generate_report(request: ReportRequest):
        """Generate a structured report for a slide using MedGemma."""
        if classifier is None:
            raise HTTPException(status_code=503, detail="Model not loaded")
        
        slide_id = request.slide_id
        emb_path = embeddings_dir / f"{slide_id}.npy"
        coord_path = embeddings_dir / f"{slide_id}_coords.npy"
        
        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")
        
        embeddings = np.load(emb_path)
        score, attention = classifier.predict(embeddings)
        label = "responder" if score > 0.5 else "non-responder"
        
        # Get top evidence patches with coordinates
        top_k = min(8, len(attention))
        top_indices = np.argsort(attention)[-top_k:][::-1]
        
        # Load coordinates if available
        coords = None
        if coord_path.exists():
            coords = np.load(coord_path)
        
        evidence_patches = []
        for rank, idx in enumerate(top_indices, 1):
            patch_info = {
                "rank": rank,
                "patch_index": int(idx),
                "attention_weight": float(attention[idx]),
                "coordinates": [int(coords[idx][0]), int(coords[idx][1])] if coords is not None else [0, 0],
            }
            evidence_patches.append(patch_info)
        
        # Get similar cases
        similar_cases = []
        if evidence_gen is not None:
            try:
                similar_results = evidence_gen.find_similar(
                    embeddings, attention, k=5, top_patches=3
                )
                for s in similar_results:
                    similar_cases.append(s)
            except Exception as e:
                logger.warning(f"Similar case search failed for report: {e}")
        
        # Try MedGemma report generation
        if reporter is not None:
            try:
                report = reporter.generate_report(
                    evidence_patches=evidence_patches,
                    score=score,
                    label=label,
                    similar_cases=similar_cases,
                    case_id=slide_id,
                )
                
                return ReportResponse(
                    slide_id=slide_id,
                    report_json=report["structured"],
                    summary_text=report["summary"],
                )
            except Exception as e:
                logger.warning(f"MedGemma report generation failed, using template: {e}")
        
        # Fallback to template report
        report_json = {
            "case_id": slide_id,
            "task": "Bevacizumab treatment response prediction",
            "model_output": {
                "label": label,
                "probability": float(score),
                "calibration_note": "Model probability requires external validation.",
            },
            "evidence": [
                {
                    "patch_id": f"patch_{p['patch_index']}",
                    "attention_weight": p["attention_weight"],
                    "coordinates": p["coordinates"],
                }
                for p in evidence_patches[:5]
            ],
            "limitations": [
                "Research tool only",
                "Not clinically validated",
                "Requires pathologist review",
            ],
            "safety_statement": "This is a research tool. All findings require validation by qualified pathologists.",
        }
        
        summary_text = f"""Case: {slide_id}
Prediction: {label.upper()}
Score: {score:.3f}

This analysis examined {len(embeddings)} tissue patches. The top {min(5, len(attention))} 
patches by attention weight show consistent morphological patterns associated with 
{label} cases in the training cohort.

IMPORTANT: This is a research tool for decision support only. All findings must be 
reviewed and validated by qualified pathologists before any clinical decision-making."""
        
        return ReportResponse(
            slide_id=slide_id,
            report_json=report_json,
            summary_text=summary_text,
        )
    
    @app.get("/api/heatmap/{slide_id}")
    async def get_heatmap(slide_id: str):
        """Get the attention heatmap for a slide as PNG."""
        if classifier is None or evidence_gen is None:
            raise HTTPException(status_code=503, detail="Model not loaded")
        
        emb_path = embeddings_dir / f"{slide_id}.npy"
        coord_path = embeddings_dir / f"{slide_id}_coords.npy"
        
        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")
        
        embeddings = np.load(emb_path)
        
        # Load or generate coordinates
        if coord_path.exists():
            coords = np.load(coord_path)
        else:
            coords = np.random.randint(0, 50000, (len(embeddings), 2))
        
        coords = [tuple(c) for c in coords]
        
        # Run prediction and create heatmap
        score, attention = classifier.predict(embeddings)
        slide_dims = (50000, 50000)
        heatmap = evidence_gen.create_heatmap(attention, coords, slide_dims, (512, 512))
        
        # Save to temp file and return
        from PIL import Image
        import io
        
        img = Image.fromarray(heatmap)
        buf = io.BytesIO()
        img.save(buf, format="PNG")
        buf.seek(0)
        
        return StreamingResponse(
            buf,
            media_type="image/png",
            headers={"Content-Disposition": f"inline; filename={slide_id}_heatmap.png"},
        )
    
    @app.post("/api/embed", response_model=EmbedResponse)
    async def embed_patches(request: EmbedRequest):
        """Generate embeddings for patch images using Path Foundation."""
        if embedder is None:
            raise HTTPException(status_code=503, detail="Embedder not initialized")
        
        # Decode patches
        patches = []
        for i, b64_patch in enumerate(request.patches):
            try:
                # Handle data URI format
                if "," in b64_patch:
                    b64_patch = b64_patch.split(",", 1)[1]
                
                image_data = base64.b64decode(b64_patch)
                image = Image.open(io.BytesIO(image_data))
                
                if image.mode != "RGB":
                    image = image.convert("RGB")
                
                if image.size != (224, 224):
                    image = image.resize((224, 224), Image.Resampling.LANCZOS)
                
                patches.append(np.array(image))
            except Exception as e:
                raise HTTPException(
                    status_code=400,
                    detail=f"Failed to decode patch {i}: {str(e)}",
                )
        
        # Generate embeddings
        try:
            embeddings = embedder.embed(patches, show_progress=False)
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Embedding generation failed: {str(e)}",
            )
        
        response = EmbedResponse(
            num_patches=len(patches),
            embedding_dim=embeddings.shape[1] if len(embeddings.shape) > 1 else 384,
        )
        
        if request.return_embeddings:
            response.embeddings = embeddings.tolist()
        
        return response
    
    @app.get("/api/similar", response_model=SimilarResponse)
    async def get_similar_cases(slide_id: str, k: int = 5, top_patches: int = 3):
        """Find similar cases from the reference cohort."""
        if classifier is None or evidence_gen is None:
            raise HTTPException(status_code=503, detail="Model not loaded")
        
        emb_path = embeddings_dir / f"{slide_id}.npy"
        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")
        
        embeddings = np.load(emb_path)
        _, attention = classifier.predict(embeddings)
        
        similar_results = evidence_gen.find_similar(
            embeddings, attention, k=k * 3, top_patches=top_patches
        )
        
        similar_cases = []
        seen_slides = set()
        
        for s in similar_results:
            meta = s.get("metadata", {})
            sid = meta.get("slide_id", "unknown")
            
            if sid == slide_id:
                continue
            
            if sid not in seen_slides:
                seen_slides.add(sid)
                similar_cases.append({
                    "slide_id": sid,
                    "distance": float(s["distance"]),
                    "similarity_score": 1.0 / (1.0 + s["distance"]),
                    "patch_index": meta.get("patch_index"),
                })
            
            if len(similar_cases) >= k:
                break
        
        return SimilarResponse(
            slide_id=slide_id,
            similar_cases=similar_cases,
            num_queries=top_patches,
        )
    
    @app.get("/api/embed/status")
    async def embedder_status():
        """Check the status of the Path Foundation embedder."""
        model_loaded = embedder is not None and embedder._model is not None
        device = "unknown"
        
        if model_loaded:
            device = str(embedder._device)
        else:
            device = "cuda" if _check_cuda() else "cpu"
        
        return {
            "model": "google/path-foundation",
            "model_loaded": model_loaded,
            "device": device,
            "embedding_dim": 384,
            "input_size": 224,
        }
    
    # WSI / DZI Tile Serving
    # Cache for OpenSlide objects and DeepZoom generators
    # slides are at data/slides (not inside demo/)
    slides_dir = embeddings_dir.parent.parent / "slides"
    wsi_cache: Dict[str, Any] = {}
    logger.info(f"Slides directory: {slides_dir}")
    
    def get_slide_and_dz(slide_id: str):
        """Get or create OpenSlide and DeepZoomGenerator for a slide."""
        if slide_id in wsi_cache:
            return wsi_cache[slide_id]
        
        # Try common WSI extensions
        slide_path = None
        for ext in [".svs", ".tiff", ".tif", ".ndpi", ".mrxs", ".vms", ".scn"]:
            candidate = slides_dir / f"{slide_id}{ext}"
            if candidate.exists():
                slide_path = candidate
                break
        
        if slide_path is None:
            return None
        
        try:
            import openslide
            from openslide.deepzoom import DeepZoomGenerator
            
            slide = openslide.OpenSlide(str(slide_path))
            # tile_size=254 with overlap=1 is standard for OpenSeadragon
            dz = DeepZoomGenerator(slide, tile_size=254, overlap=1, limit_bounds=True)
            wsi_cache[slide_id] = (slide, dz)
            logger.info(f"Loaded WSI: {slide_path}")
            return slide, dz
        except Exception as e:
            logger.error(f"Failed to load WSI {slide_path}: {e}")
            return None
    
    @app.get("/api/slides/{slide_id}/dzi")
    async def get_dzi_descriptor(slide_id: str):
        """Get Deep Zoom Image descriptor (XML) for OpenSeadragon."""
        result = get_slide_and_dz(slide_id)
        if result is None:
            raise HTTPException(
                status_code=404,
                detail=f"WSI file not found for slide {slide_id}"
            )
        
        slide, dz = result
        
        # Generate DZI XML descriptor
        dzi_xml = f'''<?xml version="1.0" encoding="UTF-8"?>
<Image xmlns="http://schemas.microsoft.com/deepzoom/2008"
  Format="jpeg"
  Overlap="1"
  TileSize="254">
  <Size Width="{dz.level_dimensions[-1][0]}" Height="{dz.level_dimensions[-1][1]}"/>
</Image>'''
        
        from fastapi.responses import Response
        return Response(
            content=dzi_xml,
            media_type="application/xml",
            headers={"Content-Disposition": f"inline; filename={slide_id}.dzi"}
        )
    
    @app.get("/api/slides/{slide_id}/dzi_files/{level}/{tile_spec}")
    async def get_dzi_tile(slide_id: str, level: int, tile_spec: str):
        """Serve a single DZI tile image."""
        result = get_slide_and_dz(slide_id)
        if result is None:
            raise HTTPException(
                status_code=404,
                detail=f"WSI file not found for slide {slide_id}"
            )
        
        slide, dz = result
        
        # Parse tile coordinates from spec like "0_0.jpeg"
        try:
            tile_name = tile_spec.rsplit(".", 1)[0]  # Remove extension
            col, row = map(int, tile_name.split("_"))
        except ValueError:
            raise HTTPException(status_code=400, detail="Invalid tile specification")
        
        # Validate level and tile coordinates
        if level < 0 or level >= dz.level_count:
            raise HTTPException(status_code=404, detail="Invalid zoom level")
        
        level_tiles = dz.level_tiles[level]
        if col < 0 or col >= level_tiles[0] or row < 0 or row >= level_tiles[1]:
            raise HTTPException(status_code=404, detail="Tile coordinates out of bounds")
        
        try:
            tile = dz.get_tile(level, (col, row))
            
            # Convert to JPEG
            buf = io.BytesIO()
            tile.save(buf, format="JPEG", quality=85)
            buf.seek(0)
            
            return StreamingResponse(
                buf,
                media_type="image/jpeg",
                headers={
                    "Cache-Control": "public, max-age=86400",
                    "Content-Disposition": f"inline; filename={level}_{col}_{row}.jpeg"
                }
            )
        except Exception as e:
            logger.error(f"Failed to get tile {level}/{col}_{row} for {slide_id}: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to generate tile: {e}")
    
    @app.get("/api/slides/{slide_id}/thumbnail")
    async def get_slide_thumbnail(slide_id: str, size: int = 512):
        """Get a thumbnail of the whole slide."""
        result = get_slide_and_dz(slide_id)
        if result is None:
            raise HTTPException(
                status_code=404,
                detail=f"WSI file not found for slide {slide_id}"
            )
        
        slide, dz = result
        
        try:
            # Get thumbnail maintaining aspect ratio
            thumb = slide.get_thumbnail((size, size))
            
            buf = io.BytesIO()
            thumb.save(buf, format="JPEG", quality=90)
            buf.seek(0)
            
            return StreamingResponse(
                buf,
                media_type="image/jpeg",
                headers={"Cache-Control": "public, max-age=3600"}
            )
        except Exception as e:
            logger.error(f"Failed to get thumbnail for {slide_id}: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to generate thumbnail: {e}")
    
    @app.get("/api/slides/{slide_id}/info")
    async def get_slide_info(slide_id: str):
        """Get detailed information about a WSI file."""
        result = get_slide_and_dz(slide_id)
        if result is None:
            # Return info even without WSI for embedding-only slides
            emb_path = embeddings_dir / f"{slide_id}.npy"
            if emb_path.exists():
                embeddings = np.load(emb_path)
                return {
                    "slide_id": slide_id,
                    "has_wsi": False,
                    "has_embeddings": True,
                    "num_patches": len(embeddings),
                }
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")
        
        slide, dz = result
        
        return {
            "slide_id": slide_id,
            "has_wsi": True,
            "dimensions": {
                "width": slide.dimensions[0],
                "height": slide.dimensions[1],
            },
            "level_count": slide.level_count,
            "level_dimensions": [list(d) for d in slide.level_dimensions],
            "properties": dict(slide.properties) if hasattr(slide, "properties") else {},
            "dzi": {
                "tile_size": 254,
                "overlap": 1,
                "level_count": dz.level_count,
            },
        }
    
    return app


# Default app instance
app = create_app()


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
