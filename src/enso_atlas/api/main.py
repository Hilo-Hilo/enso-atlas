"""
Enso Atlas API - FastAPI Backend

Provides REST API endpoints for the professional frontend:
- Slide analysis with MIL prediction
- Evidence generation (heatmaps, patches)
- Similar case retrieval (FAISS)
- Report generation (MedGemma)
- Patch embedding (Path Foundation)
- Analysis history and audit trail
"""

from pathlib import Path
from typing import Optional, List, Dict, Any
import asyncio
import logging
import json
import base64
import io
import time
from datetime import datetime
from .embedding_tasks import task_manager, TaskStatus, EmbeddingTask
from .report_tasks import report_task_manager, ReportTaskStatus, ReportTask
from .batch_tasks import batch_task_manager, BatchTaskStatus, BatchTask, BatchSlideResult
from .batch_embed_tasks import batch_embed_manager, BatchEmbedStatus, BatchEmbedSlideResult
from . import database as db
from .projects import ProjectRegistry
from .project_routes import router as project_router, set_registry as set_project_registry
from .model_scope import (
    filter_models_for_scope,
    is_model_allowed_for_scope,
    resolve_project_model_scope,
)
from .heatmap_grid import compute_heatmap_grid_coverage
from collections import deque

import numpy as np
from PIL import Image

from fastapi import FastAPI, HTTPException, UploadFile, File, BackgroundTasks, Query, Request
from .slide_metadata import SlideMetadataManager, create_metadata_router
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse, StreamingResponse, Response
from pydantic import BaseModel, Field

# PDF Export
try:
    from .pdf_export import generate_pdf_report, generate_report_pdf
    PDF_EXPORT_AVAILABLE = True
except ImportError as e:
    PDF_EXPORT_AVAILABLE = False
    generate_pdf_report = None
    generate_report_pdf = None

# Configure logging to show INFO level for our module (Python defaults to WARNING)
logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s %(name)s: %(message)s")
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# PDF Export (optional) - second import block kept for compatibility
if not PDF_EXPORT_AVAILABLE:
    try:
        from .pdf_export import generate_pdf_report, generate_report_pdf
        PDF_EXPORT_AVAILABLE = True
    except Exception as e:
        logger.warning(f"PDF export not available: {e}")
        PDF_EXPORT_AVAILABLE = False
        generate_pdf_report = None
        generate_report_pdf = None

# Multi-model TransMIL inference
import sys
_project_root = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(_project_root))
try:
    from scripts.multi_model_inference import MultiModelInference, MODEL_CONFIGS
    MULTI_MODEL_AVAILABLE = True
except ImportError:
    logger.warning("MultiModelInference not available - multi-model endpoints disabled")
    MULTI_MODEL_AVAILABLE = False
    MultiModelInference = None
    MODEL_CONFIGS = {}

# Agent workflow for multi-step analysis
try:
    from ..agent.workflow import AgentWorkflow
    from ..agent.routes import router as agent_router, set_agent_workflow
    AGENT_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Agent workflow not available: {e}")
    AGENT_AVAILABLE = False
    AgentWorkflow = None
    agent_router = None
    set_agent_workflow = None



# Analysis History Storage
# In-memory deque for fast access, limited to 100 entries
MAX_HISTORY_SIZE = 100
analysis_history: deque = deque(maxlen=MAX_HISTORY_SIZE)
audit_log: deque = deque(maxlen=500)  # Audit trail for compliance


def get_timestamp() -> str:
    """Get current ISO timestamp."""
    return datetime.utcnow().isoformat() + "Z"


# Track server startup time for uptime calculation
_STARTUP_TIME = time.time()
# Cache CUDA probe once at startup; avoid per-request GPU checks in health endpoint.
_CUDA_AVAILABLE_AT_STARTUP: Optional[bool] = None


def log_audit_event(
    event_type: str,
    slide_id: Optional[str] = None,
    user_id: str = "clinician",
    details: Optional[Dict[str, Any]] = None,
):
    """Log an audit event for compliance tracking."""
    entry = {
        "timestamp": get_timestamp(),
        "event_type": event_type,
        "user_id": user_id,
        "slide_id": slide_id,
        "details": details or {},
    }
    audit_log.append(entry)
    logger.info(f"AUDIT: {event_type} - slide={slide_id} user={user_id}")


def save_analysis_to_history(
    slide_id: str,
    prediction: str,
    score: float,
    confidence: float,
    patches_analyzed: int,
    top_evidence: List[Dict[str, Any]],
    similar_cases: List[Dict[str, Any]],
    user_id: str = "clinician",
) -> Dict[str, Any]:
    """Save analysis result to history and return the entry."""
    entry = {
        "id": f"{slide_id}_{datetime.utcnow().strftime('%Y%m%d%H%M%S%f')}",
        "timestamp": get_timestamp(),
        "slide_id": slide_id,
        "user_id": user_id,
        "prediction": prediction,
        "score": score,
        "confidence": confidence,
        "patches_analyzed": patches_analyzed,
        "top_evidence_count": len(top_evidence),
        "similar_cases_count": len(similar_cases),
    }
    analysis_history.append(entry)
    log_audit_event("analysis_completed", slide_id, user_id, {
        "prediction": prediction,
        "confidence": confidence,
    })
    return entry


def _check_cuda() -> bool:
    """Check if CUDA is available."""
    try:
        import torch
        return torch.cuda.is_available()
    except ImportError:
        return False


# Request/Response Models
class AnalyzeRequest(BaseModel):
    slide_id: str = Field(..., min_length=1, max_length=256)
    generate_report: bool = False
    project_id: Optional[str] = Field(None, description="Project ID to scope embeddings lookup")


class AnalyzeResponse(BaseModel):
    slide_id: str
    prediction: str
    score: float
    confidence: float
    patches_analyzed: int
    top_evidence: List[Dict[str, Any]]
    similar_cases: List[Dict[str, Any]]


class UncertaintyRequest(BaseModel):
    """Request for analysis with uncertainty quantification."""
    slide_id: str = Field(..., min_length=1, max_length=256)
    n_samples: int = Field(
        default=20,
        ge=5,
        le=50,
        description="Number of MC Dropout samples (5-50, default 20)"
    )


class UncertaintyResponse(BaseModel):
    """Response with MC Dropout uncertainty quantification."""
    slide_id: str
    prediction: str
    probability: float
    uncertainty: float
    confidence_interval: List[float]
    is_uncertain: bool
    requires_review: bool
    uncertainty_level: str  # "low", "moderate", "high"
    clinical_recommendation: str
    patches_analyzed: int
    n_samples: int
    samples: List[float]
    top_evidence: List[Dict[str, Any]]


class ReportRequest(BaseModel):
    slide_id: str = Field(..., min_length=1, max_length=256)
    include_evidence: bool = True
    include_similar: bool = True
    project_id: Optional[str] = Field(default=None, description="Project ID to determine cancer type for report")


class ReportResponse(BaseModel):
    slide_id: str
    report_json: Dict[str, Any]
    summary_text: str


class PatientContext(BaseModel):
    """Patient demographic and clinical context for a slide."""
    age: Optional[int] = None
    sex: Optional[str] = None
    stage: Optional[str] = None
    grade: Optional[str] = None
    prior_lines: Optional[int] = None
    histology: Optional[str] = None


class SlideDimensions(BaseModel):
    width: int = 0
    height: int = 0

class SlideInfo(BaseModel):
    slide_id: str
    patient_id: Optional[str] = None
    has_wsi: bool = False
    has_embeddings: bool = False
    has_level0_embeddings: bool = False  # Whether level 0 (full resolution) embeddings exist
    label: Optional[str] = None
    num_patches: Optional[int] = None
    patient: Optional[PatientContext] = None
    dimensions: SlideDimensions = SlideDimensions()
    mpp: Optional[float] = None  # microns per pixel
    magnification: Optional[str] = "40x"


class SlideQCResponse(BaseModel):
    """Quality control metrics for a slide."""
    slide_id: str
    tissue_coverage: float
    blur_score: float
    stain_uniformity: float
    artifact_detected: bool
    pen_marks: bool
    fold_detected: bool
    overall_quality: str


class EmbedRequest(BaseModel):
    """Request for patch embedding."""
    patches: List[str] = Field(
        ...,
        description="Base64-encoded patch images (224x224 RGB)",
        min_length=1,
        max_length=128,
    )
    return_embeddings: bool = Field(
        default=True,
        description="Whether to return embedding vectors",
    )


class EmbedResponse(BaseModel):
    """Response from embedding endpoint."""
    num_patches: int
    embedding_dim: int = 384
    embeddings: Optional[List[List[float]]] = None


class SimilarRequest(BaseModel):
    """Request for similar case search."""
    slide_id: str = Field(..., min_length=1, max_length=256)
    k: int = Field(default=5, ge=1, le=20)
    top_patches: int = Field(default=3, ge=1, le=10)


class BatchAnalyzeRequest(BaseModel):
    """Request for batch analysis of multiple slides."""
    slide_ids: List[str] = Field(
        ...,
        min_length=1,
        max_length=100,
        description="List of slide IDs to analyze (1-100 slides)"
    )
    project_id: Optional[str] = Field(default=None, description="Project ID to scope analysis")


class BatchAnalysisResult(BaseModel):
    """Result for a single slide in batch analysis."""
    slide_id: str
    prediction: str
    score: float
    confidence: float
    patches_analyzed: int
    requires_review: bool
    uncertainty_level: str = "unknown"
    error: Optional[str] = None


class BatchAnalysisSummary(BaseModel):
    """Summary statistics for batch analysis."""
    total: int
    completed: int
    failed: int
    responders: int
    non_responders: int
    uncertain: int
    avg_confidence: float
    requires_review_count: int


class BatchAnalyzeResponse(BaseModel):
    """Response from batch analysis endpoint."""
    results: List[BatchAnalysisResult]
    summary: BatchAnalysisSummary
    processing_time_ms: float


class ClassifyRegionRequest(BaseModel):
    """Request for tissue region classification."""
    x: int = Field(..., description="X coordinate of the region")
    y: int = Field(..., description="Y coordinate of the region")
    patch_index: Optional[int] = Field(None, description="Optional patch index for deterministic classification")


class ClassifyRegionResponse(BaseModel):
    """Response from tissue region classification."""
    tissue_type: str
    confidence: float
    description: str


class SimilarResponse(BaseModel):
    """Response from similar case search."""
    slide_id: str
    similar_cases: List[Dict[str, Any]]
    num_queries: int


class AnalysisHistoryEntry(BaseModel):
    """Single analysis history entry."""
    id: str
    timestamp: str
    slide_id: str
    user_id: str
    prediction: str
    score: float
    confidence: float
    patches_analyzed: int
    top_evidence_count: int
    similar_cases_count: int


class AnalysisHistoryResponse(BaseModel):
    """Response containing analysis history."""
    analyses: List[AnalysisHistoryEntry]
    total: int


class AuditLogEntry(BaseModel):
    """Single audit log entry."""
    timestamp: str
    event_type: str
    user_id: str
    slide_id: Optional[str] = None
    details: Dict[str, Any] = Field(default_factory=dict)


class AuditLogResponse(BaseModel):
    """Response containing audit log entries."""
    entries: List[AuditLogEntry]
    total: int


class SemanticSearchRequest(BaseModel):
    """Request for text-to-patch semantic search using MedSigLIP."""
    slide_id: str = Field(..., min_length=1, max_length=256)
    query: str = Field(..., min_length=1, max_length=512, description="Text query (e.g., 'tumor cells', 'lymphocytes')")
    top_k: int = Field(default=10, ge=1, le=50, description="Number of patches to return")
    project_id: Optional[str] = Field(default=None, description="Project ID to scope embeddings lookup")


class SemanticSearchResult(BaseModel):
    """Single result from semantic search."""
    patch_index: int
    similarity_score: float
    coordinates: Optional[List[int]] = None
    attention_weight: Optional[float] = None


class SemanticSearchResponse(BaseModel):
    """Response from semantic search endpoint."""
    slide_id: str
    query: str
    results: List[SemanticSearchResult]
    embedding_model: str = "siglip-so400m"


# ====== Visual Search (Image-to-Image) Models ======

class VisualSearchRequest(BaseModel):
    """Request for image-to-image visual similarity search using FAISS.
    
    Find histologically similar patches across the entire database.
    Can specify either:
    - patch_embedding: Direct embedding vector to search with
    - slide_id + patch_index: Look up embedding from stored data
    - slide_id + coordinates: Look up patch by its (x, y) location
    """
    slide_id: Optional[str] = Field(None, max_length=256, description="Source slide ID to look up patch embedding")
    patch_index: Optional[int] = Field(None, ge=0, description="Index of the patch in the source slide")
    coordinates: Optional[List[int]] = Field(None, description="[x, y] coordinates of the patch")
    patch_embedding: Optional[List[float]] = Field(None, description="Direct embedding vector (384-dim)")
    top_k: int = Field(default=10, ge=1, le=50, description="Number of similar patches to return")
    exclude_same_slide: bool = Field(default=True, description="Exclude patches from the same slide")


class VisualSearchResultPatch(BaseModel):
    """Single similar patch result from visual search."""
    slide_id: str
    patch_index: int
    coordinates: Optional[List[int]] = None
    distance: float
    similarity: float  # Converted from distance for easier interpretation
    label: Optional[str] = None  # Slide label if known (e.g., responder/non-responder)
    thumbnail_url: Optional[str] = None


class VisualSearchResponse(BaseModel):
    """Response from visual similarity search."""
    query_slide_id: Optional[str] = None
    query_patch_index: Optional[int] = None
    query_coordinates: Optional[List[int]] = None
    results: List[VisualSearchResultPatch]
    total_patches_searched: int
    search_time_ms: float


# ====== Multi-Model Prediction Models ======

class ModelPrediction(BaseModel):
    """Single model prediction result."""
    model_id: str
    model_name: str
    category: str  # cancer-specific category or general_pathology
    score: float
    label: str
    positive_label: str
    negative_label: str
    confidence: float
    auc: float
    n_training_slides: int
    description: str
    warning: Optional[str] = None


class MultiModelRequest(BaseModel):
    """Request for multi-model analysis."""
    slide_id: str = Field(..., min_length=1, max_length=256)
    models: Optional[List[str]] = None  # None = run all models
    project_id: Optional[str] = Field(default=None, description="Project ID to scope models to project's classification_models")
    return_attention: bool = False
    level: int = Field(default=0, ge=0, le=0, description="Resolution level is fixed to 0 (full resolution, dense)")
    force: bool = Field(default=False, description="Bypass cache and force fresh analysis")


class MultiModelResponse(BaseModel):
    """Response with predictions from multiple models."""
    slide_id: str
    predictions: Dict[str, ModelPrediction]
    by_category: Dict[str, List[ModelPrediction]]
    n_patches: int
    processing_time_ms: float
    warnings: Optional[List[str]] = None


class PdfExportRequest(BaseModel):
    """Request for PDF report export."""
    slide_id: str = Field(..., min_length=1, max_length=256)
    report_data: Dict[str, Any] = Field(..., description="Structured report from MedGemma")
    prediction_data: Dict[str, Any] = Field(..., description="Model prediction results")
    include_heatmap: bool = Field(default=True, description="Include attention heatmap image")
    include_evidence_patches: bool = Field(default=True, description="Include evidence patch images")
    patient_context: Optional[Dict[str, Any]] = Field(default=None, description="Patient demographic info")


class AvailableModelsResponse(BaseModel):
    """Response listing available models."""
    models: List[Dict[str, Any]]


class PatchClassifyRequest(BaseModel):
    """Request for few-shot patch classification."""
    classes: Dict[str, List[int]]  # class_name -> list of patch indices


class PatchClassificationItem(BaseModel):
    """Single patch classification result."""
    patch_idx: int
    x: int
    y: int
    predicted_class: str
    confidence: float
    probabilities: Dict[str, float]


class PatchClassifyResponse(BaseModel):
    """Response from few-shot patch classification."""
    slide_id: str
    classes: List[str]
    total_patches: int
    predictions: List[PatchClassificationItem]
    class_counts: Dict[str, int]
    accuracy_estimate: Optional[float]
    heatmap_data: List[Dict[str, Any]]


class OutlierPatch(BaseModel):
    """Single outlier patch result."""
    patch_idx: int
    x: int
    y: int
    distance: float
    z_score: float


class OutlierDetectionResponse(BaseModel):
    """Response from outlier tissue detection."""
    slide_id: str
    outlier_patches: List[OutlierPatch]
    total_patches: int
    outlier_count: int
    mean_distance: float
    std_distance: float
    threshold: float
    heatmap_data: List[Dict[str, Any]]


def create_app(
    embeddings_dir: Path = Path("data/embeddings"),
    model_path: Path = Path("models/clam_attention.pt"),
    enable_cors: bool = True,
) -> FastAPI:
    """Create and configure the FastAPI application."""

    app = FastAPI(
        title="Enso Atlas API",
        description="On-Prem Pathology Evidence Engine for Treatment-Response Insight",
        version="0.1.0",
        docs_url="/api/docs",
        redoc_url="/api/redoc",
    )

    # CORS middleware for frontend development
    if enable_cors:
        app.add_middleware(
            CORSMiddleware,
            allow_origins=["*", 
                "http://localhost:3000",
                "http://localhost:3001",
                "http://localhost:7860",
                "http://100.111.126.23:3000",
                "http://100.111.126.23:8003",
                "http://100.111.126.23:3002",
            ],
            allow_credentials=False,
            allow_methods=["*"],
            allow_headers=["*"],
        )

    # Load models on startup
    classifier = None
    evidence_gen = None
    embedder = None
    medsiglip_embedder = None
    reporter = None
    decision_support = None  # Clinical decision support engine
    multi_model_inference = None  # Multi-model TransMIL inference
    slide_siglip_embeddings = {}  # Cache for MedSigLIP embeddings per slide
    available_slides = []
    slide_labels = {}  # Cache for slide labels (slide_id -> label string)
    db_available = False  # Whether PostgreSQL is connected and populated
    project_registry: Optional[ProjectRegistry] = None  # Loaded at startup from config/projects.yaml
    # Slide-level mean-embedding FAISS index (cosine similarity)
    slide_mean_index = None  # faiss.IndexFlatIP over L2-normalized mean embeddings
    slide_mean_ids: list[str] = []
    slide_mean_meta: dict[str, dict] = {}  # slide_id -> metadata (n_patches, label, patient, etc.)
    # Directories (may be updated at startup if we fall back to demo data)
    # Data root is always "data/" regardless of embeddings subdirectory (e.g. data/embeddings/level0)
    _data_root: Path = Path("data")
    slides_dir: Path = _data_root / "slides"

    def _resolve_dataset_path(path_str: str | Path) -> Path:
        """Resolve a dataset path from config (repo-relative or absolute)."""
        p = Path(path_str)
        if p.is_absolute():
            return p
        return _data_root.parent / p

    def _project_labels_path(project_id: Optional[str]) -> Path | None:
        """Resolve the configured labels file for a project, if available."""
        if not project_id or not project_registry:
            return None
        proj_cfg = project_registry.get_project(project_id)
        if not proj_cfg or not getattr(proj_cfg, "dataset", None):
            return None
        try:
            return _resolve_dataset_path(proj_cfg.dataset.labels_file)
        except Exception:
            return None

    def resolve_slide_path(slide_id: str, project_id: Optional[str] = None) -> Path | None:
        """Resolve slide file path across possible slide directories.

        If project_id is provided, search ONLY that project's configured slides_dir
        for deterministic project-scoped behavior.
        """
        candidates_dirs: list[Path] = []

        if project_id:
            if not project_registry:
                return None
            proj_cfg = project_registry.get_project(project_id)
            if not proj_cfg:
                return None
            try:
                candidates_dirs.append(_resolve_dataset_path(proj_cfg.dataset.slides_dir))
            except Exception:
                return None
        else:
            # Global/default common locations
            candidates_dirs.extend([
                slides_dir,
                _data_root / 'tcga_full' / 'slides',
                _data_root / 'ovarian_bev' / 'slides',
                _data_root / 'demo' / 'slides',
                _data_root / 'slides',
            ])

            # Any project-configured slides dirs as a final fallback
            if project_registry:
                try:
                    for _pid, _cfg in project_registry.list_projects().items():
                        p = _resolve_dataset_path(_cfg.dataset.slides_dir)
                        if p not in candidates_dirs:
                            candidates_dirs.append(p)
                except Exception:
                    pass

        exts = [".svs", ".tiff", ".tif", ".ndpi", ".mrxs", ".vms", ".scn"]
        for d in candidates_dirs:
            if not d.exists():
                continue
            for ext in exts:
                cand = d / f"{slide_id}{ext}"
                if cand.exists():
                    return cand
        return None

    def has_wsi_file(slide_id: str, project_id: Optional[str] = None) -> bool:
        return resolve_slide_path(slide_id, project_id=project_id) is not None

    def _require_project(project_id: Optional[str]):
        """Validate and return project config when project_id is supplied."""
        if not project_id:
            return None
        if not project_registry:
            raise HTTPException(
                status_code=503,
                detail="Project registry not available",
            )
        proj_cfg = project_registry.get_project(project_id)
        if not proj_cfg:
            raise HTTPException(status_code=404, detail=f"Project '{project_id}' not found")
        return proj_cfg

    def _resolve_project_embeddings_dir(
        project_id: Optional[str],
        *,
        require_exists: bool = False,
    ) -> Path:
        """Resolve embeddings dir for a project, defaulting to global embeddings_dir."""
        proj_cfg = _require_project(project_id)
        if not proj_cfg:
            return embeddings_dir

        proj_emb_dir = Path(proj_cfg.dataset.embeddings_dir)
        if not proj_emb_dir.is_absolute():
            proj_emb_dir = _data_root.parent / proj_emb_dir

        if require_exists and not proj_emb_dir.exists():
            raise HTTPException(
                status_code=404,
                detail=f"Embeddings directory not found for project '{project_id}'",
            )
        return proj_emb_dir

    def _resolve_project_label_pair(
        project_id: Optional[str],
        *,
        positive_default: str,
        negative_default: str,
        uppercase: bool = False,
    ) -> tuple[str, str]:
        """Resolve positive/negative labels for a project with sensible defaults."""
        pos_label = positive_default
        neg_label = negative_default

        proj_cfg = _require_project(project_id)
        if proj_cfg and proj_cfg.classes:
            pos_label = proj_cfg.positive_class if proj_cfg.positive_class else proj_cfg.classes[-1]
            neg_candidates = [c for c in proj_cfg.classes if c.lower() != pos_label.lower()]
            neg_label = neg_candidates[0] if neg_candidates else negative_default

        if uppercase:
            pos_label = pos_label.upper()
            neg_label = neg_label.upper()

        return pos_label, neg_label

    def _project_slide_ids(project_id: Optional[str]) -> Optional[set[str]]:
        """Return project-local slide IDs from that project's embeddings directory.

        Returns None when project_id is not provided.
        """
        if not project_id:
            return None

        proj_emb_dir = _resolve_project_embeddings_dir(project_id, require_exists=True)
        if not proj_emb_dir.exists():
            return set()

        slide_ids = set()
        for f in proj_emb_dir.glob("*.npy"):
            if not f.name.endswith("_coords.npy"):
                slide_ids.add(f.stem)
        return slide_ids

    async def _resolve_project_model_ids(project_id: Optional[str]) -> Optional[set[str]]:
        """Resolve allowed model ids for a project (DB first, YAML fallback)."""
        if not project_id:
            return None

        proj_cfg = _require_project(project_id)
        allowed: set[str] = set()

        try:
            allowed.update(await db.get_project_models(project_id))
        except Exception as e:
            logger.warning(f"DB model query failed for {project_id}: {e}")

        if not allowed and proj_cfg and proj_cfg.classification_models:
            allowed.update(proj_cfg.classification_models)

        return allowed

    def _active_batch_embed_info() -> Optional[Dict[str, Any]]:
        """Return lightweight info for the active batch embedding task, if any."""
        active = batch_embed_manager.get_active_task()
        if not active:
            return None
        return {
            "task_id": active.task_id,
            "status": active.status.value,
            "progress": round(active.progress, 1),
            "current_slide_index": active.current_slide_index,
            "total_slides": active.total_slides,
            "current_slide_id": active.current_slide_id,
            "message": active.message,
        }


    @app.on_event("startup")
    async def load_models():
        nonlocal classifier, evidence_gen, embedder, medsiglip_embedder, reporter, decision_support, multi_model_inference, available_slides, slide_labels, slides_dir, embeddings_dir, slide_mean_index, slide_mean_ids, slide_mean_meta, project_registry
        global _CUDA_AVAILABLE_AT_STARTUP

        from ..config import MILConfig, EvidenceConfig, EmbeddingConfig
        from ..mil.clam import CLAMClassifier, create_classifier
        from ..evidence.generator import EvidenceGenerator
        from ..embedding.embedder import PathFoundationEmbedder
        from ..embedding.medsiglip import MedSigLIPEmbedder, MedSigLIPConfig
        from ..reporting.medgemma import MedGemmaReporter, ReportingConfig
        from ..reporting.decision_support import ClinicalDecisionSupport


        def _count_embeddings(p: Path) -> int:
            if not p.exists() or not p.is_dir():
                return 0
            return sum(1 for f in p.glob('*.npy') if not f.name.endswith('_coords.npy'))

        def _count_slides(p: Path) -> int:
            if not p.exists() or not p.is_dir():
                return 0
            exts = {'.svs', '.tiff', '.tif', '.ndpi', '.mrxs', '.vms', '.scn'}
            return sum(1 for f in p.iterdir() if f.is_file() and f.suffix.lower() in exts)

        if _CUDA_AVAILABLE_AT_STARTUP is None:
            _CUDA_AVAILABLE_AT_STARTUP = _check_cuda()

        primary_embeddings_dir = embeddings_dir
        primary_slides_dir = slides_dir
        primary_n = _count_embeddings(primary_embeddings_dir)
        primary_s = _count_slides(primary_slides_dir)
        logger.info(f'Embeddings dir: {primary_embeddings_dir} (npy={primary_n})')
        logger.info(f'Slides dir: {primary_slides_dir} (slides={primary_s})')

        if primary_n == 0:
            demo_embeddings_dir = primary_embeddings_dir.parent / 'demo' / 'embeddings'
            demo_slides_dir = demo_embeddings_dir.parent / 'slides'
            demo_n = _count_embeddings(demo_embeddings_dir)
            demo_s = _count_slides(demo_slides_dir)
            if demo_n > 0:
                logger.warning(
                    f'No embeddings found in {primary_embeddings_dir}; falling back to demo embeddings at {demo_embeddings_dir} (npy={demo_n})'
                )
                embeddings_dir = demo_embeddings_dir
                slides_dir = demo_slides_dir
                logger.info(f'Using embeddings dir: {embeddings_dir}')
                logger.info(f'Using slides dir: {slides_dir} (slides={demo_s})')
            else:
                logger.warning(
                    f'No embeddings found in {primary_embeddings_dir} and no demo embeddings found at {demo_embeddings_dir}.'
                )

        # Load MIL classifier (architecture from env or default to clam)
        import os as _os
        mil_arch = _os.environ.get("MIL_ARCHITECTURE", "clam")
        mil_threshold_str = _os.environ.get("MIL_THRESHOLD", "")
        mil_threshold = float(mil_threshold_str) if mil_threshold_str else None
        threshold_cfg_path = _os.environ.get(
            "MIL_THRESHOLD_CONFIG",
            str(model_path.parent / "threshold_config.json"),
        )

        # Resolve model checkpoint: use architecture-specific file if it exists
        mil_model_path_env = _os.environ.get("MIL_MODEL_PATH", "")
        if mil_model_path_env:
            mil_model_path = Path(mil_model_path_env)
        elif mil_arch == "transmil":
            candidate = model_path.parent / "transmil_best.pt"
            mil_model_path = candidate if candidate.exists() else model_path
        else:
            mil_model_path = model_path

        config = MILConfig(
            input_dim=384,
            hidden_dim=512,
            architecture=mil_arch,
            threshold=mil_threshold,
            threshold_config_path=threshold_cfg_path,
        )
        classifier = create_classifier(config)
        if mil_model_path.exists():
            classifier.load(mil_model_path)
            logger.info(
                "Loaded MIL model (%s) from %s  [threshold=%.4f]",
                mil_arch, mil_model_path, classifier.threshold,
            )

        # Initialize multi-model TransMIL inference
        multi_model_inference = None
        if MULTI_MODEL_AVAILABLE:
            outputs_dir = Path(__file__).parent.parent.parent.parent / "outputs"
            if outputs_dir.exists():
                try:
                    multi_model_inference = MultiModelInference(
                        models_dir=outputs_dir,
                        device="auto",
                        load_all=True,
                    )
                    logger.info(f"Multi-model inference initialized with {len(multi_model_inference.models)} models")
                except Exception as e:
                    logger.warning(f"Failed to initialize multi-model inference: {e}")
            else:
                logger.warning(f"Outputs directory not found: {outputs_dir}")
        else:
            logger.warning("Multi-model inference not available (missing dependencies)")

        # Setup evidence generator
        evidence_config = EvidenceConfig()
        evidence_gen = EvidenceGenerator(evidence_config)

        # Setup embedder (lazy-loaded on first use)
        embedding_config = EmbeddingConfig()
        embedder = PathFoundationEmbedder(embedding_config)

        # Setup MedGemma reporter (lazy-loaded on first use)
        reporting_config = ReportingConfig()
        reporter = MedGemmaReporter(reporting_config)
        logger.info("MedGemma reporter initialized, warming up model...")

        # Warm up MedGemma model during startup to avoid timeout on first call
        # This runs a test inference to ensure CUDA kernels are loaded
        if reporter is not None:
            try:
                logger.info("Starting MedGemma warmup (may take ~60-120s for GPU kernel compilation)...")
                warmup_start = time.time()
                await asyncio.wait_for(
                    asyncio.to_thread(reporter._warmup_inference),
                    timeout=180.0,
                )
                warmup_duration = time.time() - warmup_start
                logger.info(f"MedGemma reporter warmed up successfully in {warmup_duration:.1f}s")
            except asyncio.TimeoutError:
                warmup_duration = time.time() - warmup_start
                logger.warning(f"MedGemma warmup timed out after {warmup_duration:.1f}s, continuing startup")
            except Exception as e:
                warmup_duration = time.time() - warmup_start
                logger.warning(f"MedGemma warmup failed after {warmup_duration:.1f}s: {e}")

        # Setup clinical decision support engine
        decision_support = ClinicalDecisionSupport()
        logger.info("Clinical decision support engine initialized")

        # Setup MedSigLIP embedder for semantic search
        # Share GPU with MedGemma â€” SigLIP is ~800MB fp16, fits alongside MedGemma 4B.
        # GPU makes semantic search 10-50x faster (seconds vs minutes for 6000+ patches).
        siglip_config = MedSigLIPConfig(
            cache_dir=str(embeddings_dir / "medsiglip_cache"),
            device="auto",
        )
        medsiglip_embedder = MedSigLIPEmbedder(siglip_config)
        # Load MedSigLIP model on startup to enable semantic search immediately
        try:
            logger.info("Loading MedSigLIP model on startup...")
            medsiglip_embedder._load_model()
            logger.info("MedSigLIP model loaded successfully")
        except Exception as e:
            logger.warning(f"MedSigLIP model loading failed: {e}")

        # Find available slides and build FAISS index
        all_embeddings = []
        all_metadata = []

        if embeddings_dir.exists():
            for f in sorted(embeddings_dir.glob("*.npy")):
                if not f.name.endswith("_coords.npy"):
                    slide_id = f.stem
                    available_slides.append(slide_id)

                    # Load embeddings for FAISS index
                    embs = np.load(f)
                    all_embeddings.append(embs)
                    all_metadata.append({
                        "slide_id": slide_id,
                        "n_patches": len(embs),
                    })

            logger.info(f"Found {len(available_slides)} slides with embeddings")

            # Build FAISS index for similarity search
            if all_embeddings:
                evidence_gen.build_reference_index(all_embeddings, all_metadata)
                logger.info(f"Built FAISS index with {len(all_embeddings)} slides")

            # Build slide-level mean-embedding FAISS index for similar-case retrieval
            try:
                import faiss
                means = []
                slide_mean_ids = []
                slide_mean_meta = {}
                for embs, meta in zip(all_embeddings, all_metadata):
                    sid = meta.get('slide_id')
                    if sid is None:
                        continue
                    if embs is None or len(embs) == 0:
                        continue
                    mean = np.asarray(embs, dtype=np.float32).mean(axis=0)
                    mean = mean / (np.linalg.norm(mean) + 1e-12)
                    means.append(mean)
                    slide_mean_ids.append(sid)
                    slide_mean_meta[sid] = {
                        'slide_id': sid,
                        'n_patches': int(meta.get('n_patches', len(embs))),
                    }
                if means:
                    mat = np.vstack(means).astype(np.float32)
                    slide_mean_index = faiss.IndexFlatIP(mat.shape[1])
                    slide_mean_index.add(mat)
                    logger.info(f'Built slide-mean FAISS index with {len(slide_mean_ids)} slides')
                else:
                    slide_mean_index = None
                    logger.warning('No slide means available to build slide-mean FAISS index')
            except Exception as e:
                slide_mean_index = None
                logger.warning(f'Failed to build slide-mean FAISS index: {e}')

        # Load slide labels from labels.csv for similar case retrieval
        # Check multiple label files (primary + tcga_full)
        label_files = [
            _data_root / "labels.csv",
            _data_root / "tcga_full" / "labels.csv",
            embeddings_dir.parent / "labels.csv",
        ]
        # Build a prefix->full_slide_id lookup for matching short names to full IDs
        prefix_to_slide_ids: dict[str, list[str]] = {}
        for sid in available_slides:
            # Extract prefix before UUID: "TCGA-04-1331-01A-01-BS1.uuid" -> "TCGA-04-1331-01A-01-BS1"
            parts = sid.split('.')
            if len(parts) >= 2:
                prefix = parts[0]
            else:
                prefix = sid
            prefix_to_slide_ids.setdefault(prefix, []).append(sid)

        for labels_path in label_files:
            if not labels_path.exists():
                continue
            import csv
            with open(labels_path) as f:
                reader = csv.DictReader(f)
                for row in reader:
                    if "slide_id" in row:
                        sid = row["slide_id"]
                        label_val = row.get("label", "")
                    else:
                        slide_file = row.get("slide_file", "")
                        sid = slide_file.replace(".svs", "").replace(".SVS", "")
                        response = row.get("treatment_response", "")
                        label_val = "responder" if response == "responder" else "non-responder" if response == "non-responder" else ""

                    # Normalize label format
                    if label_val == "1":
                        label_val = "responder"
                    elif label_val == "0":
                        label_val = "non-responder"
                    # Also check platinum_status column
                    if not label_val:
                        platinum = row.get("platinum_status", "")
                        if platinum == "sensitive":
                            label_val = "responder"
                        elif platinum == "resistant":
                            label_val = "non-responder"

                    if not (sid and label_val):
                        continue

                    # Direct match (full slide ID in CSV)
                    if sid in available_slides or '.' in sid:
                        slide_labels[sid] = label_val
                    # Prefix match: short slide name -> all matching full IDs
                    for full_sid in prefix_to_slide_ids.get(sid, []):
                        if full_sid not in slide_labels:
                            slide_labels[full_sid] = label_val

        logger.info(f"Loaded labels for {len(slide_labels)} slides")

        # Attach labels to slide-mean metadata
        try:
            for sid, lab in slide_labels.items():
                if sid in slide_mean_meta:
                    slide_mean_meta[sid]['label'] = lab
        except Exception as e:
            logger.warning(f'Failed to attach labels to slide-mean metadata: {e}')


        # Initialize PostgreSQL database (creates tables, populates from flat files on first run)
        nonlocal db_available
        try:
            logger.info("Initializing PostgreSQL database...")
            await db.init_schema()
            already_populated = await db.is_populated()
            if not already_populated:
                logger.info("Database is empty, populating from flat files (this may take a few minutes on first run)...")
                await db.populate_from_flat_files(
                    data_root=_data_root,
                    embeddings_dir=embeddings_dir,
                )
            else:
                logger.info("Database already populated, skipping flat-file import")
            db_available = True
            logger.info("PostgreSQL database ready")
        except Exception as e:
            logger.warning(f"PostgreSQL not available, falling back to flat-file mode: {e}")
            db_available = False

        # Load project registry from YAML config
        try:
            _projects_yaml = Path("config/projects.yaml")
            if _projects_yaml.exists():
                _project_registry = ProjectRegistry(_projects_yaml)
                project_registry = _project_registry
                set_project_registry(_project_registry)
                logger.info(f"Project registry loaded: {list(_project_registry.list_projects().keys())}")
                # Sync projects to database
                if db_available:
                    try:
                        await db.populate_projects_from_registry(_project_registry)
                    except Exception as e:
                        logger.warning(f"Failed to sync projects to database: {e}")
            else:
                logger.warning("config/projects.yaml not found, project system disabled")
        except Exception as e:
            logger.warning(f"Failed to load project registry: {e}")

        # Initialize agent workflow now that models and indexes are ready
        if AGENT_AVAILABLE:
            try:
                agent_workflow = AgentWorkflow(
                    embeddings_dir=embeddings_dir,
                    multi_model_inference=multi_model_inference,
                    evidence_generator=evidence_gen,
                    medgemma_reporter=reporter,
                    medsiglip_embedder=medsiglip_embedder,
                    slide_labels=slide_labels,
                    slide_mean_index=slide_mean_index,
                    slide_mean_ids=slide_mean_ids,
                    slide_mean_meta=slide_mean_meta,
                )
                set_agent_workflow(agent_workflow)
                logger.info("Agent workflow initialized")
            except Exception as e:
                logger.warning(f"Failed to initialize agent workflow: {e}")

    @app.on_event("shutdown")
    async def shutdown():
        """Clean up resources on shutdown."""
        try:
            await db.close_pool()
        except Exception:
            pass

    @app.get("/health")
    async def health_check():
        """Health check endpoint."""
        # Build per-project slide counts if registry is available
        slides_by_project = {}
        if project_registry and hasattr(project_registry, 'list_projects'):
            projects_dict = project_registry.list_projects()  # Dict[str, ProjectConfig]
            for pid in projects_dict:
                try:
                    proj_cfg = projects_dict[pid]
                    if proj_cfg and hasattr(proj_cfg, 'dataset') and proj_cfg.dataset:
                        proj_emb_dir = Path(proj_cfg.dataset.embeddings_dir)
                        if not proj_emb_dir.is_absolute():
                            proj_emb_dir = _data_root.parent / proj_emb_dir
                        if proj_emb_dir.exists():
                            slides_by_project[pid] = len([f for f in proj_emb_dir.glob("*.npy") if not f.name.endswith("_coords.npy")])
                except Exception:
                    pass
        total_slides = sum(slides_by_project.values()) if slides_by_project else len(available_slides)
        active_batch_embedding = _active_batch_embed_info()
        return {
            "status": "healthy",
            "version": "0.1.0",
            "model_loaded": classifier is not None,
            "cuda_available": bool(_CUDA_AVAILABLE_AT_STARTUP),
            "slides_available": total_slides,
            "slides_by_project": slides_by_project if slides_by_project else None,
            "db_available": db_available,
            "uptime": time.time() - _STARTUP_TIME,
            "active_batch_embedding": active_batch_embedding,
        }

    # ====== Tags & Groups stubs ======
    # These endpoints are called by the Slide Manager frontend.
    # Return empty arrays until full implementation is needed.

    @app.get("/api/tags")
    async def get_tags():
        """List all tags (stub)."""
        return []

    @app.post("/api/tags")
    async def create_tag(request: dict):
        """Create a tag (stub)."""
        return {"name": request.get("name", ""), "color": request.get("color", "#888"), "count": 0}

    @app.get("/api/groups")
    async def get_groups():
        """List all slide groups (stub)."""
        return []

    @app.post("/api/groups")
    async def create_group(request: dict):
        """Create a slide group (stub)."""
        import uuid
        return {"id": str(uuid.uuid4()), "name": request.get("name", ""), "description": request.get("description", ""), "slide_ids": [], "created_at": "", "updated_at": ""}

    @app.get("/api/health")
    async def api_health_check():
        """Health check endpoint (aliased for frontend compatibility)."""
        return await health_check()

    @app.get("/")
    async def root():
        """API root endpoint."""
        return {
            "name": "Enso Atlas API",
            "version": "0.1.0",
            "docs": "/api/docs",
        }

    @app.get("/docs", include_in_schema=False)
    async def docs_redirect():
        """Redirect /docs to /api/docs for convenience."""
        from starlette.responses import RedirectResponse
        return RedirectResponse(url="/api/docs")

    @app.get("/redoc", include_in_schema=False)
    async def redoc_redirect():
        """Redirect /redoc to /api/redoc for convenience."""
        from starlette.responses import RedirectResponse
        return RedirectResponse(url="/api/redoc")

    @app.get("/api/slides", response_model=List[SlideInfo])
    async def list_slides(project_id: Optional[str] = Query(None, description="Filter slides by project")):
        """List all available slides with patient context.

        Uses PostgreSQL when available (< 100ms), falls back to flat-file
        scan (30-60s) if DB is not connected.

        When project_id is provided, only returns slides assigned to that project
        via the project_slides junction table.
        """
        proj_cfg = _require_project(project_id)

        # ---- Fast path: PostgreSQL ----
        # For project-scoped requests we intentionally bypass DB rows because
        # patch counts can lag behind active re-embedding jobs. Flat-file scan
        # reads live embedding arrays from the project's directory.
        if db_available and not project_id:
            try:
                t0 = time.time()
                rows = await db.get_all_slides()
                slides = []
                for r in rows:
                    patient_ctx = None
                    if any(r.get(k) for k in ("age", "sex", "stage", "grade", "prior_lines", "histology")):
                        patient_ctx = PatientContext(
                            age=r.get("age"),
                            sex=r.get("sex"),
                            stage=r.get("stage"),
                            grade=r.get("grade"),
                            prior_lines=r.get("prior_lines"),
                            histology=r.get("histology"),
                        )
                    slides.append(SlideInfo(
                        slide_id=r["slide_id"],
                        patient_id=r.get("patient_id"),
                        has_wsi=has_wsi_file(r["slide_id"], project_id=project_id),
                        has_embeddings=r.get("has_embeddings", False),
                        has_level0_embeddings=r.get("has_level0_embeddings", False),
                        label=r.get("label"),
                        num_patches=r.get("num_patches"),
                        patient=patient_ctx,
                        dimensions=SlideDimensions(
                            width=r.get("width") or 0,
                            height=r.get("height") or 0,
                        ),
                        mpp=r.get("mpp"),
                        magnification=r.get("magnification") or "40x",
                    ))
                elapsed_ms = (time.time() - t0) * 1000
                logger.info(f"/api/slides returned {len(slides)} slides from DB in {elapsed_ms:.0f}ms")
                return slides
            except Exception as e:
                logger.warning(f"DB query failed, falling back to flat-file scan: {e}")
        # project validity is enforced above via _require_project(project_id)

        # ---- Slow fallback: flat-file scan (with caching) ----
        # Cache flat-file scan results per project for 60s to avoid re-scanning
        _cache_key = project_id or "__global__"
        if not hasattr(list_slides, "_cache"):
            list_slides._cache = {}
        _cached = list_slides._cache.get(_cache_key)
        if _cached and (time.time() - _cached["ts"]) < 60:
            logger.info(f"Returning cached flat-file scan for {_cache_key} ({len(_cached['data'])} slides)")
            return _cached["data"]

        # When project_id is given, determine project-specific embeddings/slides dirs.
        _fallback_embeddings_dir = embeddings_dir
        _fallback_slides_dir = slides_dir
        if proj_cfg:
            _fallback_embeddings_dir = _resolve_dataset_path(proj_cfg.dataset.embeddings_dir)
            _fallback_slides_dir = _resolve_dataset_path(proj_cfg.dataset.slides_dir)

        slides = []
        if proj_cfg:
            labels_path = _project_labels_path(project_id)
        else:
            labels_path = _data_root / "labels.csv"
            if not labels_path.exists():
                labels_path = _data_root / "tcga_full" / "labels.csv"
            if not labels_path.exists():
                labels_path = embeddings_dir.parent / "labels.csv"

        slide_data = {}

        if labels_path and labels_path.exists():
            if labels_path.suffix.lower() == ".json":
                try:
                    with open(labels_path) as f:
                        labels_json = json.load(f)
                    if isinstance(labels_json, dict):
                        for sid, label in labels_json.items():
                            slide_data[str(sid)] = {
                                "label": str(label),
                                "patient": None,
                            }
                except Exception as e:
                    logger.warning(f"Could not parse labels JSON {labels_path}: {e}")
            else:
                import csv
                with open(labels_path) as f:
                    reader = csv.DictReader(f)
                    for row in reader:
                        if "slide_id" in row:
                            sid = row["slide_id"]
                            label = row.get("label", "")
                        else:
                            slide_file = row.get("slide_file", "")
                            sid = slide_file.replace(".svs", "").replace(".SVS", "")
                            response = row.get("treatment_response", "")
                            label = "1" if response == "responder" else "0" if response == "non-responder" else ""

                        if sid:
                            patient_ctx = None
                            if any(k in row for k in ["age", "sex", "stage", "grade", "prior_treatments", "histology"]):
                                try:
                                    age_val = row.get("age")
                                    prior_val = row.get("prior_treatments")
                                    patient_ctx = PatientContext(
                                        age=int(age_val) if age_val else None,
                                        sex=row.get("sex") or None,
                                        stage=row.get("stage") or None,
                                        grade=row.get("grade") or None,
                                        prior_lines=int(prior_val) if prior_val else None,
                                        histology=row.get("histology") or None,
                                    )
                                except (ValueError, TypeError):
                                    patient_ctx = None

                            slide_data[sid] = {
                                "label": label,
                                "patient": patient_ctx,
                            }

        # When filtering by project, only include slides whose embeddings
        # exist in the project's embeddings directory.
        if proj_cfg:
            # Scan the project-specific embeddings directory
            _project_slide_ids = []
            if _fallback_embeddings_dir.exists():
                for f in sorted(_fallback_embeddings_dir.glob("*.npy")):
                    if not f.name.endswith("_coords.npy"):
                        _project_slide_ids.append(f.stem)
            fallback_slide_ids = _project_slide_ids
        else:
            fallback_slide_ids = available_slides

        for slide_id in fallback_slide_ids:
            emb_path = _fallback_embeddings_dir / f"{slide_id}.npy"
            num_patches = None
            if emb_path.exists():
                try:
                    emb = np.load(emb_path)
                    num_patches = len(emb)
                except Exception:
                    pass

            data = slide_data.get(slide_id, {})
            dims = SlideDimensions()
            mpp = None
            slide_path = resolve_slide_path(slide_id, project_id=project_id)
            if slide_path is not None and slide_path.exists():
                try:
                    import openslide
                    with openslide.OpenSlide(str(slide_path)) as slide:
                        dims = SlideDimensions(
                            width=slide.dimensions[0],
                            height=slide.dimensions[1]
                        )
                        mpp_x = slide.properties.get(openslide.PROPERTY_NAME_MPP_X)
                        if mpp_x:
                            mpp = float(mpp_x)
                except Exception as e:
                    logger.warning(f"Could not read slide {slide_id}: {e}")
            elif num_patches is not None and num_patches > 0:
                # No WSI on disk â€” estimate dimensions from patch count.
                # Assume a roughly square grid of 256Ã—256 px patches (level 0).
                import math
                grid_side = int(math.ceil(math.sqrt(num_patches)))
                estimated_px = grid_side * 256
                dims = SlideDimensions(width=estimated_px, height=estimated_px)

            has_level0 = False
            if _fallback_embeddings_dir.name == "level0":
                emb_check = _fallback_embeddings_dir / f"{slide_id}.npy"
                has_level0 = emb_check.exists()
            else:
                level0_dir = _fallback_embeddings_dir / "level0"
                if level0_dir.exists():
                    has_level0 = (level0_dir / f"{slide_id}.npy").exists()
                elif proj_cfg:
                    # Project-specific embeddings dir (e.g. data/projects/<project>/embeddings)
                    # that isn't named "level0" and has no level0/ subdirectory.
                    # Treat these as level 0 embeddings (full-resolution patches).
                    emb_check = _fallback_embeddings_dir / f"{slide_id}.npy"
                    has_level0 = emb_check.exists()

            slides.append(SlideInfo(
                slide_id=slide_id,
                has_wsi=(slide_path is not None and slide_path.exists()),
                has_embeddings=True,
                has_level0_embeddings=has_level0,
                label=data.get("label"),
                num_patches=num_patches,
                patient=data.get("patient"),
                dimensions=dims,
                mpp=mpp,
            ))

        # Cache the result
        list_slides._cache[_cache_key] = {"data": slides, "ts": time.time()}
        logger.info(f"Flat-file scan for {_cache_key}: {len(slides)} slides (cached for 60s)")
        return slides

    @app.get("/api/slides/search")
    async def search_slides(
        search: Optional[str] = None,
        label: Optional[str] = None,
        has_embeddings: Optional[bool] = None,
        min_patches: Optional[int] = None,
        max_patches: Optional[int] = None,
        tags: Optional[str] = None,
        group_id: Optional[str] = None,
        starred: Optional[bool] = None,
        date_from: Optional[str] = None,
        date_to: Optional[str] = None,
        sort_by: Optional[str] = "date",
        sort_order: Optional[str] = "desc",
        page: int = 1,
        per_page: int = 20,
        project_id: Optional[str] = Query(None, description="Filter slides by project"),
    ):
        """Search and filter slides with pagination."""
        # Get all slides first (reuse existing logic from list_slides)
        all_slides = await list_slides(project_id=project_id)
        
        # Convert SlideInfo objects to dicts for filtering
        slides_data = [
            {
                "slide_id": s.slide_id,
                "patient_id": s.patient_id,
                "has_wsi": s.has_wsi,
                "has_embeddings": s.has_embeddings,
                "has_level0_embeddings": s.has_level0_embeddings,
                "label": s.label,
                "num_patches": s.num_patches,
                "patient": s.patient.dict() if s.patient else None,
                "dimensions": s.dimensions.dict() if s.dimensions else {"width": 0, "height": 0},
                "mpp": s.mpp,
                "magnification": s.magnification,
            }
            for s in all_slides
        ]
        
        # Apply filters
        filtered = slides_data
        
        if search:
            search_lower = search.lower()
            filtered = [s for s in filtered if search_lower in s.get("slide_id", "").lower()]
        
        if label:
            # Map frontend values to data values
            label_map = {
                "platinum_sensitive": "1",
                "platinum_resistant": "0",
                "Sensitive": "1",
                "Resistant": "0",
                "sensitive": "1",
                "resistant": "0",
                "responder": "1",
                "non-responder": "0",
            }
            data_label = label_map.get(label, label)
            filtered = [s for s in filtered if s.get("label") == data_label]
        
        if has_embeddings is not None:
            filtered = [s for s in filtered if s.get("has_embeddings") == has_embeddings]
        
        if min_patches is not None:
            filtered = [s for s in filtered if (s.get("num_patches") or 0) >= min_patches]
        
        if max_patches is not None:
            filtered = [s for s in filtered if (s.get("num_patches") or 0) <= max_patches]
        
        # Calculate pagination
        total = len(filtered)
        start = (page - 1) * per_page
        end = start + per_page
        paginated = filtered[start:end]
        
        return {
            "slides": paginated,
            "total": total,
            "page": page,
            "per_page": per_page,
            "filters": {"label": label, "search": search}
        }

    @app.post("/api/db/repopulate")
    async def repopulate_database():
        """Force re-population of the PostgreSQL database from flat files.

        This re-reads all CSV, .npy, and SVS files and updates the database.
        Useful after adding new slides or re-embedding.
        """
        if not db_available:
            raise HTTPException(status_code=503, detail="Database not available")
        try:
            t0 = time.time()
            await db.populate_from_flat_files(
                data_root=_data_root,
                embeddings_dir=embeddings_dir,
            )
            elapsed = time.time() - t0
            return {
                "status": "ok",
                "message": f"Database repopulated in {elapsed:.1f}s",
            }
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

    @app.get("/api/db/status")
    async def database_status():
        """Check database connection and population status."""
        if not db_available:
            return {"status": "unavailable", "message": "PostgreSQL not connected"}
        try:
            pool = await db.get_pool()
            async with pool.acquire() as conn:
                slide_count = await conn.fetchval("SELECT COUNT(*) FROM slides")
                patient_count = await conn.fetchval("SELECT COUNT(*) FROM patients")
                meta_count = await conn.fetchval("SELECT COUNT(*) FROM slide_metadata")
                dims_count = await conn.fetchval("SELECT COUNT(*) FROM slides WHERE width > 0")
            return {
                "status": "connected",
                "slides": slide_count,
                "patients": patient_count,
                "metadata_entries": meta_count,
                "slides_with_dimensions": dims_count,
            }
        except Exception as e:
            return {"status": "error", "message": str(e)}

    # Tissue type constants for classification
    TISSUE_TYPES = ["tumor", "stroma", "necrosis", "inflammatory", "normal", "artifact"]
    TISSUE_DESCRIPTIONS = {
        "tumor": "Region appears to contain tumor tissue with atypical cellular morphology",
        "stroma": "Region shows stromal tissue with fibrous connective tissue patterns",
        "necrosis": "Region displays necrotic tissue with cell death indicators",
        "inflammatory": "Region contains inflammatory infiltrate with immune cell presence",
        "normal": "Region appears to contain normal tissue architecture",
        "artifact": "Region may contain processing artifacts or technical issues",
    }

    def classify_tissue_type(x: int, y: int, patch_index: Optional[int] = None) -> dict:
        """Classify tissue type of a region. Mock implementation - deterministic based on coordinates."""
        # Use patch_index if provided for more consistent results, otherwise use coordinates
        if patch_index is not None:
            idx = patch_index % len(TISSUE_TYPES)
        else:
            idx = (x + y) % len(TISSUE_TYPES)
        tissue_type = TISSUE_TYPES[idx]
        # Generate confidence based on hash for variety (0.70 - 0.95 range)
        confidence = 0.70 + ((x * 7 + y * 13) % 26) / 100.0
        return {
            "tissue_type": tissue_type,
            "confidence": round(confidence, 2),
            "description": TISSUE_DESCRIPTIONS[tissue_type],
        }

    @app.post("/api/classify-region", response_model=ClassifyRegionResponse)
    async def classify_region(request: ClassifyRegionRequest):
        """Classify tissue type of a region.

        In production, this would use a trained tissue classifier model.
        Current implementation is deterministic based on coordinates for demo purposes.

        Tissue types:
        - tumor: Neoplastic cells with atypical morphology
        - stroma: Fibrous connective tissue
        - necrosis: Dead/dying tissue
        - inflammatory: Immune cell infiltrates
        - normal: Healthy tissue architecture
        - artifact: Technical issues (blur, folds, etc.)
        """
        result = classify_tissue_type(request.x, request.y, request.patch_index)
        return ClassifyRegionResponse(**result)

    @app.post("/api/analyze", response_model=AnalyzeResponse)
    async def analyze_slide(request: AnalyzeRequest):
        """Analyze a slide and return prediction with evidence."""
        if classifier is None:
            raise HTTPException(status_code=503, detail="Model not loaded")

        slide_id = request.slide_id
        
        project_requested = request.project_id is not None
        _analysis_embeddings_dir = _resolve_project_embeddings_dir(
            request.project_id,
            require_exists=project_requested,
        )

        emb_path = _analysis_embeddings_dir / f"{slide_id}.npy"

        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")

        # Load embeddings
        embeddings = np.load(emb_path)

        # Run prediction
        score, attention = classifier.predict(embeddings)
        threshold = classifier.threshold
        _pos_label, _neg_label = _resolve_project_label_pair(
            request.project_id,
            positive_default="RESPONDER",
            negative_default="NON-RESPONDER",
            uppercase=True,
        )
        label = _pos_label if score >= threshold else _neg_label
        # Confidence based on distance from threshold, normalized to [0,1]
        if score >= threshold:
            # Sigmoid-like scaling: small margins -> ~50%, large margins -> ~99%
            margin = score - threshold
            confidence = min(1.0 - 0.5 * (2.0 ** (-20.0 * margin)), 0.99)
        else:
            margin = threshold - score
            confidence = min(1.0 - 0.5 * (2.0 ** (-20.0 * margin)), 0.99)

        # Load coordinates if available for tissue classification
        coord_path = _analysis_embeddings_dir / f"{slide_id}_coords.npy"
        coords = None
        if coord_path.exists():
            coords = np.load(coord_path)

        # Get top evidence patches
        top_k = min(8, len(attention))
        top_indices = np.argsort(attention)[-top_k:][::-1]

        top_evidence = []
        for i, idx in enumerate(top_indices):
            # Get coordinates for this patch
            patch_x = int(coords[idx][0]) if coords is not None else 0
            patch_y = int(coords[idx][1]) if coords is not None else 0

            # Classify tissue type for this patch
            tissue_info = classify_tissue_type(patch_x, patch_y, int(idx))

            top_evidence.append({
                "rank": i + 1,
                "patch_index": int(idx),
                "attention_weight": float(attention[idx]),
                "coordinates": [patch_x, patch_y],
                "tissue_type": tissue_info["tissue_type"],
                "tissue_confidence": tissue_info["confidence"],
            })

        # Get similar cases using slide-mean cosine similarity (same as /api/similar)
        similar_cases = []
        allowed_slide_ids = _project_slide_ids(request.project_id)
        if slide_mean_index is not None:
            try:
                q = np.asarray(embeddings, dtype=np.float32).mean(axis=0)
                q = q / (np.linalg.norm(q) + 1e-12)
                q = q.reshape(1, -1).astype(np.float32)

                search_k = min(len(slide_mean_ids), max(15, 5 * 3))
                sims, idxs = slide_mean_index.search(q, search_k)

                seen_slides = set()
                for sim, idx_val in zip(sims[0], idxs[0]):
                    if idx_val < 0 or idx_val >= len(slide_mean_ids):
                        continue
                    sid = slide_mean_ids[int(idx_val)]
                    if sid == slide_id or sid in seen_slides:
                        continue
                    if allowed_slide_ids is not None and sid not in allowed_slide_ids:
                        continue
                    seen_slides.add(sid)
                    meta = slide_mean_meta.get(sid, {})
                    case_label = meta.get("label") or slide_labels.get(sid)
                    similar_cases.append({
                        "slide_id": sid,
                        "similarity_score": float(sim),
                        "distance": float(1.0 - float(sim)),
                        "label": case_label,
                    })
                    if len(similar_cases) >= 5:
                        break
            except Exception as e:
                logger.warning(f"Similar case search (cosine) failed: {e}")
        # Fallback to L2-based evidence_gen if slide_mean_index unavailable
        if not similar_cases and evidence_gen is not None:
            try:
                similar_results = evidence_gen.find_similar(
                    embeddings, attention, k=10, top_patches=3
                )
                seen_slides = set()
                for s in similar_results:
                    meta = s.get("metadata", {})
                    sid = meta.get("slide_id", "unknown")
                    if sid != slide_id and sid not in seen_slides:
                        if allowed_slide_ids is not None and sid not in allowed_slide_ids:
                            continue
                        seen_slides.add(sid)
                        case_label = slide_labels.get(sid)
                        similar_cases.append({
                            "slide_id": sid,
                            "similarity_score": 1.0 / (1.0 + s["distance"]),
                            "distance": float(s["distance"]),
                            "label": case_label,
                        })
                    if len(similar_cases) >= 5:
                        break
            except Exception as e:
                logger.warning(f"Similar case search (L2 fallback) failed: {e}")

        # Save to analysis history for audit trail
        save_analysis_to_history(
            slide_id=slide_id,
            prediction=label,
            score=float(score),
            confidence=float(confidence),
            patches_analyzed=len(embeddings),
            top_evidence=top_evidence,
            similar_cases=similar_cases[:5],
        )

        return AnalyzeResponse(
            slide_id=slide_id,
            prediction=label,
            score=float(score),
            confidence=float(confidence),
            patches_analyzed=len(embeddings),
            top_evidence=top_evidence,
            similar_cases=similar_cases[:5],
        )

    # ====== Batch Analysis Endpoint ======

    @app.post("/api/analyze-batch", response_model=BatchAnalyzeResponse)
    async def analyze_batch(request: BatchAnalyzeRequest):
        """
        Analyze multiple slides in batch for clinical workflow efficiency.

        This endpoint processes multiple slides at once and returns:
        - Individual results for each slide
        - Summary statistics across all slides
        - Priority ordering (uncertain cases flagged first)

        Clinical use cases:
        - Review incoming cases for a day/week
        - Triage cases by prediction confidence
        - Identify cases requiring human review
        """
        import time
        start_time = time.time()

        if classifier is None:
            raise HTTPException(status_code=503, detail="Model not loaded")

        project_requested = getattr(request, "project_id", None) is not None
        _batch_embeddings_dir = _resolve_project_embeddings_dir(
            getattr(request, "project_id", None),
            require_exists=project_requested,
        )
        _b_pos, _b_neg = _resolve_project_label_pair(
            getattr(request, "project_id", None),
            positive_default="RESPONDER",
            negative_default="NON-RESPONDER",
            uppercase=True,
        )

        results = []
        for slide_id in request.slide_ids:
            emb_path = _batch_embeddings_dir / f"{slide_id}.npy"

            if not emb_path.exists():
                # Record failed slide
                results.append(BatchAnalysisResult(
                    slide_id=slide_id,
                    prediction="ERROR",
                    score=0.0,
                    confidence=0.0,
                    patches_analyzed=0,
                    requires_review=True,
                    uncertainty_level="unknown",
                    error=f"Slide {slide_id} not found",
                ))
                continue

            try:
                # Load embeddings
                embeddings = np.load(emb_path)

                # Run prediction
                score, attention = classifier.predict(embeddings)
                _b_threshold = classifier.threshold
                label = _b_pos if score >= _b_threshold else _b_neg
                confidence = abs(score - _b_threshold) * 2

                # Determine uncertainty level and review requirement
                if confidence < 0.3:
                    uncertainty_level = "high"
                    requires_review = True
                elif confidence < 0.6:
                    uncertainty_level = "moderate"
                    requires_review = True
                else:
                    uncertainty_level = "low"
                    requires_review = False

                results.append(BatchAnalysisResult(
                    slide_id=slide_id,
                    prediction=label,
                    score=float(score),
                    confidence=float(confidence),
                    patches_analyzed=len(embeddings),
                    requires_review=requires_review,
                    uncertainty_level=uncertainty_level,
                    error=None,
                ))

                # Log to audit trail
                log_audit_event(
                    "batch_analysis_slide",
                    slide_id,
                    details={
                        "prediction": label,
                        "confidence": float(confidence),
                        "requires_review": requires_review,
                    },
                )

            except Exception as e:
                logger.error(f"Batch analysis failed for {slide_id}: {e}")
                results.append(BatchAnalysisResult(
                    slide_id=slide_id,
                    prediction="ERROR",
                    score=0.0,
                    confidence=0.0,
                    patches_analyzed=0,
                    requires_review=True,
                    uncertainty_level="unknown",
                    error=str(e),
                ))

        # Sort results: uncertain cases first (by confidence ascending)
        results.sort(key=lambda r: (
            0 if r.error else 1,  # Errors first
            r.confidence if not r.error else 999,  # Then by confidence (lowest first)
        ))

        # Calculate summary statistics
        completed = [r for r in results if r.error is None]
        failed = [r for r in results if r.error is not None]
        # Count positive vs negative predictions (project-aware)
        responders = [r for r in completed if r.prediction == _b_pos]
        non_responders = [r for r in completed if r.prediction != _b_pos]
        uncertain = [r for r in completed if r.requires_review]
        avg_confidence = (
            sum(r.confidence for r in completed) / len(completed)
            if completed else 0.0
        )

        summary = BatchAnalysisSummary(
            total=len(results),
            completed=len(completed),
            failed=len(failed),
            responders=len(responders),
            non_responders=len(non_responders),
            uncertain=len(uncertain),
            avg_confidence=round(avg_confidence, 3),
            requires_review_count=sum(1 for r in results if r.requires_review),
        )

        processing_time_ms = (time.time() - start_time) * 1000

        # Log batch completion
        log_audit_event(
            "batch_analysis_completed",
            details={
                "total_slides": len(results),
                "completed": len(completed),
                "failed": len(failed),
                "processing_time_ms": processing_time_ms,
            },
        )

        return BatchAnalyzeResponse(
            results=results,
            summary=summary,
            processing_time_ms=round(processing_time_ms, 2),
        )


    # ====== Async Batch Analysis Endpoints ======

    class AsyncBatchRequest(BaseModel):
        """Request for async batch analysis."""
        slide_ids: List[str] = Field(
            ...,
            min_length=1,
            max_length=100,
            description="List of slide IDs to analyze"
        )
        concurrency: int = Field(
            default=4,
            ge=1,
            le=10,
            description="Number of slides to process in parallel (1-10)"
        )
        model_ids: Optional[List[str]] = Field(
            default=None,
            description="Model IDs to run. If None, uses default classifier."
        )
        level: int = Field(
            default=1,
            ge=0,
            le=1,
            description="Embedding resolution level (0=full, 1=downsampled)"
        )
        force_reembed: bool = Field(
            default=False,
            description="Force re-computation of embeddings even if cached"
        )
        project_id: Optional[str] = Field(
            default=None,
            description="Project ID to scope embeddings and labels"
        )

    class AsyncBatchResponse(BaseModel):
        """Response from async batch analysis start."""
        task_id: str
        status: str
        total_slides: int
        message: str

    @app.post("/api/analyze-batch/async", response_model=AsyncBatchResponse)
    async def analyze_batch_async(request: AsyncBatchRequest, background_tasks: BackgroundTasks):
        """
        Start async batch analysis with progress tracking and cancellation support.

        Returns a task_id immediately. Poll /api/analyze-batch/status/{task_id} for progress.
        Use POST /api/analyze-batch/cancel/{task_id} to cancel a running analysis.

        Benefits over synchronous batch:
        - Real-time progress ("Analyzing slide 3/10...")
        - Cancellation support for long batches
        - Better handling of partial failures
        - Non-blocking for large batches
        """
        if classifier is None:
            raise HTTPException(status_code=503, detail="Model not loaded")

        _resolve_project_embeddings_dir(
            request.project_id,
            require_exists=request.project_id is not None,
        )
        allowed_model_ids = await _resolve_project_model_ids(request.project_id)
        if request.model_ids is not None and allowed_model_ids is not None:
            disallowed = sorted(set(request.model_ids) - allowed_model_ids)
            if disallowed:
                raise HTTPException(
                    status_code=400,
                    detail={
                        "error": "MODELS_NOT_ALLOWED_FOR_PROJECT",
                        "project_id": request.project_id,
                        "disallowed_models": disallowed,
                    },
                )

        # Create task
        task = batch_task_manager.create_task(request.slide_ids)

        # Run batch analysis in background
        def run_batch_analysis():
            _run_batch_analysis_background(
                task.task_id,
                request.slide_ids,
                request.concurrency,
                model_ids=request.model_ids,
                level=request.level,
                force_reembed=request.force_reembed,
                project_id=request.project_id,
            )

        background_tasks.add_task(run_batch_analysis)

        return AsyncBatchResponse(
            task_id=task.task_id,
            status="pending",
            total_slides=len(request.slide_ids),
            message=f"Batch analysis started. Poll /api/analyze-batch/status/{task.task_id} for progress.",
        )

    def _run_batch_analysis_background(
        task_id: str,
        slide_ids: List[str],
        concurrency: int = 4,
        model_ids: Optional[List[str]] = None,
        level: int = 1,
        force_reembed: bool = False,
        project_id: Optional[str] = None,
    ):
        """Background task to run batch analysis with progress tracking."""
        import time
        import concurrent.futures
        from .batch_tasks import BatchModelResult

        task = batch_task_manager.get_task(task_id)
        if not task:
            return

        batch_task_manager.update_task(task_id,
            status=BatchTaskStatus.RUNNING,
            started_at=time.time(),
            message="Starting batch analysis..."
        )

        use_multi_model = model_ids is not None and multi_model_inference is not None
        project_requested = project_id is not None
        try:
            batch_embeddings_dir = _resolve_project_embeddings_dir(
                project_id,
                require_exists=project_requested,
            )
            project_pos_label, project_neg_label = _resolve_project_label_pair(
                project_id,
                positive_default="RESPONDER",
                negative_default="NON-RESPONDER",
                uppercase=True,
            )
        except Exception as e:
            batch_task_manager.update_task(
                task_id,
                status=BatchTaskStatus.FAILED,
                error=str(e),
                message=f"Batch analysis failed: {str(e)}",
            )
            return

        effective_model_ids = list(model_ids or [])

        def _resolve_emb_path(slide_id: str):
            """Resolve embedding path based on requested level and project scope."""
            candidate_dirs: List[Path] = []
            if level == 0:
                if batch_embeddings_dir.name == "level0":
                    candidate_dirs.append(batch_embeddings_dir)
                else:
                    candidate_dirs.extend([batch_embeddings_dir / "level0", batch_embeddings_dir])
                if not project_requested and batch_embeddings_dir != embeddings_dir:
                    if embeddings_dir.name == "level0":
                        candidate_dirs.append(embeddings_dir)
                    else:
                        candidate_dirs.extend([embeddings_dir / "level0", embeddings_dir])
            else:
                candidate_dirs.append(batch_embeddings_dir)
                if batch_embeddings_dir.name != "level1":
                    candidate_dirs.append(batch_embeddings_dir / "level1")
                if not project_requested and batch_embeddings_dir != embeddings_dir:
                    candidate_dirs.extend([embeddings_dir, embeddings_dir / "level1"])

            for d in candidate_dirs:
                p = d / f"{slide_id}.npy"
                if p.exists():
                    return p
            return None

        def analyze_single_slide(slide_id: str) -> BatchSlideResult:
            """Analyze a single slide and return result."""
            emb_path = _resolve_emb_path(slide_id)

            if emb_path is None or not emb_path.exists():
                return BatchSlideResult(
                    slide_id=slide_id,
                    prediction="ERROR",
                    error=f"Slide {slide_id} embeddings not found (level {level})",
                )

            try:
                embeddings = np.load(emb_path)

                # Multi-model path
                if use_multi_model:
                    model_results_list = []
                    primary_score = 0.0
                    primary_label = "UNKNOWN"
                    primary_conf = 0.0

                    if not effective_model_ids:
                        return BatchSlideResult(
                            slide_id=slide_id,
                            prediction="ERROR",
                            error="No permitted models available for this request",
                        )

                    for i, mid in enumerate(effective_model_ids):
                        try:
                            model_obj = multi_model_inference.models.get(mid)
                            if model_obj is None:
                                model_results_list.append(BatchModelResult(
                                    model_id=mid,
                                    model_name=mid,
                                    error=f"Model {mid} not found",
                                ))
                                continue
                            cfg = MODEL_CONFIGS.get(mid, {})
                            # Use predict_single which handles tensor conversion
                            pred_result = multi_model_inference.predict_single(embeddings, mid)
                            if "error" in pred_result:
                                model_results_list.append(BatchModelResult(
                                    model_id=mid,
                                    model_name=cfg.get("display_name", mid),
                                    error=pred_result["error"],
                                ))
                                if i == 0:
                                    primary_label = "ERROR"
                                continue
                            s = float(pred_result["score"])
                            c = abs(s - 0.5) * 2
                            pos_label = cfg.get("positive_label", "Positive")
                            neg_label = cfg.get("negative_label", "Negative")
                            lbl = pos_label if s > 0.5 else neg_label
                            model_results_list.append(BatchModelResult(
                                model_id=mid,
                                model_name=cfg.get("display_name", mid),
                                prediction=lbl,
                                score=s,
                                confidence=c,
                                positive_label=pos_label,
                                negative_label=neg_label,
                            ))
                            # Use first model as primary result
                            if i == 0:
                                primary_score = s
                                primary_label = lbl
                                primary_conf = c
                        except Exception as me:
                            model_results_list.append(BatchModelResult(
                                model_id=mid,
                                model_name=mid,
                                error=str(me),
                            ))

                    if primary_conf < 0.3:
                        uncertainty_level = "high"
                        requires_review = True
                    elif primary_conf < 0.6:
                        uncertainty_level = "moderate"
                        requires_review = True
                    else:
                        uncertainty_level = "low"
                        requires_review = False

                    return BatchSlideResult(
                        slide_id=slide_id,
                        prediction=primary_label,
                        score=primary_score,
                        confidence=primary_conf,
                        patches_analyzed=len(embeddings),
                        requires_review=requires_review,
                        uncertainty_level=uncertainty_level,
                        model_results=model_results_list,
                    )

                # Single classifier path (legacy)
                score, attention = classifier.predict(embeddings)
                threshold_val = getattr(classifier, "threshold", 0.5)
                label = project_pos_label if score >= threshold_val else project_neg_label
                confidence = abs(score - threshold_val) * 2

                if confidence < 0.3:
                    uncertainty_level = "high"
                    requires_review = True
                elif confidence < 0.6:
                    uncertainty_level = "moderate"
                    requires_review = True
                else:
                    uncertainty_level = "low"
                    requires_review = False

                return BatchSlideResult(
                    slide_id=slide_id,
                    prediction=label,
                    score=float(score),
                    confidence=float(confidence),
                    patches_analyzed=len(embeddings),
                    requires_review=requires_review,
                    uncertainty_level=uncertainty_level,
                )
            except Exception as e:
                logger.error(f"Batch analysis failed for {slide_id}: {e}")
                return BatchSlideResult(
                    slide_id=slide_id,
                    prediction="ERROR",
                    error=str(e),
                )

        try:
            total = len(slide_ids)
            completed = 0

            # Process slides with concurrency limit
            with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as executor:
                # Submit all tasks
                future_to_slide = {
                    executor.submit(analyze_single_slide, slide_id): slide_id
                    for slide_id in slide_ids
                }

                # Process as they complete
                for future in concurrent.futures.as_completed(future_to_slide):
                    # Check for cancellation
                    if batch_task_manager.is_cancelled(task_id):
                        # Cancel remaining futures
                        for f in future_to_slide:
                            f.cancel()
                        batch_task_manager.update_task(task_id,
                            status=BatchTaskStatus.CANCELLED,
                            message=f"Cancelled after {completed}/{total} slides",
                            completed_at=time.time(),
                        )
                        logger.info(f"Batch analysis {task_id} cancelled after {completed} slides")
                        return

                    slide_id = future_to_slide[future]
                    try:
                        result = future.result()
                        batch_task_manager.add_result(task_id, result)
                        completed += 1

                        # Update progress
                        progress = (completed / total) * 100
                        batch_task_manager.update_task(task_id,
                            progress=progress,
                            current_slide_index=completed,
                            current_slide_id=slide_id,
                            message=f"Analyzing slide {completed}/{total}: {slide_id[:20]}...",
                        )

                    except Exception as e:
                        logger.error(f"Future failed for {slide_id}: {e}")
                        batch_task_manager.add_result(task_id, BatchSlideResult(
                            slide_id=slide_id,
                            prediction="ERROR",
                            error=str(e),
                        ))
                        completed += 1

            # Complete
            batch_task_manager.update_task(task_id,
                status=BatchTaskStatus.COMPLETED,
                progress=100,
                message=f"Completed analysis of {total} slides",
                completed_at=time.time(),
            )

            # Log completion
            log_audit_event(
                "batch_analysis_async_completed",
                details={
                    "task_id": task_id,
                    "total_slides": total,
                },
            )

            logger.info(f"Batch analysis {task_id} completed: {total} slides")

        except Exception as e:
            logger.error(f"Batch analysis {task_id} failed: {e}")
            batch_task_manager.update_task(task_id,
                status=BatchTaskStatus.FAILED,
                error=str(e),
                message=f"Batch analysis failed: {str(e)}",
            )

    @app.get("/api/analyze-batch/status/{task_id}")
    async def get_batch_status(task_id: str):
        """
        Get status of an async batch analysis task.

        Returns:
        - status: pending, running, completed, cancelled, or failed
        - progress: 0-100 percentage
        - current_slide_index: which slide is being processed
        - current_slide_id: ID of current slide
        - message: human-readable status message
        - results: full results (only when completed)
        """
        task = batch_task_manager.get_task(task_id)
        if not task:
            raise HTTPException(status_code=404, detail=f"Task {task_id} not found")

        # Return full results if completed
        if task.status in [BatchTaskStatus.COMPLETED, BatchTaskStatus.CANCELLED]:
            return task.to_full_dict()

        return task.to_dict()

    @app.post("/api/analyze-batch/cancel/{task_id}")
    async def cancel_batch_analysis(task_id: str):
        """
        Cancel a running batch analysis task.

        Already completed slides will be retained in the results.
        """
        task = batch_task_manager.get_task(task_id)
        if not task:
            raise HTTPException(status_code=404, detail=f"Task {task_id} not found")

        if task.status != BatchTaskStatus.RUNNING:
            raise HTTPException(
                status_code=400,
                detail=f"Cannot cancel task with status {task.status.value}"
            )

        success = batch_task_manager.request_cancel(task_id)

        if success:
            log_audit_event(
                "batch_analysis_cancelled",
                details={"task_id": task_id, "slides_completed": task.completed_slides},
            )
            return {
                "success": True,
                "message": f"Cancellation requested for task {task_id}",
                "completed_slides": task.completed_slides,
            }

        return {
            "success": False,
            "message": "Failed to request cancellation",
        }

    @app.get("/api/analyze-batch/tasks")
    async def list_batch_tasks(
        status: Optional[str] = Query(None, description="Filter by status"),
    ):
        """
        List all batch analysis tasks.

        Optionally filter by status: pending, running, completed, cancelled, failed
        """
        status_filter = None
        if status:
            try:
                status_filter = BatchTaskStatus(status)
            except ValueError:
                raise HTTPException(
                    status_code=400,
                    detail=f"Invalid status: {status}. Valid values: pending, running, completed, cancelled, failed"
                )

        tasks = batch_task_manager.list_tasks(status_filter)

        return {
            "tasks": tasks,
            "total": len(tasks),
        }

    # ====== Uncertainty Quantification Endpoint ======

    @app.post("/api/analyze-uncertainty", response_model=UncertaintyResponse)
    async def analyze_with_uncertainty(request: UncertaintyRequest):
        """
        Analyze a slide with MC Dropout uncertainty quantification.

        Uses Monte Carlo Dropout to estimate predictive uncertainty by running
        multiple forward passes with dropout enabled. High variance indicates
        the model is uncertain about its prediction.

        Clinical interpretation:
        - Low uncertainty (< 0.10): Model is confident, prediction reliable
        - Moderate uncertainty (0.10 - 0.20): Some uncertainty, review recommended
        - High uncertainty (> 0.20): Model is uncertain, consider additional testing

        When uncertainty is high, the system flags the case for human review
        and provides conservative recommendations.
        """
        if classifier is None:
            raise HTTPException(status_code=503, detail="Model not loaded")

        slide_id = request.slide_id
        emb_path = embeddings_dir / f"{slide_id}.npy"

        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")

        # Load embeddings
        embeddings = np.load(emb_path)

        # Run MC Dropout prediction
        try:
            result = classifier.predict_with_uncertainty(
                embeddings, n_samples=request.n_samples
            )
        except Exception as e:
            logger.error(f"Uncertainty prediction failed: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Uncertainty prediction failed: {str(e)}"
            )

        # Determine uncertainty level and clinical recommendation
        uncertainty = result["uncertainty"]
        probability = result["probability"]

        if uncertainty < 0.10:
            uncertainty_level = "low"
            requires_review = False
            _pred_label = result.get("prediction", "unknown")
            clinical_recommendation = (
                f"Model shows high confidence in {_pred_label} prediction. "
                "Consider proceeding with clinical evaluation based on full context."
            )
        elif uncertainty < 0.20:
            uncertainty_level = "moderate"
            requires_review = True
            clinical_recommendation = (
                "Model shows moderate uncertainty. Recommend pathologist review "
                "of high-attention regions and correlation with clinical factors."
            )
        else:
            uncertainty_level = "high"
            requires_review = True
            clinical_recommendation = (
                "Model is uncertain about this case - consider additional testing. "
                "Do not rely solely on this prediction. Recommend molecular profiling "
                "and/or expert pathology consultation."
            )

        # Load coordinates for evidence patches
        coord_path = embeddings_dir / f"{slide_id}_coords.npy"
        coords = None
        if coord_path.exists():
            coords = np.load(coord_path)

        # Get top evidence patches using mean attention
        attention = result["attention_weights"]
        attention_std = result["attention_uncertainty"]
        top_k = min(8, len(attention))
        top_indices = np.argsort(attention)[-top_k:][::-1]

        top_evidence = []
        for i, idx in enumerate(top_indices):
            patch_x = int(coords[idx][0]) if coords is not None else 0
            patch_y = int(coords[idx][1]) if coords is not None else 0

            # Include attention uncertainty for each patch
            top_evidence.append({
                "rank": i + 1,
                "patch_index": int(idx),
                "attention_weight": float(attention[idx]),
                "attention_uncertainty": float(attention_std[idx]),
                "coordinates": [patch_x, patch_y],
            })

        # Log audit event
        log_audit_event(
            "uncertainty_analysis_completed",
            slide_id,
            details={
                "prediction": result["prediction"],
                "uncertainty": uncertainty,
                "uncertainty_level": uncertainty_level,
                "requires_review": requires_review,
            },
        )

        return UncertaintyResponse(
            slide_id=slide_id,
            prediction=result["prediction"],
            probability=probability,
            uncertainty=uncertainty,
            confidence_interval=result["confidence_interval"],
            is_uncertain=result["is_uncertain"],
            requires_review=requires_review,
            uncertainty_level=uncertainty_level,
            clinical_recommendation=clinical_recommendation,
            patches_analyzed=len(embeddings),
            n_samples=result["n_samples"],
            samples=result["samples"],
            top_evidence=top_evidence,
        )

    # ====== Analysis History Endpoints ======

    @app.get("/api/history", response_model=AnalysisHistoryResponse)
    async def get_analysis_history(
        limit: int = 50,
        offset: int = 0,
        slide_id: Optional[str] = None,
        prediction: Optional[str] = None,
    ):
        """
        Get recent analysis history.

        Args:
            limit: Maximum number of entries to return (default 50, max 100)
            offset: Number of entries to skip
            slide_id: Filter by specific slide ID
            prediction: Filter by prediction result (RESPONDER/NON-RESPONDER)

        Returns:
            List of analysis history entries, most recent first.
        """
        # Convert deque to list for filtering and slicing
        all_entries = list(analysis_history)

        # Apply filters
        if slide_id:
            all_entries = [e for e in all_entries if e["slide_id"] == slide_id]
        if prediction:
            all_entries = [e for e in all_entries if e["prediction"] == prediction.upper()]

        # Sort by timestamp descending (most recent first)
        all_entries.sort(key=lambda x: x["timestamp"], reverse=True)

        # Apply pagination
        limit = min(limit, 100)
        paginated = all_entries[offset:offset + limit]

        return AnalysisHistoryResponse(
            analyses=[AnalysisHistoryEntry(**e) for e in paginated],
            total=len(all_entries),
        )

    @app.get("/api/slides/{slide_id}/history", response_model=AnalysisHistoryResponse)
    async def get_slide_history(slide_id: str, limit: int = 20):
        """
        Get analysis history for a specific slide.

        Args:
            slide_id: The slide ID to get history for
            limit: Maximum number of entries to return (default 20)

        Returns:
            List of analysis history entries for this slide, most recent first.
        """
        # Filter entries for this slide
        slide_entries = [e for e in analysis_history if e["slide_id"] == slide_id]

        # Sort by timestamp descending
        slide_entries.sort(key=lambda x: x["timestamp"], reverse=True)

        # Apply limit
        limit = min(limit, 50)
        paginated = slide_entries[:limit]

        return AnalysisHistoryResponse(
            analyses=[AnalysisHistoryEntry(**e) for e in paginated],
            total=len(slide_entries),
        )

    @app.get("/api/audit-log", response_model=AuditLogResponse)
    async def get_audit_log(
        limit: int = 100,
        offset: int = 0,
        event_type: Optional[str] = None,
        slide_id: Optional[str] = None,
    ):
        """
        Get audit log entries for compliance tracking.

        Event types:
        - analysis_completed: Slide analysis was run
        - report_generated: Clinical report was generated
        - pdf_exported: Report was exported as PDF
        - json_exported: Report was exported as JSON

        Args:
            limit: Maximum number of entries to return (default 100)
            offset: Number of entries to skip
            event_type: Filter by event type
            slide_id: Filter by slide ID

        Returns:
            List of audit log entries, most recent first.
        """
        all_entries = list(audit_log)

        # Apply filters
        if event_type:
            all_entries = [e for e in all_entries if e["event_type"] == event_type]
        if slide_id:
            all_entries = [e for e in all_entries if e.get("slide_id") == slide_id]

        # Sort by timestamp descending
        all_entries.sort(key=lambda x: x["timestamp"], reverse=True)

        # Apply pagination
        limit = min(limit, 500)
        paginated = all_entries[offset:offset + limit]

        return AuditLogResponse(
            entries=[AuditLogEntry(**e) for e in paginated],
            total=len(all_entries),
        )

    def _load_patient_context(
        slide_id: str,
        project_id: Optional[str] = None,
    ) -> Optional[Dict[str, Any]]:
        """Load patient context from a project's labels file for a slide."""
        _require_project(project_id)
        labels_path = _project_labels_path(project_id)
        if labels_path is None:
            labels_path = _data_root / "labels.csv"
            if not labels_path.exists():
                labels_path = embeddings_dir.parent / "labels.csv"

        if not labels_path.exists() or labels_path.suffix.lower() != ".csv":
            return None

        import csv

        with open(labels_path) as f:
            reader = csv.DictReader(f)
            for row in reader:
                # Handle different CSV formats
                if "slide_id" in row:
                    sid = row["slide_id"]
                else:
                    slide_file = row.get("slide_file", "")
                    sid = slide_file.replace(".svs", "").replace(".SVS", "")

                if sid == slide_id:
                    patient_ctx = {}
                    if row.get("age"):
                        try:
                            patient_ctx["age"] = int(row["age"])
                        except ValueError:
                            pass
                    if row.get("sex"):
                        patient_ctx["sex"] = row["sex"]
                    if row.get("stage"):
                        patient_ctx["stage"] = row["stage"]
                    if row.get("grade"):
                        patient_ctx["grade"] = row["grade"]
                    if row.get("prior_treatments"):
                        try:
                            patient_ctx["prior_lines"] = int(row["prior_treatments"])
                        except ValueError:
                            pass
                    if row.get("histology"):
                        patient_ctx["histology"] = row["histology"]
                    return patient_ctx if patient_ctx else None
        return None

    def _format_patient_summary(patient_ctx: Optional[Dict[str, Any]]) -> str:
        """Format patient context into a clinical summary sentence."""
        if not patient_ctx:
            return ""

        parts = []
        if patient_ctx.get("age"):
            parts.append(f"{patient_ctx['age']}-year-old")
        if patient_ctx.get("sex"):
            sex_full = "female" if patient_ctx["sex"].upper() == "F" else "male" if patient_ctx["sex"].upper() == "M" else patient_ctx["sex"]
            parts.append(sex_full)

        summary = " ".join(parts) if parts else "Patient"

        clinical_parts = []
        if patient_ctx.get("stage"):
            clinical_parts.append(f"Stage {patient_ctx['stage']}")
        if patient_ctx.get("histology"):
            clinical_parts.append(patient_ctx["histology"].lower())
        if clinical_parts:
            summary += " with " + " ".join(clinical_parts)

        if patient_ctx.get("prior_lines") is not None:
            lines = patient_ctx["prior_lines"]
            if lines == 0:
                summary += ", treatment-naive"
            else:
                summary += f", {lines} prior line{'s' if lines > 1 else ''} of therapy"

        return summary

    @app.post("/api/report", response_model=ReportResponse)
    async def generate_report(request: ReportRequest):
        """Generate a structured report for a slide using MedGemma."""
        if classifier is None:
            raise HTTPException(status_code=503, detail="Model not loaded")

        slide_id = request.slide_id

        project_requested = request.project_id is not None
        proj_cfg = _require_project(request.project_id)
        _report_embeddings_dir = _resolve_project_embeddings_dir(
            request.project_id,
            require_exists=project_requested,
        )

        emb_path = _report_embeddings_dir / f"{slide_id}.npy"
        coord_path = _report_embeddings_dir / f"{slide_id}_coords.npy"

        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")

        embeddings = np.load(emb_path)
        score, attention = classifier.predict(embeddings)
        threshold = classifier.threshold
        _pos_label, _neg_label = _resolve_project_label_pair(
            request.project_id,
            positive_default="responder",
            negative_default="non-responder",
            uppercase=False,
        )
        label = _pos_label if score >= threshold else _neg_label

        # Load patient context
        patient_ctx = _load_patient_context(slide_id, project_id=request.project_id)

        # Get top evidence patches with coordinates
        top_k = min(8, len(attention))
        top_indices = np.argsort(attention)[-top_k:][::-1]

        # Load coordinates if available
        coords = None
        if coord_path.exists():
            coords = np.load(coord_path)

        evidence_patches = []
        for rank, idx in enumerate(top_indices, 1):
            patch_info = {
                "rank": rank,
                "patch_index": int(idx),
                "attention_weight": float(attention[idx]),
                "coordinates": [int(coords[idx][0]), int(coords[idx][1])] if coords is not None else [0, 0],
            }
            evidence_patches.append(patch_info)

        # Get similar cases
        similar_cases = []
        allowed_slide_ids = _project_slide_ids(request.project_id)
        if evidence_gen is not None:
            try:
                similar_results = evidence_gen.find_similar(
                    embeddings, attention, k=5, top_patches=3
                )
                for s in similar_results:
                    sid = s.get("slide_id") if isinstance(s, dict) else None
                    if allowed_slide_ids is not None and sid and sid not in allowed_slide_ids:
                        continue
                    similar_cases.append(s)
            except Exception as e:
                logger.warning(f"Similar case search failed for report: {e}")

        # Get slide quality metrics for decision support
        quality_metrics = None
        try:
            # Generate deterministic quality metrics based on slide_id
            import hashlib
            hash_val = int(hashlib.md5(slide_id.encode()).hexdigest(), 16)
            tissue_coverage = 0.60 + (hash_val % 40) / 100.0
            blur_score = (hash_val % 30) / 100.0
            stain_uniformity = 0.70 + (hash_val % 30) / 100.0
            artifact_detected = (hash_val % 10) == 0
            
            quality_score = (
                tissue_coverage * 0.3 +
                (1 - blur_score) * 0.3 +
                stain_uniformity * 0.2 +
                (0 if artifact_detected else 0.2)
            )
            overall_quality = "good" if quality_score >= 0.75 else "acceptable" if quality_score >= 0.50 else "poor"
            
            quality_metrics = {
                "overall_quality": overall_quality,
                "tissue_coverage": tissue_coverage,
                "blur_score": blur_score,
                "artifact_detected": artifact_detected,
            }
        except Exception as e:
            logger.warning(f"Could not compute quality metrics: {e}")

        # Look up cancer type from project config (used by both decision support and MedGemma)
        cancer_type = (proj_cfg.cancer_type if proj_cfg else "Cancer") or "Cancer"

        # Generate clinical decision support
        decision_support_data = None
        if decision_support is not None:
            try:
                ds_output = decision_support.generate(
                    prediction=label,
                    score=float(score),
                    similar_cases=similar_cases,
                    quality_metrics=quality_metrics,
                    patient_context=patient_ctx,
                    cancer_type=cancer_type,
                )
                decision_support_data = decision_support.to_dict(ds_output)
                logger.info(f"Generated decision support for {slide_id}: risk_level={ds_output.risk_level.value}")
            except Exception as e:
                logger.warning(f"Decision support generation failed: {e}")

        # Try MedGemma report generation
        if reporter is not None:
            timeout_s = None
            try:
                timeout_s = getattr(reporter.config, "max_generation_time_s", None)
                if timeout_s is None:
                    timeout_s = 120.0
                # Allow generous timeout for CPU inference (120s gen + 60s buffer)
                timeout_s = max(10.0, float(timeout_s) + 60.0)

                report = await asyncio.wait_for(
                    asyncio.to_thread(
                        reporter.generate_report,
                        evidence_patches=evidence_patches,
                        score=score,
                        label=label,
                        similar_cases=similar_cases,
                        case_id=slide_id,
                        patient_context=patient_ctx,
                        cancer_type=cancer_type,
                    ),
                    timeout=timeout_s,
                )

                # Merge decision support into structured report
                structured = report["structured"]
                if decision_support_data:
                    structured["decision_support"] = decision_support_data

                return ReportResponse(
                    slide_id=slide_id,
                    report_json=structured,
                    summary_text=report["summary"],
                )
            except asyncio.TimeoutError:
                logger.warning(
                    "MedGemma report generation timed out after %.1fs for slide %s",
                    timeout_s or 0.0,
                    slide_id,
                )
            except Exception as e:
                logger.warning(f"MedGemma report generation failed, using template: {e}")

        # Format patient summary for report
        patient_summary = _format_patient_summary(patient_ctx)

        # Generate morphology descriptions based on tissue types and attention
        def generate_morphology_description(patch: dict, rank: int) -> tuple[str, str]:
            """Generate morphology description and significance for a patch."""
            tissue_type = patch.get("tissue_type", "unknown")
            attention = patch.get("attention_weight", 0)
            coords = patch.get("coordinates", [0, 0])
            
            # Tissue-type specific morphology descriptions
            morphology_templates = {
                "tumor": [
                    "Dense cellular region with atypical epithelial morphology and increased nuclear-to-cytoplasmic ratio",
                    "Papillary architecture with stratified epithelium showing nuclear atypia",
                    "Solid sheets of cells with irregular nuclear contours and prominent nucleoli",
                    "Glandular structures with back-to-back arrangement and cribriform patterns",
                ],
                "stroma": [
                    "Desmoplastic stroma with spindle-shaped fibroblasts and collagen deposition",
                    "Fibrovascular core with loose connective tissue and scattered vessels",
                    "Dense fibrous stroma with hyalinized collagen bundles",
                ],
                "necrosis": [
                    "Geographic necrosis with ghost cell outlines and nuclear debris",
                    "Coagulative necrosis with preserved tissue architecture",
                    "Necrotic debris with inflammatory cell infiltration",
                ],
                "inflammatory": [
                    "Lymphocytic infiltrate with peritumoral distribution",
                    "Tumor-infiltrating lymphocytes forming dense aggregates",
                    "Mixed inflammatory infiltrate with plasma cells and lymphocytes",
                ],
                "normal": [
                    "Normal epithelial architecture with maintained polarity",
                    "Benign glandular tissue with regular spacing",
                ],
            }
            
            # Significance based on label prediction (generic - uses positive/negative keys)
            significance_templates = {
                "positive": {
                    "tumor": "Tumor morphology patterns in this region are associated with the positive prediction in the training cohort",
                    "stroma": "Stromal composition in this area correlates with the predicted outcome",
                    "inflammatory": "Inflammatory infiltrate pattern suggests a tumor microenvironment consistent with the prediction",
                    "necrosis": "Necrotic pattern may indicate tissue changes relevant to prognosis",
                    "normal": "Preserved tissue architecture in adjacent regions may indicate better overall tissue health",
                },
                "negative": {
                    "tumor": "Tumor morphology in this region shows patterns associated with the predicted outcome",
                    "stroma": "Stromal characteristics suggest mechanisms consistent with the prediction",
                    "inflammatory": "Inflammatory pattern may indicate a tumor microenvironment associated with the predicted outcome",
                    "necrosis": "Necrotic patterns in this configuration are associated with the predicted outcome",
                    "normal": "Limited tumor involvement in this area provides context for overall assessment",
                },
            }
            
            # Select morphology description
            templates = morphology_templates.get(tissue_type, ["Tissue region with notable morphological features"])
            morphology = templates[rank % len(templates)]
            
            # Add coordinate context
            morphology += f" at position ({coords[0]:,}, {coords[1]:,})"
            
            # Select significance based on whether prediction matches positive class
            label_key = "positive" if label.lower() == _pos_label.lower() else "negative"
            sig_templates = significance_templates.get(label_key, {})
            significance = sig_templates.get(tissue_type, 
                f"High model attention (weight: {attention:.3f}) indicates this region contributes significantly to the prediction")
            
            return morphology, significance
        
        # Fallback to template report with detailed evidence
        evidence_list = []
        for i, p in enumerate(evidence_patches[:5]):
            morphology, significance = generate_morphology_description(p, i)
            evidence_list.append({
                "patch_id": f"patch_{p['patch_index']}",
                "attention_weight": p["attention_weight"],
                "coordinates": p["coordinates"],
                "morphology_description": morphology,
                "significance": significance,
                "tissue_type": p.get("tissue_type", "unknown"),
            })
        
        report_json = {
            "case_id": slide_id,
            "task": f"{cancer_type} prediction from H&E histopathology",
            "patient_context": patient_ctx,
            "model_output": {
                "label": label,
                "probability": float(score),
                "calibration_note": "Model probability requires external validation. This is an uncalibrated research model.",
            },
            "evidence": evidence_list,
            "similar_examples": [
                {
                    "example_id": s.get("metadata", {}).get("slide_id", f"case_{i}"),
                    "slide_id": s.get("metadata", {}).get("slide_id", f"case_{i}"),
                    "distance": float(s.get("distance", 0)),
                    "similarity_score": 1.0 / (1.0 + s.get("distance", 0)),
                    "label": s.get("metadata", {}).get("label", "unknown"),
                }
                for i, s in enumerate(similar_cases[:5])
            ],
            "limitations": [
                "This is an uncalibrated research model - probabilities are not clinically validated",
                "Prediction is based on morphological patterns and may not capture all relevant clinical factors",
                "Model has been trained on a limited cancer dataset and may not generalize to all populations",
                "Slide quality and tissue representation may affect prediction accuracy",
                "Similar case comparison is based on embedding distance, not verified clinical outcomes",
            ],
            "suggested_next_steps": [
                "Review high-attention regions with attending pathologist",
                "Correlate findings with patient clinical history and imaging",
                "Consider molecular profiling (e.g., BRCA status, HRD) for additional treatment guidance",
                "Discuss findings in multidisciplinary tumor board before treatment decisions",
                "Validate prediction against institutional experience with similar cases",
            ],
            "safety_statement": "This is a research decision-support tool, not a diagnostic device. All findings must be validated by qualified pathologists and clinicians. Do not use for standalone clinical decision-making. Treatment decisions should incorporate all available clinical, pathological, and molecular data.",
            "decision_support": decision_support_data,
        }

        # Build comprehensive summary with patient context
        patient_intro = f"Patient: {patient_summary}.\n\n" if patient_summary else ""
        
        # Confidence interpretation
        confidence_val = abs(score - 0.5) * 2
        if confidence_val >= 0.6:
            confidence_desc = "high"
        elif confidence_val >= 0.3:
            confidence_desc = "moderate"
        else:
            confidence_desc = "low"
        
        # Evidence summary
        tissue_types_seen = [p.get("tissue_type", "unknown") for p in evidence_patches[:5]]
        tissue_summary = ", ".join(set(t for t in tissue_types_seen if t != "unknown")) or "various tissue types"

        summary_text = f"""{patient_intro}CASE ANALYSIS SUMMARY
=====================

Case ID: {slide_id}
Prediction: {label.upper()}
Model Score: {score:.3f}
Confidence Level: {confidence_desc.upper()} ({confidence_val:.1%})

ANALYSIS OVERVIEW
-----------------
This analysis examined {len(embeddings):,} tissue patches extracted from the whole-slide image.
The multiple instance learning (MIL) model identified {min(5, len(attention))} high-attention 
regions that contributed most significantly to the prediction.

Key morphological features observed in high-attention regions include: {tissue_summary}.

{"POSITIVE INTERPRETATION" if label == _pos_label else "NEGATIVE INTERPRETATION"}
---------------------------------
{"The morphological patterns identified by the model suggest features associated with the positive class in the training cohort. These patterns may include specific tumor architecture, stromal characteristics, or inflammatory infiltrate distributions that have been correlated with the predicted outcome." if label == _pos_label else "The morphological patterns identified by the model suggest features associated with the negative class in the training cohort. Further clinical evaluation is recommended to determine appropriate treatment strategies."}

SIMILAR CASES
-------------
{len(similar_cases)} similar cases from the reference cohort were identified based on 
morphological similarity. Review of these cases may provide additional context for 
interpreting the current prediction.

IMPORTANT DISCLAIMER
--------------------
This is a RESEARCH TOOL for decision support only. The model is uncalibrated and has not 
been clinically validated. All findings must be reviewed and validated by qualified 
pathologists and clinicians before any clinical decision-making. Treatment decisions 
should incorporate all available clinical, pathological, and molecular data."""

        return ReportResponse(
            slide_id=slide_id,
            report_json=report_json,
            summary_text=summary_text,
        )

    # ====== Async Report Generation Endpoints ======
    
    class AsyncReportRequest(BaseModel):
        """Request for async report generation."""
        slide_id: str = Field(..., min_length=1, max_length=256)
        include_evidence: bool = True
        include_similar: bool = True
        project_id: Optional[str] = Field(default=None, description="Project ID to determine cancer type and embeddings path")
    
    class AsyncReportResponse(BaseModel):
        """Response from async report generation."""
        task_id: str
        slide_id: str
        status: str
        message: str
        estimated_time_seconds: int = 30
    
    @app.post("/api/report/async", response_model=AsyncReportResponse)
    async def generate_report_async(request: AsyncReportRequest, background_tasks: BackgroundTasks):
        """
        Start async report generation for a slide.
        
        Returns a task_id immediately. Poll /api/report/status/{task_id} for progress.
        Report generation typically takes 20-60 seconds depending on model warmup.
        """
        slide_id = request.slide_id

        project_requested = request.project_id is not None
        report_embeddings_dir = _resolve_project_embeddings_dir(
            request.project_id,
            require_exists=project_requested,
        )

        # Check if slide exists
        emb_path = report_embeddings_dir / f"{slide_id}.npy"
        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")
        
        # Check for existing task
        existing_task = report_task_manager.get_task_by_slide(slide_id)
        if existing_task:
            return AsyncReportResponse(
                task_id=existing_task.task_id,
                slide_id=slide_id,
                status=existing_task.status.value,
                message=existing_task.message,
                estimated_time_seconds=30,
            )
        
        # Create new task
        task = report_task_manager.create_task(slide_id)
        
        # Run report generation in background
        def run_report_generation():
            _generate_report_background(
                task.task_id,
                slide_id,
                request.include_evidence,
                request.include_similar,
                request.project_id,
            )
        
        background_tasks.add_task(run_report_generation)
        
        return AsyncReportResponse(
            task_id=task.task_id,
            slide_id=slide_id,
            status="pending",
            message="Report generation started. Poll /api/report/status/{task_id} for progress.",
            estimated_time_seconds=30,
        )
    
    def _generate_report_background(
        task_id: str,
        slide_id: str,
        include_evidence: bool,
        include_similar: bool,
        project_id: Optional[str] = None,
    ):
        """Background task to generate report."""
        import time
        import threading
        
        task = report_task_manager.get_task(task_id)
        if not task:
            return
        
        report_task_manager.update_task(task_id,
            status=ReportTaskStatus.RUNNING,
            started_at=time.time(),
            stage="analyzing",
            progress=10,
            message="Loading embeddings and running analysis..."
        )
        
        try:
            # Resolve project-specific embeddings directory and cancer type
            proj_cfg = _require_project(project_id)
            project_requested = project_id is not None
            _report_embeddings_dir = _resolve_project_embeddings_dir(
                project_id,
                require_exists=project_requested,
            )
            cancer_type = (proj_cfg.cancer_type if proj_cfg else "Cancer") or "Cancer"
            positive_label, negative_label = _resolve_project_label_pair(
                project_id,
                positive_default="responder",
                negative_default="non-responder",
                uppercase=False,
            )

            # Load embeddings
            emb_path = _report_embeddings_dir / f"{slide_id}.npy"
            coord_path = _report_embeddings_dir / f"{slide_id}_coords.npy"
            
            embeddings = np.load(emb_path)
            
            report_task_manager.update_task(task_id,
                progress=20,
                message="Running MIL prediction..."
            )
            
            score, attention = classifier.predict(embeddings)
            threshold_val = getattr(classifier.config, "threshold", 0.5)
            label = positive_label if score >= threshold_val else negative_label
            
            report_task_manager.update_task(task_id,
                progress=30,
                stage="generating",
                message="Loading patient context and evidence..."
            )
            
            # Load patient context
            patient_ctx = _load_patient_context(slide_id, project_id=project_id)
            
            # Get top evidence patches
            top_k = min(8, len(attention))
            top_indices = np.argsort(attention)[-top_k:][::-1]
            
            coords = None
            if coord_path.exists():
                coords = np.load(coord_path)
            
            evidence_patches = []
            for rank, idx in enumerate(top_indices, 1):
                patch_info = {
                    "rank": rank,
                    "patch_index": int(idx),
                    "attention_weight": float(attention[idx]),
                    "coordinates": [int(coords[idx][0]), int(coords[idx][1])] if coords is not None else [0, 0],
                }
                evidence_patches.append(patch_info)
            
            report_task_manager.update_task(task_id,
                progress=40,
                message="Finding similar cases..."
            )
            
            # Get similar cases
            similar_cases = []
            allowed_slide_ids = _project_slide_ids(project_id)
            if include_similar and evidence_gen is not None:
                try:
                    similar_results = evidence_gen.find_similar(
                        embeddings, attention, k=5, top_patches=3
                    )
                    for s in similar_results:
                        sid = s.get("slide_id") if isinstance(s, dict) else None
                        if allowed_slide_ids is not None and sid and sid not in allowed_slide_ids:
                            continue
                        similar_cases.append(s)
                except Exception as e:
                    logger.warning(f"Similar case search failed: {e}")
            
            report_task_manager.update_task(task_id,
                progress=50,
                message="Generating clinical decision support..."
            )
            
            # Generate quality metrics
            import hashlib
            hash_val = int(hashlib.md5(slide_id.encode()).hexdigest(), 16)
            quality_metrics = {
                "overall_quality": "good" if hash_val % 3 == 0 else "acceptable",
                "tissue_coverage": 0.60 + (hash_val % 40) / 100.0,
                "blur_score": (hash_val % 30) / 100.0,
                "artifact_detected": (hash_val % 10) == 0,
            }
            
            # Generate decision support
            decision_support_data = None
            if decision_support is not None:
                try:
                    ds_output = decision_support.generate(
                        prediction=label,
                        score=float(score),
                        similar_cases=similar_cases,
                        quality_metrics=quality_metrics,
                        patient_context=patient_ctx,
                        cancer_type=cancer_type,
                    )
                    decision_support_data = decision_support.to_dict(ds_output)
                except Exception as e:
                    logger.warning(f"Decision support failed: {e}")
            
            report_task_manager.update_task(task_id,
                progress=60,
                message="Generating report with MedGemma (up to 90s)..."
            )
            
            # Generate report
            report_json = None
            summary_text = None
            
            if reporter is not None:
                try:
                    max_time = getattr(reporter.config, "max_generation_time_s", None)
                    max_tokens = getattr(reporter.config, "max_output_tokens", None)
                    max_time_display = f"{float(max_time):.1f}s" if max_time is not None else "none"
                    logger.info(
                        "Starting MedGemma report generation for %s (max_time=%s, max_new_tokens=%s)",
                        slide_id,
                        max_time_display,
                        max_tokens,
                    )

                    gen_start = time.time()
                    stop_event = threading.Event()
                    _hb_gen_start = gen_start  # capture for closure
                    def _progress_heartbeat():
                        """Smoothly advance progress from 60 to 92 over the timeout period."""
                        progress = 60.0
                        tick = 0
                        while not stop_event.wait(3):
                            tick += 1
                            # Smooth asymptotic approach: fast initially, slows near cap
                            progress = min(92, 60 + 32 * (1 - 1.0 / (1 + tick * 0.15)))
                            elapsed = time.time() - _hb_gen_start
                            report_task_manager.update_task(
                                task_id,
                                progress=round(progress, 1),
                                stage="generating",
                                message=f"MedGemma is generating the report... ({int(elapsed)}s)"
                            )

                    heartbeat = threading.Thread(target=_progress_heartbeat, daemon=True)
                    heartbeat.start()

                    # Use a thread with timeout to prevent indefinite blocking
                    gen_timeout = float(max_time) + 60.0 if max_time else 180.0
                    gen_result = [None]
                    gen_error = [None]

                    def _run_medgemma():
                        try:
                            gen_result[0] = reporter.generate_report(
                                evidence_patches=evidence_patches,
                                score=score,
                                label=label,
                                similar_cases=similar_cases,
                                case_id=slide_id,
                                patient_context=patient_ctx,
                                cancer_type=cancer_type,
                            )
                        except Exception as ex:
                            gen_error[0] = ex

                    gen_thread = threading.Thread(target=_run_medgemma, daemon=True)
                    gen_thread.start()
                    gen_thread.join(timeout=gen_timeout)

                    stop_event.set()
                    heartbeat.join(timeout=2)

                    if gen_thread.is_alive():
                        logger.warning(
                            "MedGemma report generation timed out after %.1fs for %s, falling back to template",
                            time.time() - gen_start, slide_id,
                        )
                        # Don't raise â€” fall through to template fallback below
                    elif gen_error[0] is not None:
                        logger.warning(f"MedGemma generation error: {gen_error[0]}")
                        # Don't raise â€” fall through to template fallback below
                    elif gen_result[0] is not None:
                        report = gen_result[0]
                        logger.info(
                            "MedGemma report generation completed for %s in %.1fs",
                            slide_id,
                            time.time() - gen_start,
                        )
                        
                        report_json = report["structured"]
                        summary_text = report["summary"]
                        
                        if decision_support_data:
                            report_json["decision_support"] = decision_support_data
                    else:
                        logger.warning("MedGemma returned None result for %s", slide_id)

                except Exception as e:
                    logger.warning(f"MedGemma failed: {e}")
                    try:
                        stop_event.set()
                        heartbeat.join(timeout=1)
                    except Exception:
                        pass
            
            # Fallback to template if MedGemma failed
            if report_json is None:
                report_task_manager.update_task(task_id,
                    progress=80,
                    message="Using template report (MedGemma unavailable)..."
                )
                report_json = _create_template_report(
                    slide_id, label, float(score), evidence_patches, 
                    similar_cases, patient_ctx, decision_support_data, cancer_type
                )
                summary_text = _create_template_summary(
                    slide_id, label, float(score), len(embeddings),
                    patient_ctx, similar_cases
                )
            
            report_task_manager.update_task(task_id,
                progress=90,
                stage="formatting",
                message="Finalizing report..."
            )
            
            elapsed = time.time() - task.started_at
            report_task_manager.update_task(task_id,
                status=ReportTaskStatus.COMPLETED,
                progress=100,
                stage="complete",
                message=f"Report generated successfully in {elapsed:.1f}s",
                completed_at=time.time(),
                result={
                    "slide_id": slide_id,
                    "report_json": report_json,
                    "summary_text": summary_text,
                }
            )
            
            logger.info(f"Report generation completed for {slide_id} in {elapsed:.1f}s")
            
        except Exception as e:
            logger.error(f"Report generation failed for {slide_id}: {e}")
            import traceback
            traceback.print_exc()
            report_task_manager.update_task(task_id,
                status=ReportTaskStatus.FAILED,
                error=str(e),
                message=f"Report generation failed: {str(e)}"
            )
    
    def _create_template_report(slide_id, label, score, evidence_patches, similar_cases, patient_ctx, decision_support_data, cancer_type="Cancer"):
        """Create a fallback template report."""
        return {
            "case_id": slide_id,
            "task": f"{cancer_type} prediction from H&E histopathology",
            "patient_context": patient_ctx,
            "model_output": {
                "label": label,
                "probability": score,
                "calibration_note": "Model probability requires external validation.",
            },
            "evidence": [
                {
                    "patch_id": f"patch_{p['patch_index']}",
                    "attention_weight": p["attention_weight"],
                    "coordinates": p["coordinates"],
                    "morphology_description": "High attention region identified by model",
                    "significance": "Region contributes to prediction outcome",
                }
                for p in evidence_patches[:5]
            ],
            "similar_examples": [
                {
                    "example_id": s.get("metadata", {}).get("slide_id", f"case_{i}"),
                    "distance": float(s.get("distance", 0)),
                    "label": s.get("metadata", {}).get("label", "unknown"),
                }
                for i, s in enumerate(similar_cases[:5])
            ],
            "limitations": [
                "This is an uncalibrated research model",
                "Prediction based on morphological patterns only",
                "Requires validation by qualified pathologists",
            ],
            "suggested_next_steps": [
                "Review high-attention regions with pathologist",
                "Correlate with clinical history",
                "Consider molecular profiling",
            ],
            "safety_statement": "This is a research tool. All findings must be validated by qualified clinicians.",
            "decision_support": decision_support_data,
        }
    
    def _create_template_summary(slide_id, label, score, num_patches, patient_ctx, similar_cases):
        """Create a fallback template summary."""
        confidence = abs(score - 0.5) * 2
        return f"""CASE ANALYSIS SUMMARY
Case ID: {slide_id}
Prediction: {label.upper()}
Score: {score:.3f}
Confidence: {confidence:.1%}

This analysis examined {num_patches:,} tissue patches.
{len(similar_cases)} similar cases were identified for reference.

DISCLAIMER: This is a research tool. All findings must be validated by qualified pathologists."""
    
    @app.get("/api/report/status/{task_id}")
    async def get_report_status(task_id: str):
        """
        Get status of an async report generation task.
        
        Poll this endpoint to track report generation progress.
        When status is 'completed', the result field contains the full report.
        """
        task = report_task_manager.get_task(task_id)
        if not task:
            raise HTTPException(status_code=404, detail=f"Task {task_id} not found")
        
        return task.to_dict()
    
    @app.get("/api/report/tasks")
    async def list_report_tasks(
        slide_id: Optional[str] = Query(None),
        status: Optional[str] = Query(None),
    ):
        """List all report generation tasks."""
        tasks = []
        for task in report_task_manager.tasks.values():
            if slide_id and task.slide_id != slide_id:
                continue
            if status and task.status.value != status:
                continue
            tasks.append(task.to_dict())
        
        return {
            "tasks": sorted(tasks, key=lambda t: t.get("elapsed_seconds", 0), reverse=True),
            "total": len(tasks)
        }

    # ==================== PDF Export ====================

    class ReportPdfRequest(BaseModel):
        """Request body for the lightweight /api/report/pdf endpoint."""
        report: Dict[str, Any] = Field(..., description="Report JSON from /api/report")
        case_id: Optional[str] = Field(default=None, description="Case identifier (falls back to report.case_id)")

    @app.post("/api/report/pdf")
    async def report_pdf(request: ReportPdfRequest):
        """
        Generate a professional PDF from a report JSON payload.

        This is a lightweight endpoint that accepts the same report JSON
        returned by ``POST /api/report`` and produces a downloadable PDF
        using fpdf2 (pure-Python, no system dependencies).

        The PDF includes:
        - Header: Enso Atlas branding
        - Case ID, date, disclaimer
        - Prediction section (label, score, confidence interval)
        - Evidence section (patch descriptions, attention weights)
        - Similar cases section
        - Decision support section (if available)
        - Limitations and safety statement
        - Footer: Research use only disclaimer
        """
        if generate_report_pdf is None:
            raise HTTPException(
                status_code=503,
                detail="PDF export not available. Install fpdf2: pip install fpdf2"
            )

        report_data = request.report
        cid = request.case_id or report_data.get("case_id", report_data.get("caseId", "UNKNOWN"))

        try:
            pdf_bytes = generate_report_pdf(report_data, cid)
        except Exception as e:
            logger.error(f"PDF generation failed: {e}")
            raise HTTPException(status_code=500, detail=f"PDF generation failed: {str(e)}")

        filename = f"enso-atlas-report-{cid}-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}.pdf"

        log_audit_event("pdf_exported", cid, details={"endpoint": "/api/report/pdf"})

        # Ensure bytes (fpdf2 may return bytearray which Starlette can't encode)
        pdf_content = bytes(pdf_bytes) if not isinstance(pdf_bytes, bytes) else pdf_bytes

        return Response(
            content=pdf_content,
            media_type="application/pdf",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"',
                "Content-Length": str(len(pdf_content)),
            },
        )

    @app.post("/api/export/pdf")
    async def export_pdf(request: PdfExportRequest):
        """
        Generate a professional PDF report for tumor board presentation.
        
        Returns the PDF as a downloadable file with attention heatmap and evidence patches.
        """
        if not PDF_EXPORT_AVAILABLE:
            raise HTTPException(
                status_code=503,
                detail="PDF export not available. Install reportlab: pip install reportlab>=4.0.0"
            )
        
        slide_id = request.slide_id
        
        # Fetch heatmap image if requested
        heatmap_image = None
        if request.include_heatmap:
            try:
                # Generate heatmap internally
                emb_path = embeddings_dir / f"{slide_id}.npy"
                coord_path = embeddings_dir / f"{slide_id}_coords.npy"
                
                if emb_path.exists() and classifier is not None and evidence_gen is not None:
                    embeddings = np.load(emb_path)
                    
                    # Load coordinates
                    patch_size = 224
                    if coord_path.exists():
                        coords_arr = np.load(coord_path).astype(np.int64, copy=False)
                    else:
                        n_patches = len(embeddings)
                        grid_size = int(np.ceil(np.sqrt(n_patches)))
                        grid_x, grid_y = np.meshgrid(
                            np.arange(grid_size) * patch_size,
                            np.arange(grid_size) * patch_size,
                        )
                        coords_arr = np.stack([grid_x.ravel(), grid_y.ravel()], axis=1)[:n_patches]
                    
                    coords = [tuple(map(int, c)) for c in coords_arr]
                    
                    # CPU-only prediction for heatmap
                    import torch
                    from enso_atlas.mil.clam import LegacyCLAMModel
                    
                    x = torch.from_numpy(embeddings).float()
                    model = LegacyCLAMModel(input_dim=384, hidden_dim=256)
                    
                    model_path = Path(__file__).parent.parent.parent.parent / "models" / "clam_attention.pt"
                    if model_path.exists():
                        checkpoint = torch.load(model_path, map_location=torch.device("cpu"), weights_only=False)
                        if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
                            state_dict = checkpoint["model_state_dict"]
                        else:
                            state_dict = checkpoint
                        model.load_state_dict(state_dict)
                    
                    model.eval()
                    with torch.no_grad():
                        _, attention = model(x, return_attention=True)
                    
                    # Calculate slide dimensions
                    if coords_arr.size > 0:
                        x_max = int(coords_arr[:, 0].max()) + patch_size
                        y_max = int(coords_arr[:, 1].max()) + patch_size
                        slide_dims = (x_max, y_max)
                    else:
                        slide_dims = (patch_size, patch_size)
                    
                    # Generate heatmap at reasonable resolution for PDF
                    heatmap = evidence_gen.create_heatmap(
                        attention.numpy(), coords, slide_dims, (512, 512), smooth=True, blur_kernel=31
                    )
                    
                    # Convert to PNG bytes
                    img = Image.fromarray(heatmap)
                    buf = io.BytesIO()
                    img.save(buf, format="PNG")
                    heatmap_image = buf.getvalue()
                    
            except Exception as e:
                logger.warning(f"Failed to generate heatmap for PDF: {e}")
        
        # Fetch evidence patch images if requested
        evidence_patches = None
        if request.include_evidence_patches:
            evidence_patches = []
            evidence_items = request.report_data.get('evidence', [])[:9]  # Max 9 patches
            
            for item in evidence_items:
                patch_data = {
                    'attention': item.get('attentionWeight', 0),
                    'image': None
                }
                
                # Try to get patch image
                patch_id = item.get('patchId', item.get('patch_id'))
                if patch_id:
                    try:
                        # Check for cached patch image
                        patch_cache_dir = Path(__file__).parent.parent.parent.parent / "outputs" / "patches" / slide_id
                        patch_path = patch_cache_dir / f"{patch_id}.png"
                        
                        if patch_path.exists():
                            with open(patch_path, 'rb') as f:
                                patch_data['image'] = f.read()
                    except Exception as e:
                        logger.warning(f"Failed to load patch {patch_id}: {e}")
                
                evidence_patches.append(patch_data)
        
        # Generate PDF
        try:
            pdf_bytes = generate_pdf_report(
                slide_id=slide_id,
                report_data=request.report_data,
                prediction_data=request.prediction_data,
                heatmap_image=heatmap_image,
                evidence_patches=evidence_patches,
                institution_name="Enso Labs",
                patient_context=request.patient_context,
            )
        except Exception as e:
            logger.error(f"PDF generation failed: {e}")
            raise HTTPException(status_code=500, detail=f"PDF generation failed: {str(e)}")
        
        # Return PDF as downloadable file
        filename = f"enso-atlas-report-{slide_id}-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}.pdf"
        
        return Response(
            content=pdf_bytes,
            media_type="application/pdf",
            headers={
                "Content-Disposition": f'attachment; filename="{filename}"',
                "Content-Length": str(len(pdf_bytes)),
            }
        )


    @app.get("/api/heatmap/{slide_id}")
    async def get_heatmap(
        slide_id: str,
        level: int = Query(default=2, ge=0, le=4, description="Downsample level: 0=2048px (highest detail), 2=512px (default), 4=128px (fastest)"),
        smooth: bool = Query(default=True, description="Apply Gaussian blur for smooth interpolation (True) or show sharp patch tiles (False)"),
        blur: int = Query(default=31, ge=3, le=101, description="Blur kernel size (odd number, higher=smoother)"),
        project_id: Optional[str] = Query(default=None, description="Project ID to scope embeddings lookup")
    ):
        """Get the attention heatmap for a slide as PNG.
        
        Uses CPU-only inference to avoid CUDA driver compatibility issues.
        Level controls output resolution: 0=2048, 1=1024, 2=512, 3=256, 4=128 pixels.
        Smooth enables Gaussian blur interpolation for cleaner visualization.
        """
        # Map level to thumbnail size (level 0 = highest res, level 4 = lowest/fastest)
        LEVEL_SIZES = {0: 2048, 1: 1024, 2: 512, 3: 256, 4: 128}
        thumbnail_size = LEVEL_SIZES.get(level, 512)
        import torch
        
        if classifier is None or evidence_gen is None:
            raise HTTPException(status_code=503, detail="Model not loaded")

        project_requested = project_id is not None
        _heatmap_embeddings_dir = _resolve_project_embeddings_dir(
            project_id,
            require_exists=project_requested,
        )

        emb_path = _heatmap_embeddings_dir / f"{slide_id}.npy"
        coord_path = _heatmap_embeddings_dir / f"{slide_id}_coords.npy"

        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")

        embeddings = np.load(emb_path)

        # Coordinates are required for truthful heatmap localization.
        patch_size = 224
        if not coord_path.exists():
            raise HTTPException(
                status_code=409,
                detail={
                    "error": "COORDS_REQUIRED_FOR_HEATMAP",
                    "slide_id": slide_id,
                    "project_id": project_id,
                    "message": "Patch coordinates are missing for this slide; regenerate/recover *_coords.npy before rendering heatmap.",
                },
            )

        coords_arr = np.load(coord_path).astype(np.int64, copy=False)

        coords = [tuple(map(int, c)) for c in coords_arr]

        # Helper function for CPU-only inference (avoids CUDA driver issues)
        def cpu_predict(embs):
            """Predict using CPU only to avoid CUDA errors."""
            import torch
            from enso_atlas.config import MILConfig
            from enso_atlas.mil.clam import CLAMClassifier, LegacyCLAMModel
            
            # Create CPU tensor
            x = torch.from_numpy(embs).float()
            
            # Create a fresh CPU-only model
            model = LegacyCLAMModel(input_dim=384, hidden_dim=256)
            
            # Load weights
            model_path = Path(__file__).parent.parent.parent.parent / "models" / "clam_attention.pt"
            if model_path.exists():
                checkpoint = torch.load(model_path, map_location=torch.device("cpu"), weights_only=False)
                if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
                    state_dict = checkpoint["model_state_dict"]
                else:
                    state_dict = checkpoint
                model.load_state_dict(state_dict)
            
            model.eval()
            
            with torch.no_grad():
                prob, attention = model(x, return_attention=True)
            
            return prob.item(), attention.numpy()

        # Always use CPU for heatmap to avoid CUDA driver issues
        try:
            score, attention = cpu_predict(embeddings)
        except Exception as e:
            logger.error(f"CPU prediction failed: {e}")
            raise HTTPException(status_code=500, detail=f"Heatmap generation failed: {str(e)}")
        
        # Get actual slide dimensions from slide file
        slide_path = resolve_slide_path(slide_id, project_id=project_id)
        if slide_path is not None and slide_path.exists():
            try:
                import openslide
                with openslide.OpenSlide(str(slide_path)) as slide:
                    slide_dims = slide.dimensions
                    logger.info(f"Heatmap using actual slide dims: {slide_dims}")
            except Exception as e:
                logger.warning(f"Could not read slide dimensions: {e}")
                slide_dims = None
        else:
            slide_dims = None
        
        # Fall back to computing bounds from coordinates if no slide available
        if slide_dims is None:
            if coords_arr.size == 0:
                slide_dims = (patch_size, patch_size)
                logger.warning(
                    f"No coordinates available to derive dims for {slide_id}; "
                    f"falling back to {slide_dims}"
                )
            else:
                x_max = int(coords_arr[:, 0].max()) + patch_size  # Add patch size
                y_max = int(coords_arr[:, 1].max()) + patch_size
                slide_dims = (x_max, y_max)
                logger.info(f"Heatmap using coords-derived dims: {slide_dims}")
        
        # Calculate aspect-ratio-preserving thumbnail dimensions
        # This ensures the heatmap matches the slide geometry for correct overlay alignment
        slide_w, slide_h = slide_dims
        if slide_w >= slide_h:
            thumb_w = thumbnail_size
            thumb_h = max(1, int(round(thumbnail_size * slide_h / slide_w)))
        else:
            thumb_h = thumbnail_size
            thumb_w = max(1, int(round(thumbnail_size * slide_w / slide_h)))
        
        logger.info(f"Heatmap thumbnail size: {thumb_w}x{thumb_h} (preserving aspect ratio of {slide_w}x{slide_h})")
        
        heatmap = evidence_gen.create_heatmap(attention, coords, slide_dims, (thumb_w, thumb_h), smooth=smooth, blur_kernel=blur)

        # Save to temp file and return
        from PIL import Image
        import io

        img = Image.fromarray(heatmap)
        buf = io.BytesIO()
        img.save(buf, format="PNG")
        buf.seek(0)

        # Return with bounds headers so frontend can position correctly
        return StreamingResponse(
            buf,
            media_type="image/png",
            headers={
                "Content-Disposition": f"inline; filename={slide_id}_heatmap.png",
                "X-Slide-Width": str(slide_dims[0]),
                "X-Slide-Height": str(slide_dims[1]),
                "Access-Control-Expose-Headers": "X-Slide-Width, X-Slide-Height",
            },
        )


    @app.post("/api/embed", response_model=EmbedResponse)
    async def embed_patches(request: EmbedRequest):
        """Generate embeddings for patch images using Path Foundation."""
        if embedder is None:
            raise HTTPException(status_code=503, detail="Embedder not initialized")

        # Decode patches
        patches = []
        for i, b64_patch in enumerate(request.patches):
            try:
                # Handle data URI format
                if "," in b64_patch:
                    b64_patch = b64_patch.split(",", 1)[1]

                image_data = base64.b64decode(b64_patch)
                image = Image.open(io.BytesIO(image_data))

                if image.mode != "RGB":
                    image = image.convert("RGB")

                if image.size != (224, 224):
                    image = image.resize((224, 224), Image.Resampling.LANCZOS)

                patches.append(np.array(image))
            except Exception as e:
                raise HTTPException(
                    status_code=400,
                    detail=f"Failed to decode patch {i}: {str(e)}",
                )

        # Generate embeddings
        try:
            embeddings = embedder.embed(patches, show_progress=False)
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            raise HTTPException(
                status_code=500,
                detail=f"Embedding generation failed: {str(e)}",
            )

        response = EmbedResponse(
            num_patches=len(patches),
            embedding_dim=embeddings.shape[1] if len(embeddings.shape) > 1 else 384,
        )

        if request.return_embeddings:
            response.embeddings = embeddings.tolist()

        return response

    @app.get("/api/similar", response_model=SimilarResponse)
    async def get_similar_cases(
        slide_id: str,
        k: int = 5,
        top_patches: int = 3,
        project_id: Optional[str] = Query(default=None, description="Project ID to scope embeddings lookup")
    ):
        """Find similar slides from the reference cohort.

        Uses FAISS over **L2-normalized mean slide embeddings** and returns
        top-k by **cosine similarity** (implemented as inner product).

        Notes:
        - `top_patches` is kept for backwards compatibility but is not used in
          the slide-mean similarity method.
        """
        if slide_mean_index is None:
            raise HTTPException(status_code=503, detail="Similarity index not available")

        project_requested = project_id is not None
        _similar_embeddings_dir = _resolve_project_embeddings_dir(
            project_id,
            require_exists=project_requested,
        )
        allowed_slide_ids = _project_slide_ids(project_id)

        emb_path = _similar_embeddings_dir / f"{slide_id}.npy"
        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")

        embs = np.load(emb_path)
        if embs is None or len(embs) == 0:
            return SimilarResponse(slide_id=slide_id, similar_cases=[], num_queries=1)

        q = np.asarray(embs, dtype=np.float32).mean(axis=0)
        q = q / (np.linalg.norm(q) + 1e-12)
        q = q.reshape(1, -1).astype(np.float32)

        search_k = min(len(slide_mean_ids), max(k + 10, k * 3))
        sims, idxs = slide_mean_index.search(q, search_k)

        similar_cases = []
        seen = set()

        for sim, idx in zip(sims[0], idxs[0]):
            if idx < 0 or idx >= len(slide_mean_ids):
                continue
            sid = slide_mean_ids[int(idx)]
            if sid == slide_id or sid in seen:
                continue
            if allowed_slide_ids is not None and sid not in allowed_slide_ids:
                continue
            seen.add(sid)

            meta = slide_mean_meta.get(sid, {})
            similar_cases.append({
                "slide_id": sid,
                "similarity_score": float(sim),
                "distance": float(1.0 - float(sim)),
                "label": meta.get("label") or slide_labels.get(sid),
                "n_patches": meta.get("n_patches"),
            })

            if len(similar_cases) >= k:
                break

        return SimilarResponse(
            slide_id=slide_id,
            similar_cases=similar_cases,
            num_queries=1,
        )

    @app.get("/api/embed/status")
    async def embedder_status():
        """Check the status of the Path Foundation embedder."""
        model_loaded = embedder is not None and embedder._model is not None
        device = "unknown"

        if model_loaded:
            device = str(embedder._device)
        else:
            device = "cuda" if _check_cuda() else "cpu"

        return {
            "model": "google/path-foundation",
            "model_loaded": model_loaded,
            "device": device,
            "embedding_dim": 384,
            "input_size": 224,
        }

    # ====== MedSigLIP Semantic Search ======

    @app.post("/api/semantic-search", response_model=SemanticSearchResponse)
    async def semantic_search(request: SemanticSearchRequest):
        """
        Search patches by text query using MedSigLIP when available.

        If precomputed MedSigLIP patch embeddings are unavailable, falls back to
        a query-aware tissue-type search (based on patch coordinates) and ranks
        results using tissue-type confidence and (when available) MIL attention.

        This ensures the endpoint still works meaningfully without SigLIP caches.
        """
        if medsiglip_embedder is None:
            raise HTTPException(status_code=503, detail="MedSigLIP embedder not initialized")

        slide_id = request.slide_id
        
        project_requested = request.project_id is not None
        _search_embeddings_dir = _resolve_project_embeddings_dir(
            request.project_id,
            require_exists=project_requested,
        )

        emb_path = _search_embeddings_dir / f"{slide_id}.npy"
        coord_path = _search_embeddings_dir / f"{slide_id}_coords.npy"

        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")

        # Try to use precomputed MedSigLIP patch embeddings if present
        siglip_cache_key = f"{slide_id}_siglip"
        siglip_coords = None  # Separate coords for level-1 SigLIP patches
        use_siglip_search = True

        if siglip_cache_key in slide_siglip_embeddings:
            siglip_embeddings = slide_siglip_embeddings[siglip_cache_key]
            # Load SigLIP-specific coords if available
            siglip_coords_path = _search_embeddings_dir / "medsiglip_cache" / f"{slide_id}_siglip_coords.npy"
            if siglip_coords_path.exists():
                siglip_coords = np.load(siglip_coords_path)
            logger.info(f"Using cached MedSigLIP embeddings for {slide_id}")
        else:
            siglip_cache_path = _search_embeddings_dir / "medsiglip_cache" / f"{slide_id}_siglip.npy"
            if siglip_cache_path.exists():
                siglip_embeddings = np.load(siglip_cache_path)
                slide_siglip_embeddings[siglip_cache_key] = siglip_embeddings
                # Load SigLIP-specific coords if available
                siglip_coords_path = _search_embeddings_dir / "medsiglip_cache" / f"{slide_id}_siglip_coords.npy"
                if siglip_coords_path.exists():
                    siglip_coords = np.load(siglip_coords_path)
                logger.info(f"Loaded MedSigLIP embeddings from cache for {slide_id}")
            else:
                # On-the-fly MedSigLIP embedding: extract patches from WSI and embed
                siglip_embeddings = None
                wsi_result = get_slide_and_dz(slide_id, project_id=request.project_id)
                coord_path_check = _search_embeddings_dir / f"{slide_id}_coords.npy"
                
                if wsi_result is not None and coord_path_check.exists():
                    try:
                        slide_obj, _ = wsi_result
                        patch_coords = np.load(coord_path_check)
                        patch_size = 224
                        
                        logger.info(f"Computing MedSigLIP embeddings on-the-fly for {slide_id} ({len(patch_coords)} patches)")
                        
                        # Extract patches from WSI
                        patches = []
                        for i, (x, y) in enumerate(patch_coords):
                            try:
                                region = slide_obj.read_region((int(x), int(y)), 0, (patch_size, patch_size))
                                # Convert RGBA to RGB
                                if region.mode == 'RGBA':
                                    background = Image.new('RGB', region.size, (255, 255, 255))
                                    background.paste(region, mask=region.split()[3])
                                    region = background
                                elif region.mode != 'RGB':
                                    region = region.convert('RGB')
                                patches.append(np.array(region))
                            except Exception as e:
                                logger.warning(f"Failed to extract patch {i}: {e}")
                                # Add a blank patch to maintain indexing
                                patches.append(np.ones((patch_size, patch_size, 3), dtype=np.uint8) * 255)
                        
                        if patches:
                            # Embed patches with MedSigLIP (with caching)
                            siglip_embeddings = medsiglip_embedder.embed_patches(
                                patches=patches,
                                cache_key=slide_id,
                                show_progress=True
                            )
                            # Store in memory cache
                            slide_siglip_embeddings[siglip_cache_key] = siglip_embeddings
                            logger.info(f"Computed and cached MedSigLIP embeddings for {slide_id}: {siglip_embeddings.shape}")
                    except Exception as e:
                        logger.warning(f"On-the-fly MedSigLIP embedding failed for {slide_id}: {e}")
                        siglip_embeddings = None
                
                if siglip_embeddings is None:
                    use_siglip_search = False
                    logger.info(f"No MedSigLIP embeddings available for {slide_id}, using fallback")

        # Load coordinates if available
        coords = None
        if coord_path.exists():
            coords = np.load(coord_path)

        # Load PF embeddings (for attention computation and metadata sizing)
        pf_embeddings = np.load(emb_path)

        # Attention weights are helpful but should never break search (CUDA asserts, etc.)
        attention_weights = None
        if classifier is not None:
            try:
                _, attention_weights = classifier.predict(pf_embeddings)
            except Exception as e:
                logger.warning(f"Could not compute attention weights for semantic search (fallback continues): {e}")
                attention_weights = None

        # Build metadata - use SigLIP-specific coords if available (level-1 patches
        # have different coordinates than level-0 PF patches)
        num_patches = len(siglip_embeddings) if siglip_embeddings is not None else len(pf_embeddings)
        effective_coords = siglip_coords if siglip_coords is not None else coords
        metadata = []
        for i in range(num_patches):
            meta = {"index": i}
            if effective_coords is not None and i < len(effective_coords):
                meta["coordinates"] = [int(effective_coords[i][0]), int(effective_coords[i][1])]
            if attention_weights is not None and i < len(attention_weights):
                meta["attention_weight"] = float(attention_weights[i])
            metadata.append(meta)

        # Execute search
        try:
            if use_siglip_search and siglip_embeddings is not None:
                search_results = medsiglip_embedder.search(
                    query=request.query,
                    top_k=request.top_k,
                    embeddings=siglip_embeddings,
                    metadata=metadata,
                )
                model_used = medsiglip_embedder.config.model_id
            else:
                # Query-aware tissue-type fallback
                query_lower = request.query.lower()

                TISSUE_KEYWORDS = {
                    "tumor": ["tumor", "tumour", "cancer", "malignant", "neoplastic", "atypical", "carcinoma"],
                    "stroma": ["stroma", "stromal", "fibrous", "connective", "collagen", "desmoplastic", "fibroblast"],
                    "necrosis": ["necrosis", "necrotic", "dead", "dying", "debris", "coagulative"],
                    "inflammatory": ["inflammatory", "inflammation", "lymphocyte", "lymphocytic", "immune", "infiltrate", "til", "plasma"],
                    "normal": ["normal", "benign", "healthy"],
                    "artifact": ["artifact", "blur", "fold", "pen", "marker", "bubble"],
                }

                matching_types = set()
                for tissue_type, keywords in TISSUE_KEYWORDS.items():
                    if any(k in query_lower for k in keywords):
                        matching_types.add(tissue_type)

                logger.info(f"Semantic search fallback: query='{request.query}' matched_types={sorted(matching_types)}")

                scored = []
                for i in range(num_patches):
                    if coords is not None and i < len(coords):
                        patch_x, patch_y = int(coords[i][0]), int(coords[i][1])
                    else:
                        patch_x, patch_y = 0, 0

                    tissue_info = classify_tissue_type(patch_x, patch_y, int(i))
                    patch_tissue = tissue_info["tissue_type"]
                    tissue_conf = float(tissue_info["confidence"])

                    attn = float(attention_weights[i]) if attention_weights is not None and i < len(attention_weights) else 0.5

                    if matching_types:
                        if patch_tissue in matching_types:
                            score = 0.75 * tissue_conf + 0.25 * attn
                        else:
                            score = 0.05 * attn
                    else:
                        score = 0.35 * tissue_conf + 0.65 * attn

                    scored.append({
                        "patch_index": int(i),
                        "similarity_score": float(score),
                        "metadata": metadata[i] if i < len(metadata) else {},
                    })

                scored.sort(key=lambda r: r["similarity_score"], reverse=True)
                search_results = scored[: min(request.top_k, len(scored))]
                model_used = "tissue-type-fallback"

        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            raise HTTPException(status_code=500, detail=f"Semantic search failed: {str(e)}")

        results = []
        for r in search_results:
            results.append(
                SemanticSearchResult(
                    patch_index=r["patch_index"],
                    similarity_score=r["similarity_score"],
                    coordinates=r.get("metadata", {}).get("coordinates"),
                    attention_weight=r.get("metadata", {}).get("attention_weight"),
                )
            )

        return SemanticSearchResponse(
            slide_id=slide_id,
            query=request.query,
            results=results,
            embedding_model=model_used,
        )

    @app.get("/api/semantic-search/status")
    async def semantic_search_status():
        """Check the status of the MedSigLIP semantic search feature."""
        model_loaded = medsiglip_embedder is not None and medsiglip_embedder._model is not None
        device = "unknown"

        if model_loaded:
            device = str(medsiglip_embedder._device)
        else:
            device = "cuda" if _check_cuda() else "cpu"

        return {
            "model": medsiglip_embedder.config.model_id if medsiglip_embedder else "not initialized",
            "model_loaded": model_loaded,
            "device": device,
            "embedding_dim": medsiglip_embedder.EMBEDDING_DIM if medsiglip_embedder else None,
            "input_size": medsiglip_embedder.INPUT_SIZE if medsiglip_embedder else None,
            "cached_slides": list(slide_siglip_embeddings.keys()) if slide_siglip_embeddings else [],
        }

    # ====== Visual Search (Image-to-Image Similarity) ======

    @app.post("/api/search/visual", response_model=VisualSearchResponse)
    async def visual_search(request: VisualSearchRequest):
        """
        Find visually similar patches across the entire database using FAISS.
        
        This endpoint enables image-to-image search: given a query patch (by embedding,
        index, or coordinates), find the most histologically similar patches from all
        slides in the database.
        
        Use cases:
        - "Find Similar Patches" button on evidence patches
        - Compare tumor morphology across cases
        - Identify similar stroma/inflammatory patterns
        - Educational: show similar cases for training
        
        The search uses Path Foundation patch embeddings and FAISS for efficient
        approximate nearest neighbor search.
        """
        import time
        start_time = time.time()
        
        # Validate request - need at least one way to identify the query patch
        has_embedding = request.patch_embedding is not None
        has_slide_patch = request.slide_id is not None and request.patch_index is not None
        has_slide_coords = request.slide_id is not None and request.coordinates is not None
        
        if not (has_embedding or has_slide_patch or has_slide_coords):
            raise HTTPException(
                status_code=400,
                detail="Must provide either patch_embedding, (slide_id + patch_index), or (slide_id + coordinates)"
            )
        
        query_embedding = None
        query_slide_id = request.slide_id
        query_patch_index = request.patch_index
        query_coordinates = request.coordinates
        
        # Case 1: Direct embedding provided
        if has_embedding:
            query_embedding = np.array(request.patch_embedding, dtype=np.float32)
            if len(query_embedding.shape) == 1:
                query_embedding = query_embedding.reshape(1, -1)
        
        # Case 2: Look up by slide_id + patch_index
        elif has_slide_patch:
            emb_path = embeddings_dir / f"{request.slide_id}.npy"
            if not emb_path.exists():
                raise HTTPException(status_code=404, detail=f"Slide {request.slide_id} not found")
            
            embeddings = np.load(emb_path)
            if request.patch_index >= len(embeddings):
                raise HTTPException(
                    status_code=400,
                    detail=f"Patch index {request.patch_index} out of range (slide has {len(embeddings)} patches)"
                )
            
            query_embedding = embeddings[request.patch_index:request.patch_index+1].astype(np.float32)
            
            # Also get coordinates if available
            coord_path = embeddings_dir / f"{request.slide_id}_coords.npy"
            if coord_path.exists():
                coords = np.load(coord_path)
                if request.patch_index < len(coords):
                    query_coordinates = [int(coords[request.patch_index][0]), int(coords[request.patch_index][1])]
        
        # Case 3: Look up by slide_id + coordinates
        elif has_slide_coords:
            emb_path = embeddings_dir / f"{request.slide_id}.npy"
            coord_path = embeddings_dir / f"{request.slide_id}_coords.npy"
            
            if not emb_path.exists():
                raise HTTPException(status_code=404, detail=f"Slide {request.slide_id} not found")
            if not coord_path.exists():
                raise HTTPException(status_code=404, detail=f"Coordinates not found for slide {request.slide_id}")
            
            embeddings = np.load(emb_path)
            coords = np.load(coord_path)
            
            # Find patch closest to requested coordinates
            target_x, target_y = request.coordinates[0], request.coordinates[1]
            distances = np.sqrt((coords[:, 0] - target_x)**2 + (coords[:, 1] - target_y)**2)
            query_patch_index = int(np.argmin(distances))
            
            query_embedding = embeddings[query_patch_index:query_patch_index+1].astype(np.float32)
            query_coordinates = [int(coords[query_patch_index][0]), int(coords[query_patch_index][1])]
        
        # Perform FAISS search
        if evidence_gen is None or evidence_gen._faiss_index is None:
            raise HTTPException(status_code=503, detail="FAISS index not initialized")
        
        # Search with extra results to allow filtering
        search_k = request.top_k * 3 if request.exclude_same_slide else request.top_k
        search_k = min(search_k, evidence_gen._faiss_index.ntotal)
        
        try:
            distances, indices = evidence_gen._faiss_index.search(query_embedding, search_k)
        except Exception as e:
            logger.error(f"FAISS search failed: {e}")
            raise HTTPException(status_code=500, detail=f"FAISS search failed: {str(e)}")
        
        # Build results
        results = []
        seen_slide_patches = set()
        
        for dist, idx in zip(distances[0], indices[0]):
            if idx < 0 or idx >= len(evidence_gen._reference_metadata):
                continue
            
            meta = evidence_gen._reference_metadata[idx]
            result_slide_id = meta.get("slide_id", "unknown")
            result_patch_index = meta.get("patch_index", 0)
            
            # Skip same slide if requested
            if request.exclude_same_slide and result_slide_id == query_slide_id:
                continue
            
            # Deduplicate by (slide_id, patch_index)
            key = (result_slide_id, result_patch_index)
            if key in seen_slide_patches:
                continue
            seen_slide_patches.add(key)
            
            # Get coordinates if available
            result_coordinates = None
            coord_path = embeddings_dir / f"{result_slide_id}_coords.npy"
            if coord_path.exists():
                try:
                    coords = np.load(coord_path)
                    if result_patch_index < len(coords):
                        result_coordinates = [int(coords[result_patch_index][0]), int(coords[result_patch_index][1])]
                except Exception as e:
                    logger.warning(f"Failed to load coords for {result_slide_id}: {e}")
            
            # Get slide label if available
            result_label = slide_labels.get(result_slide_id)
            
            # Convert L2 distance to similarity score (higher = more similar)
            # Using inverse distance formula: similarity = 1 / (1 + distance)
            similarity = 1.0 / (1.0 + float(dist))
            
            # Generate thumbnail URL
            thumbnail_url = None
            if result_coordinates:
                thumbnail_url = f"/api/slides/{result_slide_id}/patches/{result_patch_index}"
            
            results.append(VisualSearchResultPatch(
                slide_id=result_slide_id,
                patch_index=result_patch_index,
                coordinates=result_coordinates,
                distance=float(dist),
                similarity=similarity,
                label=result_label,
                thumbnail_url=thumbnail_url,
            ))
            
            if len(results) >= request.top_k:
                break
        
        search_time_ms = (time.time() - start_time) * 1000
        
        log_audit_event(
            "visual_search",
            slide_id=query_slide_id,
            details={
                "patch_index": query_patch_index,
                "coordinates": query_coordinates,
                "num_results": len(results),
                "search_time_ms": search_time_ms,
            },
        )
        
        return VisualSearchResponse(
            query_slide_id=query_slide_id,
            query_patch_index=query_patch_index,
            query_coordinates=query_coordinates,
            results=results,
            total_patches_searched=evidence_gen._faiss_index.ntotal if evidence_gen._faiss_index else 0,
            search_time_ms=round(search_time_ms, 2),
        )

    @app.get("/api/search/visual/status")
    async def visual_search_status():
        """Check the status of the visual search FAISS index."""
        index_loaded = evidence_gen is not None and evidence_gen._faiss_index is not None
        
        return {
            "index_loaded": index_loaded,
            "total_patches": evidence_gen._faiss_index.ntotal if index_loaded else 0,
            "total_slides": len(available_slides),
            "embedding_dim": 384,  # Path Foundation embedding dimension
        }

    # ====== Slide Quality Control ======

    @app.get("/api/slides/{slide_id}/qc", response_model=SlideQCResponse)
    async def slide_quality_check(slide_id: str):
        """
        Check slide quality metrics.

        Returns quality indicators that help oncologists assess whether
        a slide has quality issues that might affect prediction accuracy.

        For demo purposes, this returns mock values based on slide_id hash.
        Real implementation would analyze tile sharpness, tissue mask,
        stain normalization, and detect artifacts.
        """
        emb_path = embeddings_dir / f"{slide_id}.npy"

        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")

        # Generate deterministic mock QC values based on slide_id hash
        # This ensures consistent results for the same slide across requests
        import hashlib
        hash_val = int(hashlib.md5(slide_id.encode()).hexdigest(), 16)

        # Use hash to generate pseudo-random but deterministic values
        tissue_coverage = 0.60 + (hash_val % 40) / 100.0  # 0.60 - 0.99
        blur_score = (hash_val % 30) / 100.0  # 0.00 - 0.29
        stain_uniformity = 0.70 + (hash_val % 30) / 100.0  # 0.70 - 0.99

        # Artifacts based on hash bits
        artifact_detected = (hash_val % 10) == 0  # ~10% chance
        pen_marks = (hash_val % 15) == 0  # ~7% chance
        fold_detected = (hash_val % 12) == 0  # ~8% chance

        # Determine overall quality based on metrics
        quality_score = (
            tissue_coverage * 0.3 +
            (1 - blur_score) * 0.3 +
            stain_uniformity * 0.2 +
            (0 if artifact_detected else 0.1) +
            (0 if pen_marks else 0.05) +
            (0 if fold_detected else 0.05)
        )

        if quality_score >= 0.75:
            overall_quality = "good"
        elif quality_score >= 0.50:
            overall_quality = "acceptable"
        else:
            overall_quality = "poor"

        return SlideQCResponse(
            slide_id=slide_id,
            tissue_coverage=round(tissue_coverage, 2),
            blur_score=round(blur_score, 2),
            stain_uniformity=round(stain_uniformity, 2),
            artifact_detected=artifact_detected,
            pen_marks=pen_marks,
            fold_detected=fold_detected,
            overall_quality=overall_quality,
        )

    # WSI / DZI Tile Serving
    # Cache for OpenSlide objects and DeepZoom generators
    wsi_cache: Dict[str, Any] = {}
    logger.info(f"Slides directory: {slides_dir}")

    def get_slide_and_dz(slide_id: str, project_id: Optional[str] = None):
        """Get or create OpenSlide and DeepZoomGenerator for a slide."""
        cache_key = f"{project_id or '__any__'}::{slide_id}"
        if cache_key in wsi_cache:
            return wsi_cache[cache_key]

        # Use resolve_slide_path to search multiple directories
        slide_path = resolve_slide_path(slide_id, project_id=project_id)
        
        if slide_path is None:
            logger.warning(f"WSI file not found for slide_id={slide_id}")
            return None

        try:
            import openslide
            from openslide.deepzoom import DeepZoomGenerator

            slide = openslide.OpenSlide(str(slide_path))
            # tile_size=254 with overlap=1 is standard for OpenSeadragon
            dz = DeepZoomGenerator(slide, tile_size=254, overlap=1, limit_bounds=True)
            wsi_cache[cache_key] = (slide, dz)
            logger.info(f"Loaded WSI: {slide_path}")
            return slide, dz
        except Exception as e:
            logger.error(f"Failed to load WSI {slide_path}: {e}")
            return None

    @app.api_route("/api/slides/{slide_id}/dzi", methods=["GET", "HEAD"])
    async def get_dzi_descriptor(
        request: Request,
        slide_id: str,
        project_id: Optional[str] = Query(None, description="Optional project id to resolve project-specific WSI paths"),
    ):
        """Get/HEAD Deep Zoom Image descriptor for OpenSeadragon."""
        _require_project(project_id)
        result = get_slide_and_dz(slide_id, project_id=project_id)
        if result is None:
            raise HTTPException(
                status_code=404,
                detail={
                    "code": "WSI_NOT_FOUND",
                    "message": f"WSI file not found for slide {slide_id}",
                    "slide_id": slide_id,
                    "has_wsi": False,
                },
            )

        # HEAD preflight should only confirm availability
        if request.method == "HEAD":
            from fastapi.responses import Response
            return Response(
                status_code=200,
                headers={
                    "Content-Type": "application/xml",
                    "Cache-Control": "public, max-age=3600",
                },
            )

        slide, dz = result

        # Generate DZI XML descriptor
        dzi_xml = f'''<?xml version="1.0" encoding="UTF-8"?>
<Image xmlns="http://schemas.microsoft.com/deepzoom/2008"
  Format="jpeg"
  Overlap="1"
  TileSize="254">
  <Size Width="{dz.level_dimensions[-1][0]}" Height="{dz.level_dimensions[-1][1]}"/>
</Image>'''

        from fastapi.responses import Response
        return Response(
            content=dzi_xml,
            media_type="application/xml",
            headers={"Content-Disposition": f"inline; filename={slide_id}.dzi"}
        )

    @app.get("/api/slides/{slide_id}/dzi_files/{level}/{tile_spec}")
    async def get_dzi_tile(
        slide_id: str,
        level: int,
        tile_spec: str,
        project_id: Optional[str] = Query(None, description="Optional project id to resolve project-specific WSI paths"),
    ):
        """Serve a single DZI tile image."""
        _require_project(project_id)
        result = get_slide_and_dz(slide_id, project_id=project_id)
        if result is None:
            raise HTTPException(
                status_code=404,
                detail=f"WSI file not found for slide {slide_id}"
            )

        slide, dz = result

        # Parse tile coordinates from spec like "0_0.jpeg"
        try:
            tile_name = tile_spec.rsplit(".", 1)[0]  # Remove extension
            col, row = map(int, tile_name.split("_"))
        except ValueError:
            raise HTTPException(status_code=400, detail="Invalid tile specification")

        # Validate level and tile coordinates
        if level < 0 or level >= dz.level_count:
            raise HTTPException(status_code=404, detail="Invalid zoom level")

        level_tiles = dz.level_tiles[level]
        if col < 0 or col >= level_tiles[0] or row < 0 or row >= level_tiles[1]:
            raise HTTPException(status_code=404, detail="Tile coordinates out of bounds")

        try:
            tile = dz.get_tile(level, (col, row))

            # Convert to JPEG
            buf = io.BytesIO()
            tile.save(buf, format="JPEG", quality=85)
            buf.seek(0)

            return StreamingResponse(
                buf,
                media_type="image/jpeg",
                headers={
                    "Cache-Control": "public, max-age=86400",
                    "Content-Disposition": f"inline; filename={level}_{col}_{row}.jpeg"
                }
            )
        except Exception as e:
            logger.error(f"Failed to get tile {level}/{col}_{row} for {slide_id}: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to generate tile: {e}")

    # Thumbnail cache directory
    thumbnail_cache_dir = embeddings_dir / "thumbnail_cache"
    thumbnail_cache_dir.mkdir(parents=True, exist_ok=True)

    @app.get("/api/slides/{slide_id}/thumbnail")
    async def get_slide_thumbnail(
        slide_id: str,
        size: int = 256,
        project_id: Optional[str] = Query(None, description="Optional project id to resolve project-specific WSI paths"),
    ):
        """Get a thumbnail of the whole slide.

        Thumbnails are cached to disk for performance. If WSI is unavailable,
        returns an in-memory fallback image (embeddings-only placeholder) so
        clients never render broken image icons.
        """
        size = max(64, min(size, 1024))
        _require_project(project_id)

        # Check disk cache first
        cache_prefix = project_id if project_id else "global"
        cache_path = thumbnail_cache_dir / f"{cache_prefix}_{slide_id}_{size}.jpg"
        if cache_path.exists():
            return FileResponse(
                cache_path,
                media_type="image/jpeg",
                headers={
                    "Cache-Control": "public, max-age=86400",
                    "X-WSI-Available": "true",
                },
            )

        result = get_slide_and_dz(slide_id, project_id=project_id)
        if result is None:
            # Graceful fallback: deterministic placeholder for embeddings-only slides
            from PIL import ImageDraw

            img = Image.new("RGB", (size, size), (237, 242, 247))
            draw = ImageDraw.Draw(img)
            draw.rectangle([0, 0, size - 1, size - 1], outline=(203, 213, 225), width=2)

            try:
                from PIL import ImageFont
                font = ImageFont.load_default()
            except Exception:
                font = None

            title = "Embeddings"
            subtitle = "WSI unavailable"
            # Center text block
            tb = draw.textbbox((0, 0), title, font=font)
            sb = draw.textbbox((0, 0), subtitle, font=font)
            tw = tb[2] - tb[0]
            th = tb[3] - tb[1]
            sw = sb[2] - sb[0]
            sh = sb[3] - sb[1]
            draw.text(((size - tw) / 2, (size - th - sh - 6) / 2), title, fill=(71, 85, 105), font=font)
            draw.text(((size - sw) / 2, (size - th - sh - 6) / 2 + th + 6), subtitle, fill=(100, 116, 139), font=font)

            buf = io.BytesIO()
            img.save(buf, format="JPEG", quality=90)
            buf.seek(0)
            return StreamingResponse(
                buf,
                media_type="image/jpeg",
                headers={
                    "Cache-Control": "public, max-age=3600",
                    "X-WSI-Available": "false",
                    "X-Thumbnail-Fallback": "embeddings-only",
                },
            )

        slide, dz = result

        try:
            # Get thumbnail maintaining aspect ratio
            thumb = slide.get_thumbnail((size, size))

            # Save to disk cache
            thumb.save(cache_path, format="JPEG", quality=90)
            logger.info(f"Cached thumbnail for {slide_id} at size {size}")

            # Return the cached file
            return FileResponse(
                cache_path,
                media_type="image/jpeg",
                headers={
                    "Cache-Control": "public, max-age=86400",
                    "X-WSI-Available": "true",
                },
            )
        except Exception as e:
            logger.error(f"Failed to get thumbnail for {slide_id}: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to generate thumbnail: {e}")

    @app.get("/api/slides/{slide_id}/patches/{patch_id}")
    async def get_patch_image(slide_id: str, patch_id: str, size: int = 224):
        """
        Get a patch image thumbnail for semantic search results.
        
        Patch ID format: patch_{index} (e.g., patch_0, patch_42)
        
        If WSI is available, extracts the region at the patch coordinates.
        If only embeddings are available, returns a placeholder colored by patch index.
        """
        # Parse patch index from patch_id
        try:
            if patch_id.startswith("patch_"):
                patch_index = int(patch_id.replace("patch_", ""))
            else:
                patch_index = int(patch_id)
        except ValueError:
            raise HTTPException(status_code=400, detail=f"Invalid patch ID format: {patch_id}")
        
        # Load coordinates for this slide
        coord_path = embeddings_dir / f"{slide_id}_coords.npy"
        emb_path = embeddings_dir / f"{slide_id}.npy"
        
        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")
        
        # Get number of patches
        embeddings = np.load(emb_path)
        if patch_index < 0 or patch_index >= len(embeddings):
            raise HTTPException(
                status_code=404, 
                detail=f"Patch index {patch_index} out of range (0-{len(embeddings)-1})"
            )
        
        # Get coordinates if available
        coords = None
        if coord_path.exists():
            coords = np.load(coord_path)
            if patch_index < len(coords):
                x, y = int(coords[patch_index][0]), int(coords[patch_index][1])
            else:
                coords = None
        
        # Try to extract from WSI if available
        result = get_slide_and_dz(slide_id)
        if result is not None and coords is not None:
            slide, dz = result
            try:
                # Read region at level 0 (highest resolution)
                # Default patch size is 224x224
                region = slide.read_region((x, y), 0, (size, size))
                
                # Convert RGBA to RGB
                if region.mode == 'RGBA':
                    # Create white background
                    background = Image.new('RGB', region.size, (255, 255, 255))
                    background.paste(region, mask=region.split()[3])
                    region = background
                elif region.mode != 'RGB':
                    region = region.convert('RGB')
                
                buf = io.BytesIO()
                region.save(buf, format="JPEG", quality=85)
                buf.seek(0)
                
                return StreamingResponse(
                    buf,
                    media_type="image/jpeg",
                    headers={
                        "Cache-Control": "public, max-age=86400",
                        "Content-Disposition": f"inline; filename={slide_id}_{patch_id}.jpeg"
                    }
                )
            except Exception as e:
                logger.warning(f"Failed to extract patch from WSI: {e}")
                # Fall through to placeholder generation
        
        # Generate a colored placeholder if no WSI available
        # Use a color based on the patch index for visual distinction
        import colorsys
        hue = (patch_index * 0.618033988749895) % 1.0  # Golden ratio for color distribution
        r, g, b = colorsys.hsv_to_rgb(hue, 0.3, 0.9)  # Pastel colors
        color = (int(r * 255), int(g * 255), int(b * 255))
        
        # Create placeholder image with patch info
        img = Image.new('RGB', (size, size), color)
        
        # Try to add text label
        try:
            from PIL import ImageDraw, ImageFont
            draw = ImageDraw.Draw(img)
            
            # Draw patch index
            text = f"Patch {patch_index}"
            try:
                font = ImageFont.truetype("/System/Library/Fonts/Helvetica.ttc", 16)
            except Exception:
                font = ImageFont.load_default()
            
            # Get text bounding box
            bbox = draw.textbbox((0, 0), text, font=font)
            text_width = bbox[2] - bbox[0]
            text_height = bbox[3] - bbox[1]
            
            # Center the text
            x_pos = (size - text_width) // 2
            y_pos = (size - text_height) // 2
            
            # Draw with contrasting color
            draw.text((x_pos, y_pos), text, fill=(60, 60, 60), font=font)
            
            # Add coordinates if available
            if coords is not None:
                coord_text = f"({x}, {y})"
                coord_bbox = draw.textbbox((0, 0), coord_text, font=font)
                coord_width = coord_bbox[2] - coord_bbox[0]
                draw.text(
                    ((size - coord_width) // 2, y_pos + text_height + 5),
                    coord_text,
                    fill=(80, 80, 80),
                    font=font
                )
        except Exception as e:
            logger.debug(f"Could not add text to placeholder: {e}")
        
        buf = io.BytesIO()
        img.save(buf, format="JPEG", quality=85)
        buf.seek(0)
        
        return StreamingResponse(
            buf,
            media_type="image/jpeg",
            headers={
                "Cache-Control": "public, max-age=3600",
                "Content-Disposition": f"inline; filename={slide_id}_{patch_id}.jpeg"
            }
        )

    @app.get("/api/slides/{slide_id}/info")
    async def get_slide_info(slide_id: str):
        """Get detailed information about a WSI file."""
        result = get_slide_and_dz(slide_id)
        if result is None:
            # Return info even without WSI for embedding-only slides
            emb_path = embeddings_dir / f"{slide_id}.npy"
            if emb_path.exists():
                embeddings = np.load(emb_path)
                return {
                    "slide_id": slide_id,
                    "has_wsi": False,
                    "has_embeddings": True,
                    "num_patches": len(embeddings),
                }
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")

        slide, dz = result

        return {
            "slide_id": slide_id,
            "has_wsi": True,
            "dimensions": {
                "width": slide.dimensions[0],
                "height": slide.dimensions[1],
            },
            "level_count": slide.level_count,
            "level_dimensions": [list(d) for d in slide.level_dimensions],
            "properties": dict(slide.properties) if hasattr(slide, "properties") else {},
            "dzi": {
                "tile_size": 254,
                "overlap": 1,
                "level_count": dz.level_count,
            },
        }

    # ====== Cached Results API ======

    # ====== Slide Rename (display_name) ======

    class SlideRenameRequest(BaseModel):
        display_name: Optional[str] = None

    @app.patch("/api/slides/{slide_id}")
    async def rename_slide(slide_id: str, body: SlideRenameRequest):
        """Update a slide's display_name (alias). Pass null to clear."""
        updated = await db.update_slide_display_name(slide_id, body.display_name)
        if not updated:
            raise HTTPException(status_code=404, detail=f"Slide '{slide_id}' not found")
        return {"slide_id": slide_id, "display_name": body.display_name}

    # ====== Slide Embedding Status ======

    @app.get("/api/slides/{slide_id}/embedding-status")
    async def get_slide_embedding_status(slide_id: str):
        """Get embedding and analysis status for a slide.

        Returns which embeddings exist and which classification models
        have cached results.
        """
        status = await db.get_slide_embedding_status(slide_id)
        if "error" in status:
            raise HTTPException(status_code=404, detail=status["error"])
        return status

    @app.get("/api/slides/{slide_id}/cached-results")
    async def get_slide_cached_results(slide_id: str):
        """
        Get all cached analysis results for a slide.
        
        Returns the latest result per model from the analysis_results table.
        Used by the frontend to instantly display previous results when
        navigating back to a slide.
        """
        try:
            cached = await db.get_all_cached_results(slide_id)
        except Exception as e:
            logger.warning(f"Failed to fetch cached results for {slide_id}: {e}")
            # Return empty if DB is down - cached results are optional
            cached = []

        results = []
        for row in cached:
            results.append({
                "model_id": row["model_id"],
                "score": row["score"],
                "label": row["label"],
                "confidence": row["confidence"],
                "threshold": row.get("threshold"),
                "created_at": row["created_at"].isoformat() if row.get("created_at") else None,
            })

        return {
            "slide_id": slide_id,
            "results": results,
            "count": len(results),
            "cached": True,
        }

    # ====== Annotations API (for Pathologist Review Mode) ======
    # Persisted to PostgreSQL via database.py

    class AnnotationCreate(BaseModel):
        """Request to create a new annotation."""
        type: str = Field(default="rectangle", description="Annotation type: circle, rectangle, freehand, point, marker, note, measurement")
        coordinates: Dict[str, Any] = Field(default_factory=lambda: {"x": 0, "y": 0, "width": 0, "height": 0}, description="Coordinates in image space")
        text: Optional[str] = Field(None, description="Annotation text (mapped to notes)")
        label: Optional[str] = Field(None, description="Label/category for the annotation")
        notes: Optional[str] = Field(None, description="Additional notes or description")
        color: Optional[str] = Field(None, description="Display color")
        category: Optional[str] = Field(None, description="Category: mitotic, tumor, stroma, etc.")

    class AnnotationUpdate(BaseModel):
        """Request to update an annotation."""
        label: Optional[str] = None
        notes: Optional[str] = None
        color: Optional[str] = None
        category: Optional[str] = None

    @app.get("/api/slides/{slide_id}/annotations")
    async def get_annotations_endpoint(slide_id: str):
        """Get all annotations for a slide (PostgreSQL-backed)."""
        try:
            rows = await db.get_annotations(slide_id)
        except Exception as e:
            logger.warning(f"Failed to fetch annotations for {slide_id}: {e}")
            rows = []

        annotations = []
        for r in rows:
            annotations.append({
                "id": r["id"],
                "slide_id": r["slide_id"],
                "type": r["type"],
                "coordinates": r["coordinates"],
                "text": r.get("notes") or r.get("label") or "",
                "label": r.get("label"),
                "notes": r.get("notes"),
                "color": r.get("color", "#3b82f6"),
                "category": r.get("category"),
                "created_at": r["created_at"].isoformat() if hasattr(r["created_at"], "isoformat") else str(r["created_at"]),
                "created_by": None,
            })

        return {
            "slide_id": slide_id,
            "annotations": annotations,
            "total": len(annotations),
        }

    @app.post("/api/slides/{slide_id}/annotations")
    async def save_annotation_endpoint(slide_id: str, body: AnnotationCreate):
        """Create a new annotation (PostgreSQL-backed)."""
        import uuid
        annotation_id = f"ann_{uuid.uuid4().hex[:12]}"
        notes = body.notes or body.text or None

        try:
            row = await db.create_annotation(
                annotation_id=annotation_id,
                slide_id=slide_id,
                ann_type=body.type,
                coordinates=body.coordinates,
                label=body.label,
                notes=notes,
                color=body.color or "#3b82f6",
                category=body.category,
            )
        except Exception as e:
            logger.error(f"Failed to create annotation: {e}")
            raise HTTPException(status_code=500, detail=str(e))

        log_audit_event("annotation_created", slide_id, "pathologist", {
            "annotation_id": annotation_id,
            "type": body.type,
        })

        return {
            "id": row["id"],
            "slide_id": row["slide_id"],
            "type": row["type"],
            "coordinates": row["coordinates"],
            "text": row.get("notes") or row.get("label") or "",
            "label": row.get("label"),
            "notes": row.get("notes"),
            "color": row.get("color", "#3b82f6"),
            "category": row.get("category"),
            "created_at": row["created_at"].isoformat() if hasattr(row["created_at"], "isoformat") else str(row["created_at"]),
            "created_by": None,
        }

    @app.put("/api/slides/{slide_id}/annotations/{annotation_id}")
    async def update_annotation_endpoint(slide_id: str, annotation_id: str, body: AnnotationUpdate):
        """Update an annotation's label, notes, color, or category."""
        row = await db.update_annotation(
            annotation_id=annotation_id,
            label=body.label,
            notes=body.notes,
            color=body.color,
            category=body.category,
        )
        if not row:
            raise HTTPException(status_code=404, detail=f"Annotation {annotation_id} not found")

        return {
            "id": row["id"],
            "slide_id": row["slide_id"],
            "type": row["type"],
            "coordinates": row["coordinates"],
            "text": row.get("notes") or row.get("label") or "",
            "label": row.get("label"),
            "notes": row.get("notes"),
            "color": row.get("color", "#3b82f6"),
            "category": row.get("category"),
            "created_at": row["created_at"].isoformat() if hasattr(row["created_at"], "isoformat") else str(row["created_at"]),
            "created_by": None,
        }

    @app.delete("/api/slides/{slide_id}/annotations/{annotation_id}")
    async def delete_annotation_endpoint(slide_id: str, annotation_id: str):
        """Delete an annotation (PostgreSQL-backed)."""
        deleted = await db.delete_annotation(annotation_id)
        if not deleted:
            raise HTTPException(status_code=404, detail=f"Annotation {annotation_id} not found")

        log_audit_event("annotation_deleted", slide_id, "pathologist", {
            "annotation_id": annotation_id,
        })

        return {"success": True, "message": f"Annotation {annotation_id} deleted"}

    @app.get("/api/slides/{slide_id}/annotations/summary")
    async def get_annotations_summary(slide_id: str):
        """Get a summary of annotations for a slide."""
        try:
            rows = await db.get_annotations(slide_id)
        except Exception:
            rows = []

        label_counts: Dict[str, int] = {}
        for ann in rows:
            label = ann.get("label") or ann.get("category")
            if label:
                label_counts[label] = label_counts.get(label, 0) + 1

        return {
            "slide_id": slide_id,
            "total_annotations": len(rows),
            "by_label": label_counts,
        }

    # ====== Multi-Model Analysis Endpoints ======

    @app.get("/api/models", response_model=AvailableModelsResponse)
    async def list_available_models(project_id: Optional[str] = Query(None, description="Filter models by project")):
        """
        List all available TransMIL models.

        Returns model metadata including:
        - Model ID and display name
        - Description of what the model predicts
        - Training AUC score (model reliability)
        - Number of training slides
        - Category (cancer-specific or general_pathology)

        When project_id is provided, model visibility is resolved via:
        1) project_models DB assignments, then
        2) projects.yaml classification_models fallback.
        """
        if multi_model_inference is None:
            raise HTTPException(status_code=503, detail="Multi-model inference not initialized")

        models = multi_model_inference.get_available_models()
        allowed_ids = await _resolve_project_model_ids(project_id)

        if allowed_ids is not None:
            models = [m for m in models if m.get("id", m.get("model_id")) in allowed_ids]

        return AvailableModelsResponse(models=models)


    @app.post("/api/embed-slide")
    async def embed_slide_on_demand(request: dict, background_tasks: BackgroundTasks):
        """
        Extract patches and generate embeddings for a slide on-demand.
        
        Enforces level 0 (full resolution, dense) and starts a background task
        with task_id polling.

        Policy:
        - level=0 only (dense full-resolution embeddings)
        - async mode for background execution
        
        Returns immediately with task_id for background tasks.
        Poll /api/embed-slide/status/{task_id} for progress.
        """
        import time
        
        slide_id = request.get("slide_id")
        level = request.get("level", 1)
        force_reembed = request.get("force", False)
        use_async = request.get("async", level == 0)  # Default async for level 0
        
        if not slide_id:
            raise HTTPException(status_code=400, detail="slide_id required")
        
        if level != 0:
            raise HTTPException(status_code=400, detail="level must be 0 (dense full-resolution policy)")
        
        # Level-specific embedding paths
        # If embeddings_dir is already the level0 dir and level==0, use it directly
        if level == 0 and embeddings_dir.name == "level0":
            level_dir = embeddings_dir
        else:
            level_dir = embeddings_dir / f"level{level}"
        level_dir.mkdir(parents=True, exist_ok=True)
        
        emb_path = level_dir / f"{slide_id}.npy"
        coord_path = level_dir / f"{slide_id}_coords.npy"
        
        # Also check legacy flat path (for backwards compatibility)
        legacy_emb_path = embeddings_dir / f"{slide_id}.npy"
        legacy_coord_path = embeddings_dir / f"{slide_id}_coords.npy"
        
        # Check if embeddings already exist at this level
        if emb_path.exists() and coord_path.exists() and not force_reembed:
            emb = np.load(emb_path)
            return {
                "status": "exists",
                "slide_id": slide_id,
                "level": level,
                "num_patches": len(emb),
                "message": f"Level {level} embeddings already exist"
            }
        
        # For level 1, check if legacy flat embeddings exist
        if level == 1 and legacy_emb_path.exists() and legacy_coord_path.exists() and not force_reembed:
            emb = np.load(legacy_emb_path)
            return {
                "status": "exists",
                "slide_id": slide_id,
                "level": level,
                "num_patches": len(emb),
                "message": f"Level {level} embeddings exist (legacy path)"
            }
        
        # Check if there's already a running task for this slide/level
        existing_task = task_manager.get_task_by_slide(slide_id, level)
        if existing_task:
            return {
                "status": "in_progress",
                "task_id": existing_task.task_id,
                "slide_id": slide_id,
                "level": level,
                "progress": existing_task.progress,
                "message": existing_task.message,
            }
        
        # Find the slide file
        slide_path = resolve_slide_path(slide_id)
        if not slide_path:
            raise HTTPException(
                status_code=404, 
                detail=f"Slide file not found for {slide_id}. Level {level} embedding requires the original .svs file."
            )
        
        # For level 0 or explicit async mode, use background task
        if use_async:
            task = task_manager.create_task(slide_id, level)
            
            # Run embedding in background
            def run_embedding():
                _run_embedding_task(
                    task.task_id,
                    slide_id,
                    level,
                    slide_path,
                    emb_path,
                    coord_path,
                )
            
            background_tasks.add_task(run_embedding)
            
            return {
                "status": "started",
                "task_id": task.task_id,
                "slide_id": slide_id,
                "level": level,
                "message": f"Embedding started in background. Poll /api/embed-slide/status/{task.task_id} for progress.",
                "estimated_time_minutes": 15 if level == 0 else 1,
            }
        
        # For level 1 without async, run inline (original behavior)
        return _run_embedding_inline(slide_id, level, slide_path, emb_path, coord_path)

    def _resolve_pathfoundation_local() -> Optional[str]:
        """Resolve Path Foundation TF saved-model path from local cache only.
        
        Searches HF_HOME / TRANSFORMERS_CACHE / default cache for the
        model directory. Returns the snapshot path or None if not found.
        NEVER downloads anything.
        """
        import os, glob
        hf_home = os.environ.get("HF_HOME", os.environ.get(
            "TRANSFORMERS_CACHE",
            os.path.join(os.path.expanduser("~"), ".cache", "huggingface")
        ))
        model_base = os.path.join(hf_home, "hub", "models--google--path-foundation", "snapshots")
        # Also check /root/.cache (Docker) and the mounted cache volume
        candidates = [
            model_base,
            "/root/.cache/huggingface/hub/models--google--path-foundation/snapshots",
            "/app/cache/huggingface/hub/models--google--path-foundation/snapshots",
        ]
        for base in candidates:
            if os.path.isdir(base):
                # Find the first (or latest) snapshot directory
                snaps = sorted(glob.glob(os.path.join(base, "*")))
                for snap in reversed(snaps):
                    if os.path.isdir(snap) and os.path.exists(os.path.join(snap, "saved_model.pb")):
                        logger.info(f"Path Foundation TF model found at: {snap}")
                        return snap
                    # Also accept if the dir itself has model files
                    if os.path.isdir(snap):
                        logger.info(f"Path Foundation snapshot dir found at: {snap}")
                        return snap
        # Legacy hardcoded path as last resort
        legacy = os.path.expanduser(
            "~/.cache/huggingface/hub/models--google--path-foundation/snapshots/b50f2be6f055ea6ea8719f467ab44b38f37e2142"
        )
        if os.path.exists(legacy):
            return legacy
        return None
    
    def _run_embedding_task(
        task_id: str,
        slide_id: str, 
        level: int,
        slide_path: Path,
        emb_path: Path,
        coord_path: Path,
    ):
        """Background task to run embedding."""
        import time
        
        task = task_manager.get_task(task_id)
        if not task:
            return
        
        task_manager.update_task(task_id, 
            status=TaskStatus.RUNNING,
            started_at=time.time(),
            message="Starting embedding process..."
        )
        
        try:
            import openslide
            
            task_manager.update_task(task_id,
                progress=5,
                message="Opening slide file..."
            )
            
            slide = openslide.OpenSlide(str(slide_path))
            
            actual_level = min(level, slide.level_count - 1)
            level_dims = slide.level_dimensions[actual_level]
            downsample = slide.level_downsamples[actual_level]
            
            width, height = level_dims
            patch_size = 224
            stride = 224
            # No patch limit â€” embed all level-0 grid patches regardless of slide size
            max_patches = float('inf')
            
            task_manager.update_task(task_id,
                progress=10,
                message=f"Extracting tissue patches from {width}x{height} region..."
            )
            
            # Count total potential patches for progress
            total_potential = max(1, ((height - patch_size) // stride) * ((width - patch_size) // stride))
            
            patches = []
            coords = []
            processed = 0
            last_progress_update = time.time()
            
            for y in range(0, height - patch_size, stride):
                for x in range(0, width - patch_size, stride):
                    x0 = int(x * downsample)
                    y0 = int(y * downsample)
                    
                    patch = slide.read_region((x0, y0), actual_level, (patch_size, patch_size))
                    patch = patch.convert("RGB")
                    
                    patch_array = np.array(patch)
                    # Dense policy: keep all grid patches at level 0.
                    patches.append(patch_array)
                    coords.append([x0, y0])
                    
                    processed += 1
                    
                    # Update progress every 2 seconds
                    if time.time() - last_progress_update > 2:
                        extraction_progress = min(processed / total_potential, 1.0) * 40
                        task_manager.update_task(task_id,
                            progress=10 + extraction_progress,
                            message=f"Extracting patches: {len(patches)} tissue patches found ({processed}/{total_potential} checked)"
                        )
                        last_progress_update = time.time()
                    
                    if len(patches) >= max_patches:
                        break
                if len(patches) >= max_patches:
                    break
            
            slide.close()
            
            if len(patches) == 0:
                task_manager.update_task(task_id,
                    status=TaskStatus.FAILED,
                    error="No tissue patches found in slide"
                )
                return
            
            task_manager.update_task(task_id,
                progress=50,
                message=f"Extracted {len(patches)} patches. Loading Path Foundation model..."
            )
            
            # Generate embeddings using locally-cached Path Foundation model.
            # NEVER downloads from HuggingFace â€“ uses only local files.
            import os
            os.environ["CUDA_VISIBLE_DEVICES"] = ""
            os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
            import tensorflow as tf
            
            saved_model_path = _resolve_pathfoundation_local()
            
            if not saved_model_path or not os.path.exists(saved_model_path):
                task_manager.update_task(task_id,
                    status=TaskStatus.FAILED,
                    error="Path Foundation model not found locally. Pre-download it before running embedding."
                )
                return
            
            model = tf.saved_model.load(saved_model_path)
            infer = model.signatures["serving_default"]
            
            task_manager.update_task(task_id,
                progress=55,
                message=f"Model loaded. Generating embeddings for {len(patches)} patches..."
            )
            
            batch_size = 64
            all_embeddings = []
            
            for i in range(0, len(patches), batch_size):
                batch = patches[i:i+batch_size]
                batch_array = np.array(batch, dtype=np.float32) / 255.0
                batch_tensor = tf.constant(batch_array)
                
                result = infer(inputs=batch_tensor)
                embs = result["output_0"].numpy()
                all_embeddings.append(embs)
                
                embed_progress = (i + len(batch)) / len(patches) * 40
                task_manager.update_task(task_id,
                    progress=55 + embed_progress,
                    message=f"Generating embeddings: {min(i+batch_size, len(patches))}/{len(patches)} patches"
                )
            
            embeddings = np.vstack(all_embeddings).astype(np.float32)
            coords_array = np.array(coords)
            
            task_manager.update_task(task_id,
                progress=95,
                message="Saving embeddings..."
            )
            
            np.save(emb_path, embeddings)
            np.save(coord_path, coords_array)
            
            elapsed = time.time() - task.started_at
            task_manager.update_task(task_id,
                status=TaskStatus.COMPLETED,
                progress=100,
                num_patches=len(patches),
                processing_time_seconds=elapsed,
                message=f"Completed: {len(patches)} patches embedded in {elapsed:.1f}s",
                completed_at=time.time()
            )
            
            logger.info(f"Background embedding completed for {slide_id} level {level}: {len(patches)} patches in {elapsed:.1f}s")
            
        except Exception as e:
            logger.error(f"Background embedding failed for {slide_id}: {e}")
            import traceback
            traceback.print_exc()
            task_manager.update_task(task_id,
                status=TaskStatus.FAILED,
                error=str(e)
            )
    
    def _run_embedding_inline(slide_id: str, level: int, slide_path: Path, emb_path: Path, coord_path: Path):
        """Run embedding inline (original behavior for level 1)."""
        import time
        start_time = time.time()
        
        try:
            import openslide
            
            slide = openslide.OpenSlide(str(slide_path))
            
            actual_level = min(level, slide.level_count - 1)
            level_dims = slide.level_dimensions[actual_level]
            downsample = slide.level_downsamples[actual_level]
            
            width, height = level_dims
            patch_size = 224
            stride = 224
            # No patch limit â€” embed all level-0 grid patches
            max_patches = float('inf')
            
            patches = []
            coords = []
            
            for y in range(0, height - patch_size, stride):
                for x in range(0, width - patch_size, stride):
                    x0 = int(x * downsample)
                    y0 = int(y * downsample)
                    
                    patch = slide.read_region((x0, y0), actual_level, (patch_size, patch_size))
                    patch = patch.convert("RGB")
                    
                    patch_array = np.array(patch)
                    # Dense policy: keep all grid patches at level 0.
                    patches.append(patch_array)
                    coords.append([x0, y0])
                    
                    if len(patches) >= max_patches:
                        break
                if len(patches) >= max_patches:
                    break
            
            slide.close()
            
            if len(patches) == 0:
                raise HTTPException(status_code=400, detail="No tissue patches found in slide")
            
            import os
            os.environ["CUDA_VISIBLE_DEVICES"] = ""
            os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
            import tensorflow as tf
            
            saved_model_path = _resolve_pathfoundation_local()
            if not saved_model_path:
                raise HTTPException(status_code=500, detail="Path Foundation model not found locally. Pre-download it before running embedding.")
            
            model = tf.saved_model.load(saved_model_path)
            infer = model.signatures["serving_default"]
            
            batch_size = 64
            all_embeddings = []
            
            for i in range(0, len(patches), batch_size):
                batch = patches[i:i+batch_size]
                batch_array = np.array(batch, dtype=np.float32) / 255.0
                batch_tensor = tf.constant(batch_array)
                
                result = infer(inputs=batch_tensor)
                embs = result["output_0"].numpy()
                all_embeddings.append(embs)
            
            embeddings = np.vstack(all_embeddings).astype(np.float32)
            coords_array = np.array(coords)
            
            np.save(emb_path, embeddings)
            np.save(coord_path, coords_array)
            
            elapsed = time.time() - start_time
            
            return {
                "status": "completed",
                "slide_id": slide_id,
                "level": level,
                "num_patches": len(patches),
                "processing_time_seconds": round(elapsed, 1),
                "message": f"Embedded {len(patches)} patches at level {level}"
            }
            
        except HTTPException:
            raise
        except Exception as e:
            logger.error(f"Embedding failed: {e}")
            import traceback
            traceback.print_exc()
            raise HTTPException(status_code=500, detail=f"Embedding failed: {str(e)}")
    
    @app.get("/api/embed-slide/status/{task_id}")
    async def get_embedding_status(task_id: str):
        """Get status of a background embedding task."""
        task = task_manager.get_task(task_id)
        if not task:
            raise HTTPException(status_code=404, detail=f"Task {task_id} not found")
        
        result = task.to_dict()
        
        if task.status == TaskStatus.COMPLETED:
            level_dir = embeddings_dir if (task.level == 0 and embeddings_dir.name == "level0") else embeddings_dir / f"level{task.level}"
            emb_path = level_dir / f"{task.slide_id}.npy"
            result["embedding_path"] = str(emb_path) if emb_path.exists() else None
        
        return result
    
    @app.get("/api/embed-slide/tasks")
    async def list_embedding_tasks(
        slide_id: Optional[str] = Query(None),
        status: Optional[str] = Query(None)
    ):
        """List all embedding tasks."""
        tasks = []
        for task in task_manager.tasks.values():
            if slide_id and task.slide_id != slide_id:
                continue
            if status and task.status.value != status:
                continue
            tasks.append(task.to_dict())
        
        return {
            "tasks": sorted(tasks, key=lambda t: t.get("elapsed_seconds", 0), reverse=True),
            "total": len(tasks)
        }

    # ====== Batch Re-Embed Endpoints ======

    class BatchEmbedRequest(BaseModel):
        """Request for batch re-embedding of multiple slides."""
        level: int = Field(default=0, ge=0, le=0, description="Resolution level fixed to 0 (dense full-resolution)")
        force: bool = Field(default=True, description="Force re-embedding even if cached")
        slide_ids: Optional[List[str]] = Field(default=None, description="Specific slide IDs (None = all slides)")
        concurrency: int = Field(default=1, ge=1, le=4, description="Concurrent embedding workers (1-4)")

    @app.post("/api/embed-slides/batch")
    async def start_batch_embed(request: BatchEmbedRequest, background_tasks: BackgroundTasks):
        """
        Start batch re-embedding of slides.

        If slide_ids is omitted, all available slides are re-embedded.
        Returns a batch_task_id for progress polling.

        Designed for:
        - "Force Re-Embed" button in the frontend (all slides)
        - Overnight batch runs on DGX (level 0, sequential)
        """
        # Check if there's already an active batch embed task
        active = batch_embed_manager.get_active_task()
        if active:
            return {
                "batch_task_id": active.task_id,
                "status": active.status.value,
                "message": f"Batch embedding already in progress ({active.completed_slides}/{active.total_slides} done)",
                "total": active.total_slides,
            }

        # Determine slide list
        target_slides = request.slide_ids
        if target_slides is None:
            target_slides = list(available_slides)  # All slides with existing embeddings

        if not target_slides:
            raise HTTPException(status_code=400, detail="No slides to embed")

        task = batch_embed_manager.create_task(
            slide_ids=target_slides,
            level=request.level,
            force=request.force,
            concurrency=request.concurrency,
        )

        def run_batch_embed():
            _run_batch_embed_background(task.task_id)

        background_tasks.add_task(run_batch_embed)

        return {
            "batch_task_id": task.task_id,
            "status": "started",
            "total": len(target_slides),
            "message": f"Batch embedding started for {len(target_slides)} slides at level {request.level}.",
        }

    def _run_batch_embed_background(task_id: str):
        """Background worker: sequentially re-embed each slide."""
        import time as _time

        task = batch_embed_manager.get_task(task_id)
        if not task:
            return

        batch_embed_manager.update_task(task_id,
            status=BatchEmbedStatus.RUNNING,
            started_at=_time.time(),
            message="Starting batch embedding...",
        )

        total = task.total_slides
        for idx, slide_id in enumerate(task.slide_ids):
            # Check cancellation
            if batch_embed_manager.is_cancelled(task_id):
                batch_embed_manager.update_task(task_id,
                    status=BatchEmbedStatus.CANCELLED,
                    message=f"Cancelled after {idx}/{total} slides",
                    completed_at=_time.time(),
                )
                return

            batch_embed_manager.update_task(task_id,
                current_slide_index=idx + 1,
                current_slide_id=slide_id,
                progress=(idx / total) * 100,
                message=f"Embedding slide {idx+1}/{total}: {slide_id[:25]}...",
            )

            # Setup paths
            level = task.level
            level_dir = embeddings_dir if (level == 0 and embeddings_dir.name == "level0") else embeddings_dir / f"level{level}"
            level_dir.mkdir(parents=True, exist_ok=True)
            emb_path = level_dir / f"{slide_id}.npy"
            coord_path = level_dir / f"{slide_id}_coords.npy"

            # Skip if exists and not forcing
            if emb_path.exists() and coord_path.exists() and not task.force:
                try:
                    emb = np.load(emb_path)
                    batch_embed_manager.add_result(task_id, BatchEmbedSlideResult(
                        slide_id=slide_id,
                        status="skipped",
                        num_patches=len(emb),
                    ))
                except Exception:
                    batch_embed_manager.add_result(task_id, BatchEmbedSlideResult(
                        slide_id=slide_id, status="skipped",
                    ))
                continue

            # Resolve slide file
            slide_path = resolve_slide_path(slide_id)
            if not slide_path:
                batch_embed_manager.add_result(task_id, BatchEmbedSlideResult(
                    slide_id=slide_id,
                    status="failed",
                    error=f"Slide file not found for {slide_id}",
                ))
                continue

            # Run embedding for this slide
            slide_start = _time.time()
            try:
                import openslide

                slide = openslide.OpenSlide(str(slide_path))
                actual_level = min(level, slide.level_count - 1)
                level_dims = slide.level_dimensions[actual_level]
                downsample = slide.level_downsamples[actual_level]

                width, height = level_dims
                patch_size = 224
                stride = 224
                # No patch limit â€” embed all level-0 grid patches
                max_patches = float('inf')

                patches = []
                coords = []

                for y in range(0, height - patch_size, stride):
                    for x in range(0, width - patch_size, stride):
                        x0 = int(x * downsample)
                        y0 = int(y * downsample)

                        patch = slide.read_region((x0, y0), actual_level, (patch_size, patch_size))
                        patch = patch.convert("RGB")

                        patch_array = np.array(patch)
                        # Dense policy: keep all grid patches at level 0.
                        patches.append(patch_array)
                        coords.append([x0, y0])

                        if len(patches) >= max_patches:
                            break
                    if len(patches) >= max_patches:
                        break

                slide.close()

                if len(patches) == 0:
                    batch_embed_manager.add_result(task_id, BatchEmbedSlideResult(
                        slide_id=slide_id,
                        status="failed",
                        error="No tissue patches found",
                    ))
                    continue

                # Load TF model (reuse across slides via closure)
                import os
                os.environ.setdefault("TF_CPP_MIN_LOG_LEVEL", "2")
                import tensorflow as tf

                saved_model_path = _resolve_pathfoundation_local()
                if not saved_model_path:
                    batch_embed_manager.add_result(task_id, BatchEmbedSlideResult(
                        slide_id=slide_id,
                        status="failed",
                        error="Path Foundation model not found locally",
                    ))
                    continue

                model = tf.saved_model.load(saved_model_path)
                infer = model.signatures["serving_default"]

                batch_size = 64
                all_embeddings = []

                for i in range(0, len(patches), batch_size):
                    batch = patches[i:i + batch_size]
                    batch_array = np.array(batch, dtype=np.float32) / 255.0
                    batch_tensor = tf.constant(batch_array)
                    result = infer(inputs=batch_tensor)
                    embs = result["output_0"].numpy()
                    all_embeddings.append(embs)

                    # Check cancellation mid-slide
                    if batch_embed_manager.is_cancelled(task_id):
                        batch_embed_manager.update_task(task_id,
                            status=BatchEmbedStatus.CANCELLED,
                            message=f"Cancelled during slide {slide_id}",
                            completed_at=_time.time(),
                        )
                        return

                embeddings = np.vstack(all_embeddings).astype(np.float32)
                coords_array = np.array(coords)

                np.save(emb_path, embeddings)
                np.save(coord_path, coords_array)

                elapsed = _time.time() - slide_start
                batch_embed_manager.add_result(task_id, BatchEmbedSlideResult(
                    slide_id=slide_id,
                    status="completed",
                    num_patches=len(patches),
                    processing_time_seconds=elapsed,
                ))

                logger.info(f"Batch embed: {slide_id} level {level} -> {len(patches)} patches in {elapsed:.1f}s")

            except Exception as e:
                logger.error(f"Batch embed failed for {slide_id}: {e}")
                batch_embed_manager.add_result(task_id, BatchEmbedSlideResult(
                    slide_id=slide_id,
                    status="failed",
                    error=str(e),
                    processing_time_seconds=_time.time() - slide_start,
                ))

        # Complete
        batch_embed_manager.update_task(task_id,
            status=BatchEmbedStatus.COMPLETED,
            progress=100,
            message=f"Completed batch embedding of {total} slides",
            completed_at=_time.time(),
        )
        logger.info(f"Batch embed {task_id} completed: {total} slides")

    @app.get("/api/embed-slides/batch/status/{batch_task_id}")
    async def get_batch_embed_status(batch_task_id: str):
        """
        Get progress of a batch embedding task.

        Returns:
        - completed/total/current slide
        - progress percentage
        - Per-slide results (when completed)
        """
        task = batch_embed_manager.get_task(batch_task_id)
        if not task:
            raise HTTPException(status_code=404, detail=f"Batch embed task {batch_task_id} not found")

        if task.status in (BatchEmbedStatus.COMPLETED, BatchEmbedStatus.CANCELLED, BatchEmbedStatus.FAILED):
            return task.to_full_dict()
        return task.to_dict()

    @app.post("/api/embed-slides/batch/cancel/{batch_task_id}")
    async def cancel_batch_embed(batch_task_id: str):
        """Cancel a running batch embedding task."""
        task = batch_embed_manager.get_task(batch_task_id)
        if not task:
            raise HTTPException(status_code=404, detail=f"Batch embed task {batch_task_id} not found")
        if task.status != BatchEmbedStatus.RUNNING:
            raise HTTPException(status_code=400, detail=f"Cannot cancel task with status {task.status.value}")
        batch_embed_manager.request_cancel(batch_task_id)
        return {"success": True, "message": "Cancellation requested"}

    @app.get("/api/embed-slides/batch/active")
    async def get_active_batch_embed():
        """Get the currently active batch embed task, if any."""
        active = batch_embed_manager.get_active_task()
        if active:
            return active.to_dict()
        return {"status": "idle", "message": "No batch embedding in progress"}

    @app.post("/api/analyze-multi", response_model=MultiModelResponse)
    async def analyze_slide_multi(request: MultiModelRequest):
        """
        Analyze a slide with multiple TransMIL models.

        This endpoint runs all (or selected) trained models on a slide:

        **Ovarian Cancer Specific:**
        - Platinum Sensitivity (AUC 0.907): Predicts platinum chemotherapy response
        - 5-Year Survival (AUC 0.697): Predicts 5-year overall survival
        - 3-Year Survival (AUC 0.645): Predicts 3-year overall survival
        - 1-Year Survival (AUC 0.639): Predicts 1-year overall survival

        **General Pathology:**
        - Tumor Grade (AUC 0.752): Predicts high vs low tumor grade

        Use the `models` parameter to select specific models, or leave empty for all.
        Set `level=0` for full-resolution analysis (requires pre-generated level 0 embeddings).
        """
        import time
        start_time = time.time()

        if multi_model_inference is None:
            raise HTTPException(status_code=503, detail="Multi-model inference not initialized")

        slide_id = request.slide_id
        level = request.level
        if level != 0:
            raise HTTPException(status_code=400, detail="level must be 0 (dense full-resolution policy)")

        active_batch_embedding = _active_batch_embed_info()
        if active_batch_embedding:
            raise HTTPException(
                status_code=503,
                detail={
                    "error": "SERVER_BUSY",
                    "message": (
                        "Level 0 batch embedding is currently running. "
                        "Multi-model analysis is temporarily unavailable to avoid GPU contention."
                    ),
                    "retry_after_seconds": 30,
                    "active_batch_embedding": active_batch_embedding,
                },
                headers={"Retry-After": "30"},
            )

        allowed_model_ids = await _resolve_project_model_ids(request.project_id)
        if request.models is not None:
            effective_model_ids = list(request.models)
            if allowed_model_ids is not None:
                disallowed = sorted(set(effective_model_ids) - allowed_model_ids)
                if disallowed:
                    raise HTTPException(
                        status_code=400,
                        detail={
                            "error": "MODELS_NOT_ALLOWED_FOR_PROJECT",
                            "project_id": request.project_id,
                            "disallowed_models": disallowed,
                        },
                    )
        else:
            effective_model_ids = sorted(allowed_model_ids) if allowed_model_ids is not None else None

        # Cache check: if not forcing, look for existing results in the DB
        if not request.force:
            try:
                cached = await db.get_all_cached_results(slide_id)
                if cached:
                    cached_predictions = {}
                    cached_by_cat: Dict[str, list] = {}
                    requested_models = set(effective_model_ids) if effective_model_ids else None

                    for row in cached:
                        mid = row["model_id"]
                        if requested_models and mid not in requested_models:
                            continue
                        cfg = MODEL_CONFIGS.get(mid, {})
                        pred_dict = {
                            "model_id": mid,
                            "model_name": cfg.get("display_name", mid),
                            "category": cfg.get("category", "general_pathology"),
                            "score": row["score"],
                            "label": row["label"],
                            "positive_label": cfg.get("positive_label", "Positive"),
                            "negative_label": cfg.get("negative_label", "Negative"),
                            "confidence": min(row["confidence"], 0.99) if row["confidence"] else 0,
                            "auc": cfg.get("auc", 0.0),
                            "n_training_slides": cfg.get("n_training_slides", cfg.get("n_slides", 0)),
                            "description": cfg.get("description", ""),
                        }
                        mp = ModelPrediction(**pred_dict)
                        cached_predictions[mid] = mp
                        cat = cfg.get("category", "general_pathology")
                        cached_by_cat.setdefault(cat, []).append(mp)

                    if cached_predictions:
                        processing_time = (time.time() - start_time) * 1000
                        logger.info(f"Returning cached results for {slide_id} ({len(cached_predictions)} models)")
                        return MultiModelResponse(
                            slide_id=slide_id,
                            predictions=cached_predictions,
                            by_category=cached_by_cat,
                            n_patches=0,
                            processing_time_ms=processing_time,
                            warnings=["Results loaded from cache"],
                        )
            except Exception as e:
                logger.warning(f"Cache lookup failed for {slide_id}, running fresh: {e}")

        project_requested = request.project_id is not None
        analysis_embeddings_dir = _resolve_project_embeddings_dir(
            request.project_id,
            require_exists=project_requested,
        )

        emb_path: Optional[Path] = None
        if level == 0:
            candidate_dirs = []
            if analysis_embeddings_dir.name == "level0":
                candidate_dirs.append(analysis_embeddings_dir)
            else:
                candidate_dirs.extend([analysis_embeddings_dir / "level0", analysis_embeddings_dir])
            if not project_requested:
                if embeddings_dir.name == "level0":
                    candidate_dirs.append(embeddings_dir)
                else:
                    candidate_dirs.extend([embeddings_dir / "level0", embeddings_dir])

            for d in candidate_dirs:
                cand = d / f"{slide_id}.npy"
                if cand.exists():
                    emb_path = cand
                    break

            if emb_path is None:
                raise HTTPException(
                    status_code=400,
                    detail={
                        "error": "LEVEL0_EMBEDDINGS_REQUIRED",
                        "message": f"Level 0 (full resolution) embeddings do not exist for slide {slide_id}. Generate embeddings first using /api/embed-slide with level=0.",
                        "needs_embedding": True,
                        "slide_id": slide_id,
                        "level": 0,
                        "project_id": request.project_id,
                    }
                )
        else:
            candidate_dirs = [analysis_embeddings_dir]
            if analysis_embeddings_dir.name != "level1":
                candidate_dirs.append(analysis_embeddings_dir / "level1")
            if not project_requested:
                candidate_dirs.extend([
                    embeddings_dir,
                    _data_root / "embeddings" / "level1",
                ])

            for d in candidate_dirs:
                cand = d / f"{slide_id}.npy"
                if cand.exists():
                    emb_path = cand
                    break

        if emb_path is None or not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")

        embeddings = np.load(emb_path)

        try:
            results = multi_model_inference.predict_all(
                embeddings,
                model_ids=effective_model_ids,
                return_attention=request.return_attention,
            )
        except Exception as e:
            logger.error(f"Multi-model inference failed: {e}")
            raise HTTPException(status_code=500, detail=f"Inference failed: {str(e)}")

        processing_time = (time.time() - start_time) * 1000

        def _normalize_prediction_dict(p: Dict[str, Any]) -> Dict[str, Any]:
            out = {k: v for k, v in p.items() if k != "attention"}
            if "confidence" in out and out["confidence"] is not None:
                try:
                    out["confidence"] = min(float(out["confidence"]), 0.99)
                except Exception:
                    pass
            return out

        predictions = {}
        for model_id, pred in results["predictions"].items():
            if "error" not in pred:
                predictions[model_id] = ModelPrediction(**_normalize_prediction_dict(pred))

        by_category: Dict[str, List[ModelPrediction]] = {}
        for cat_key, cat_preds in results.get("by_category", {}).items():
            by_category[cat_key] = [
                ModelPrediction(**_normalize_prediction_dict(p))
                for p in cat_preds if "error" not in p
            ]

        warnings: List[str] = list(results.get("warnings") or [])
        try:
            s1 = predictions.get("survival_1y")
            s3 = predictions.get("survival_3y")
            s5 = predictions.get("survival_5y")
            if s1 and s3:
                if s1.label == s1.negative_label and s3.label == s3.positive_label:
                    warnings.append("Survival predictions inconsistent: 1-year predicts deceased but 3-year predicts survived")
            if s3 and s5:
                if s3.label == s3.negative_label and s5.label == s5.positive_label:
                    warnings.append("Survival predictions inconsistent: 3-year predicts deceased but 5-year predicts survived")
            if s1 and s5:
                if s1.label == s1.negative_label and s5.label == s5.positive_label:
                    warnings.append("Survival predictions inconsistent: 1-year predicts deceased but 5-year predicts survived")
        except Exception:
            pass

        log_audit_event(
            "multi_model_analysis",
            slide_id,
            details={
                "models_run": list(predictions.keys()),
                "processing_time_ms": processing_time,
                "project_id": request.project_id,
            },
        )

        try:
            for mid, pred in predictions.items():
                await db.save_analysis_result(
                    slide_id=slide_id,
                    model_id=mid,
                    score=pred.score,
                    label=pred.label,
                    confidence=pred.confidence,
                )
            logger.info(f"Saved {len(predictions)} analysis results to cache for {slide_id}")
        except Exception as e:
            logger.warning(f"Failed to cache analysis results for {slide_id}: {e}")

        return MultiModelResponse(
            slide_id=slide_id,
            predictions=predictions,
            by_category=by_category,
            n_patches=results["n_patches"],
            processing_time_ms=processing_time,
            warnings=warnings,
        )


    @app.get("/api/heatmap/{slide_id}/{model_id}")
    async def get_model_heatmap(
        slide_id: str,
        model_id: str,
        alpha_power: float = 0.7,
        project_id: Optional[str] = Query(default=None, description="Project ID to scope embeddings lookup")
    ):
        """Get the attention heatmap for a specific TransMIL model.
        
        Available models:
        - platinum_sensitivity
        - tumor_grade  
        - survival_5y
        - survival_3y
        - survival_1y
        """
        if multi_model_inference is None:
            raise HTTPException(status_code=503, detail="Multi-model inference not initialized")
        
        if model_id not in MODEL_CONFIGS:
            raise HTTPException(
                status_code=400, 
                detail=f"Unknown model: {model_id}. Available: {list(MODEL_CONFIGS.keys())}"
            )

        if project_id:
            scope = await resolve_project_model_scope(
                project_id,
                project_registry=project_registry,
                get_project_models=db.get_project_models,
                logger=logger,
            )
            if not scope.project_exists:
                raise HTTPException(status_code=404, detail=f"Unknown project_id: {project_id}")
            if not is_model_allowed_for_scope(model_id, scope):
                raise HTTPException(
                    status_code=403,
                    detail=(
                        f"Model '{model_id}' is not assigned to project '{project_id}'. "
                        f"Use /api/models?project_id={project_id} to fetch allowed model IDs."
                    ),
                )
        
        project_requested = project_id is not None
        _model_heatmap_embeddings_dir = _resolve_project_embeddings_dir(
            project_id,
            require_exists=project_requested,
        )

        allowed_model_ids = await _resolve_project_model_ids(project_id)
        if allowed_model_ids is not None and model_id not in allowed_model_ids:
            raise HTTPException(
                status_code=400,
                detail={
                    "error": "MODEL_NOT_ALLOWED_FOR_PROJECT",
                    "project_id": project_id,
                    "model_id": model_id,
                },
            )

        emb_path = _model_heatmap_embeddings_dir / f"{slide_id}.npy"
        coord_path = _model_heatmap_embeddings_dir / f"{slide_id}_coords.npy"

        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Slide {slide_id} not found")

        # Coordinates are required for truthful attention localization.
        # Synthetic fallback grids create misleading overlays when embeddings were
        # generated after tissue filtering without persisted coords.
        if not coord_path.exists():
            raise HTTPException(
                status_code=409,
                detail={
                    "error": "COORDS_REQUIRED_FOR_HEATMAP",
                    "slide_id": slide_id,
                    "project_id": project_id,
                    "message": "Patch coordinates are missing for this slide; regenerate/recover *_coords.npy before rendering attention heatmap.",
                },
            )

        # Check disk cache first (only for default alpha_power).
        # Use a versioned cache key to avoid serving stale overlays generated
        # with pre-fix coordinate assumptions.
        cache_dir = _model_heatmap_embeddings_dir / "heatmap_cache"
        cache_dir.mkdir(exist_ok=True)
        is_default_alpha = abs(alpha_power - 0.7) < 0.01
        cache_suffix = project_id if project_id else "global"
        cache_path = cache_dir / f"{cache_suffix}_{slide_id}_{model_id}_v2.png"

        if is_default_alpha and cache_path.exists():
            # Serve cached heatmap â€” still need slide dims for headers
            slide_path = resolve_slide_path(slide_id, project_id=project_id)
            _slide_dims = None
            if slide_path is not None and slide_path.exists():
                try:
                    import openslide
                    with openslide.OpenSlide(str(slide_path)) as _slide:
                        _slide_dims = _slide.dimensions
                except Exception:
                    pass
            if _slide_dims is None:
                patch_size_c = 224
                _ca = np.load(coord_path).astype(np.int64, copy=False)
                _slide_dims = (int(_ca[:, 0].max()) + patch_size_c, int(_ca[:, 1].max()) + patch_size_c)
            _coverage = compute_heatmap_grid_coverage(_slide_dims[0], _slide_dims[1], patch_size=224)
            logger.info(f"Serving cached heatmap for {slide_id}/{model_id}")
            return FileResponse(
                str(cache_path),
                media_type="image/png",
                headers={
                    "X-Model-Id": model_id,
                    "X-Model-Name": MODEL_CONFIGS[model_id]["display_name"],
                    "X-Slide-Width": str(_slide_dims[0]),
                    "X-Slide-Height": str(_slide_dims[1]),
                    "X-Coverage-Width": str(_coverage.coverage_width),
                    "X-Coverage-Height": str(_coverage.coverage_height),
                    "Access-Control-Expose-Headers": "X-Model-Id, X-Model-Name, X-Slide-Width, X-Slide-Height, X-Coverage-Width, X-Coverage-Height",
                }
            )

        embeddings = np.load(emb_path)

        patch_size = 224
        coords_arr = np.load(coord_path).astype(np.int64, copy=False)
        
        # Get prediction with attention from specific model
        try:
            result = multi_model_inference.predict_single(
                embeddings, 
                model_id, 
                return_attention=True
            )
            attention = result.get("attention")
            
            if attention is None:
                raise HTTPException(status_code=500, detail="Model did not return attention weights")
            
            attention = np.array(attention)
            
        except Exception as e:
            logger.error(f"Model inference failed: {e}")
            raise HTTPException(status_code=500, detail=str(e))
        
        # Generate heatmap image using EvidenceGenerator for proper coordinate scaling
        try:
            # Get actual slide dimensions
            slide_path = resolve_slide_path(slide_id, project_id=project_id)
            if slide_path is not None and slide_path.exists():
                try:
                    import openslide
                    with openslide.OpenSlide(str(slide_path)) as slide:
                        slide_dims = slide.dimensions
                        logger.info(f"Model heatmap using actual slide dims: {slide_dims}")
                except Exception as e:
                    logger.warning(f"Could not read slide dimensions: {e}")
                    slide_dims = None
            else:
                slide_dims = None
            
            # Fall back to computing bounds from coordinates
            if slide_dims is None:
                if coords_arr.size == 0:
                    slide_dims = (patch_size, patch_size)
                    logger.warning(
                        f"No coordinates available to derive dims for {slide_id}; "
                        f"falling back to {slide_dims}"
                    )
                else:
                    x_max = int(coords_arr[:, 0].max()) + patch_size
                    y_max = int(coords_arr[:, 1].max()) + patch_size
                    slide_dims = (x_max, y_max)
                    logger.info(f"Model heatmap using coords-derived dims: {slide_dims}")
            
            # Generate a patch-resolution heatmap: 1 pixel = 1 patch (224x224).
            # This produces crisp discrete patches when rendered with image-rendering: pixelated.
            coords_list = [tuple(map(int, c)) for c in coords_arr]
            _coverage = compute_heatmap_grid_coverage(slide_dims[0], slide_dims[1], patch_size=patch_size)
            grid_w = _coverage.grid_width
            grid_h = _coverage.grid_height

            logger.info(f"Model heatmap patch-resolution: {grid_w}x{grid_h} (1 pixel per {patch_size}px patch)")

            heatmap_rgba = evidence_gen.create_heatmap(
                attention,
                coords_list,
                slide_dims,
                thumbnail_size=(grid_w, grid_h),
                smooth=False,
                blur_kernel=1,
                alpha_power=alpha_power,
            )
            
            # Convert RGBA to PNG
            img = Image.fromarray(heatmap_rgba, mode="RGBA")
            
            buf = io.BytesIO()
            img.save(buf, format="PNG")
            buf.seek(0)
            
            # Save to disk cache for subsequent requests (only default alpha)
            if is_default_alpha:
                try:
                    with open(cache_path, "wb") as f:
                        f.write(buf.getvalue())
                    logger.info(f"Cached heatmap to {cache_path}")
                except Exception as cache_err:
                    logger.warning(f"Failed to cache heatmap: {cache_err}")
            
            return StreamingResponse(
                buf,
                media_type="image/png",
                headers={
                    "X-Model-Id": model_id,
                    "X-Model-Name": MODEL_CONFIGS[model_id]["display_name"],
                    "X-Slide-Width": str(slide_dims[0]),
                    "X-Slide-Height": str(slide_dims[1]),
                    "X-Coverage-Width": str(_coverage.coverage_width),
                    "X-Coverage-Height": str(_coverage.coverage_height),
                    "Access-Control-Expose-Headers": "X-Model-Id, X-Model-Name, X-Slide-Width, X-Slide-Height, X-Coverage-Width, X-Coverage-Height",
                }
            )
            
        except Exception as e:
            logger.error(f"Heatmap generation failed: {e}")
            raise HTTPException(status_code=500, detail=f"Heatmap generation failed: {e}")


    # Register slide metadata API
    metadata_path = _data_root / "slide_metadata.json"
    if not metadata_path.exists():
        metadata_path = embeddings_dir.parent / "slide_metadata.json"
    metadata_manager = SlideMetadataManager(metadata_path)

    def get_available_slide_ids():
        return list(available_slides.keys())
    metadata_router = create_metadata_router(metadata_manager, get_available_slide_ids)
    app.include_router(metadata_router)

    # Project system routes (config-driven multi-cancer support)
    app.include_router(project_router)

    # Agent workflow routes (workflow instance is initialized during startup, after models are loaded)
    if AGENT_AVAILABLE:
        try:
            app.include_router(agent_router)
            logger.info("Agent workflow routes registered")
        except Exception as e:
            logger.warning(f"Failed to register agent workflow routes: {e}")
    else:
        logger.warning("Agent workflow not available - skipping registration")

    # ====== Chat API Endpoint ======
    # Initialize ChatManager for RAG-based conversational AI
    try:
        from ..llm.chat import ChatManager
        chat_manager = ChatManager(
            embeddings_dir=embeddings_dir,
            multi_model_inference=multi_model_inference,
            evidence_generator=evidence_gen,
            medgemma_reporter=reporter,
            slide_labels=slide_labels,
            slide_mean_index=slide_mean_index,
            slide_mean_ids=slide_mean_ids,
            slide_mean_meta=slide_mean_meta,
        )
        logger.info("ChatManager initialized for RAG-based chat")
        
        class ChatRequest(BaseModel):
            """Request for chat endpoint."""
            message: str = Field(..., min_length=1, max_length=2000)
            slide_id: Optional[str] = Field(None, max_length=256)
            session_id: Optional[str] = Field(None, max_length=64)
            history: Optional[List[Dict[str, str]]] = None
        
        class ChatResponse(BaseModel):
            """Response from chat endpoint."""
            response: str
            session_id: str
            evidence_patches: Optional[List[Dict[str, Any]]] = None
        
        async def stream_chat(message: str, slide_id: Optional[str], session_id: Optional[str], history: Optional[List]):
            """Stream chat responses as SSE."""
            async for result in chat_manager.chat(
                message=message,
                session_id=session_id,
                slide_id=slide_id,
                history=history,
            ):
                yield f"data: {json.dumps(result)}\n\n"
        
        @app.post("/api/chat")
        async def chat_endpoint(request: ChatRequest):
            """
            RAG-based chat endpoint for conversational AI assistant.
            
            This endpoint provides a conversational interface for asking questions
            about slide analysis results. It uses RAG (Retrieval Augmented Generation)
            to provide context-aware answers based on:
            
            - Clinical reports generated for the slide
            - Model predictions (platinum sensitivity, survival, etc.)
            - Similar cases from the reference database
            - High-attention evidence regions
            
            Example questions:
            - "What is the prognosis?"
            - "Why was this prediction made?"
            - "Show me the high-attention regions"
            - "How does this compare to similar cases?"
            
            Returns Server-Sent Events (SSE) stream with reasoning steps.
            
            Args:
                message: User question/message
                slide_id: Optional slide ID for context (required for slide-specific questions)
                session_id: Optional session ID for multi-turn conversation
                history: Optional previous chat history
            
            Returns:
                SSE stream with chat response and evidence
            """
            return StreamingResponse(
                stream_chat(
                    request.message,
                    request.slide_id,
                    request.session_id,
                    request.history,
                ),
                media_type="text/event-stream",
                headers={
                    "Cache-Control": "no-cache",
                    "Connection": "keep-alive",
                    "X-Accel-Buffering": "no",
                },
            )
        
        @app.get("/api/chat/session/{session_id}")
        async def get_chat_session(session_id: str):
            """Get chat session history and context."""
            session = chat_manager._sessions.get(session_id)
            if not session:
                raise HTTPException(status_code=404, detail=f"Session {session_id} not found")
            
            return {
                "session_id": session.session_id,
                "slide_id": session.slide_id,
                "created_at": session.created_at,
                "history": [
                    {
                        "role": msg.role,
                        "content": msg.content,
                        "timestamp": msg.timestamp,
                        "evidence_patches": msg.evidence_patches,
                    }
                    for msg in session.history
                ],
                "has_context": session.context is not None,
            }
        
        logger.info("Chat API endpoints registered")
        
    except Exception as e:
        logger.warning(f"Failed to initialize ChatManager: {e}")

    # ------------------------------------------------------------------
    # Few-Shot Patch Classification
    # ------------------------------------------------------------------
    @app.post("/api/slides/{slide_id}/patch-classify", response_model=PatchClassifyResponse)
    async def classify_patches(slide_id: str, request: PatchClassifyRequest):
        """Few-shot patch classification using logistic regression on Path Foundation embeddings.

        Users provide a small number of example patch indices per class. A logistic
        regression is trained on the corresponding embeddings and then applied to
        ALL patches in the slide, returning per-patch class predictions suitable
        for heatmap rendering.
        """
        from sklearn.linear_model import LogisticRegression

        # Validate: need at least 2 classes
        if len(request.classes) < 2:
            raise HTTPException(status_code=400, detail="At least 2 classes are required")

        # Validate: each class needs at least 1 example
        for cls_name, indices in request.classes.items():
            if len(indices) < 1:
                raise HTTPException(status_code=400, detail=f"Class '{cls_name}' needs at least 1 example patch")

        # Load embeddings and coordinates
        emb_path = embeddings_dir / f"{slide_id}.npy"
        coord_path = embeddings_dir / f"{slide_id}_coords.npy"

        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Embeddings not found for slide {slide_id}")
        if not coord_path.exists():
            raise HTTPException(status_code=404, detail=f"Coordinates not found for slide {slide_id}")

        embeddings_data = np.load(emb_path).astype(np.float32)
        coords = np.load(coord_path)
        n_patches = len(embeddings_data)

        if n_patches == 0:
            raise HTTPException(status_code=400, detail="Slide has no patch embeddings")

        # Validate patch indices are in range
        for cls_name, indices in request.classes.items():
            for idx in indices:
                if idx < 0 or idx >= n_patches:
                    raise HTTPException(
                        status_code=400,
                        detail=f"Patch index {idx} out of range for class '{cls_name}' (slide has {n_patches} patches)"
                    )

        # Build training set
        class_names = sorted(request.classes.keys())
        train_indices = []
        train_labels = []
        for cls_name in class_names:
            for idx in request.classes[cls_name]:
                train_indices.append(idx)
                train_labels.append(cls_name)

        X_train = embeddings_data[train_indices]
        y_train = np.array(train_labels)

        # Train logistic regression
        clf = LogisticRegression(max_iter=1000, random_state=42)
        clf.fit(X_train, y_train)

        # Predict on ALL patches
        y_pred = clf.predict(embeddings_data)
        y_proba = clf.predict_proba(embeddings_data)
        proba_classes = list(clf.classes_)

        # Build predictions list
        predictions = []
        class_counts: Dict[str, int] = {c: 0 for c in class_names}
        for i in range(n_patches):
            pred_class = str(y_pred[i])
            proba_dict = {str(proba_classes[j]): round(float(y_proba[i][j]), 4) for j in range(len(proba_classes))}
            confidence = float(max(y_proba[i]))
            class_counts[pred_class] = class_counts.get(pred_class, 0) + 1
            predictions.append(PatchClassificationItem(
                patch_idx=i,
                x=int(coords[i][0]),
                y=int(coords[i][1]),
                predicted_class=pred_class,
                confidence=round(confidence, 4),
                probabilities=proba_dict,
            ))

        # Build heatmap data (class index + confidence for each patch)
        heatmap_data = []
        for i in range(n_patches):
            heatmap_data.append({
                "x": int(coords[i][0]),
                "y": int(coords[i][1]),
                "class_idx": class_names.index(str(y_pred[i])),
                "confidence": round(float(max(y_proba[i])), 4),
            })

        # Leave-one-out accuracy on training examples
        accuracy_estimate = None
        if len(train_indices) > len(class_names):
            correct = 0
            for leave_out in range(len(train_indices)):
                loo_indices = train_indices[:leave_out] + train_indices[leave_out + 1:]
                loo_labels = list(train_labels[:leave_out]) + list(train_labels[leave_out + 1:])
                # Need at least 2 distinct classes in LOO set
                if len(set(loo_labels)) < 2:
                    continue
                X_loo = embeddings_data[loo_indices]
                y_loo = np.array(loo_labels)
                clf_loo = LogisticRegression(max_iter=1000, random_state=42)
                clf_loo.fit(X_loo, y_loo)
                pred = clf_loo.predict(embeddings_data[[train_indices[leave_out]]])[0]
                if pred == train_labels[leave_out]:
                    correct += 1
            if len(train_indices) > 0:
                accuracy_estimate = round(correct / len(train_indices), 4)

        log_audit_event("patch_classification", slide_id, details={
            "classes": class_names,
            "training_examples": len(train_indices),
            "total_patches": n_patches,
            "accuracy_estimate": accuracy_estimate,
        })

        return PatchClassifyResponse(
            slide_id=slide_id,
            classes=class_names,
            total_patches=n_patches,
            predictions=predictions,
            class_counts=class_counts,
            accuracy_estimate=accuracy_estimate,
            heatmap_data=heatmap_data,
        )

    # ------------------------------------------------------------------
    # Outlier Tissue Detection
    # ------------------------------------------------------------------
    @app.post("/api/slides/{slide_id}/outlier-detection", response_model=OutlierDetectionResponse)
    async def detect_outlier_tissue(slide_id: str, threshold: float = 2.0):
        """Detect outlier tissue patches using embedding distance from centroid.

        Computes the mean centroid of all patch embeddings for a slide, then
        flags patches whose Euclidean distance exceeds mean + threshold * std.
        Returns per-patch normalized scores suitable for heatmap rendering.
        """
        emb_path = embeddings_dir / f"{slide_id}.npy"
        coord_path = embeddings_dir / f"{slide_id}_coords.npy"

        if not emb_path.exists():
            raise HTTPException(status_code=404, detail=f"Embeddings not found for slide {slide_id}")
        if not coord_path.exists():
            raise HTTPException(status_code=404, detail=f"Coordinates not found for slide {slide_id}")

        embeddings_data = np.load(emb_path).astype(np.float32)
        coords = np.load(coord_path)

        if len(embeddings_data) == 0:
            raise HTTPException(status_code=400, detail="Slide has no patch embeddings")

        # Compute centroid and distances
        centroid = np.mean(embeddings_data, axis=0)
        distances = np.linalg.norm(embeddings_data - centroid, axis=1)

        mean_dist = float(np.mean(distances))
        std_dist = float(np.std(distances))

        # Identify outliers
        cutoff = mean_dist + threshold * std_dist
        outlier_mask = distances > cutoff

        # Build outlier patch list sorted by distance descending
        outlier_indices = np.where(outlier_mask)[0]
        outlier_patches = []
        for idx in outlier_indices:
            z = (float(distances[idx]) - mean_dist) / std_dist if std_dist > 0 else 0.0
            outlier_patches.append(OutlierPatch(
                patch_idx=int(idx),
                x=int(coords[idx][0]),
                y=int(coords[idx][1]),
                distance=float(distances[idx]),
                z_score=round(z, 3),
            ))
        outlier_patches.sort(key=lambda p: p.distance, reverse=True)

        # Normalize distances to 0-1 for heatmap rendering
        d_min = float(distances.min())
        d_max = float(distances.max())
        if d_max - d_min > 0:
            scores = (distances - d_min) / (d_max - d_min)
        else:
            scores = np.zeros_like(distances)

        heatmap_data = []
        for i in range(len(coords)):
            heatmap_data.append({
                "x": int(coords[i][0]),
                "y": int(coords[i][1]),
                "score": round(float(scores[i]), 4),
            })

        log_audit_event("outlier_detection", slide_id, details={
            "threshold": threshold,
            "total_patches": len(embeddings_data),
            "outlier_count": len(outlier_patches),
        })

        return OutlierDetectionResponse(
            slide_id=slide_id,
            outlier_patches=outlier_patches,
            total_patches=len(embeddings_data),
            outlier_count=len(outlier_patches),
            mean_distance=round(mean_dist, 4),
            std_distance=round(std_dist, 4),
            threshold=threshold,
            heatmap_data=heatmap_data,
        )

    # ------------------------------------------------------------------
    # Patch Coordinates (for spatial selection in the viewer)
    # ------------------------------------------------------------------
    @app.get("/api/slides/{slide_id}/patch-coords")
    async def get_patch_coords(slide_id: str):
        """Return patch (x,y) coordinates for a slide.

        Used by the frontend to enable spatial patch selection on the WSI
        viewer (e.g. click-to-select patches for few-shot classification).
        """
        coord_path = embeddings_dir / f"{slide_id}_coords.npy"
        if not coord_path.exists():
            raise HTTPException(status_code=404, detail=f"No coordinates found for slide {slide_id}")
        coords = np.load(coord_path)
        return {
            "slide_id": slide_id,
            "count": len(coords),
            "coords": coords.tolist(),
        }

    return app


# Default app instance â€” prefer level0 embeddings when available
import os as _os_app
_emb_dir = Path(_os_app.environ.get("EMBEDDINGS_DIR", "data/embeddings"))
# Auto-detect level0 subdirectory (full-resolution re-embeddings)
if not _os_app.environ.get("EMBEDDINGS_DIR") and (_emb_dir / "level0").is_dir():
    _emb_dir = _emb_dir / "level0"
app = create_app(embeddings_dir=_emb_dir)


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
